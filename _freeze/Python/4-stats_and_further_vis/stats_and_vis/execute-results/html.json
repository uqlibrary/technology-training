{
  "hash": "bb8916c7f86a872469b1d7381090dbf7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: 'Python training (4 of 4): Statistics and Further Visualisation'\njupyter: python3\n---\n\n<!-- Upcoming workshops -->\n:::{.callout-tip}\n# Upcoming workshop(s) available!\n\nThe next workshop is on **Tue Nov 25 at 09:30 AM.**\n\n[Book in to the next offering now.](https://studenthub.uq.edu.au/students/events/detail/5909088)\n\nAlternatively, [check our calendar](https://web.library.uq.edu.au/study-and-learning-support/training-and-workshops/online-and-person-workshops#keyword=;campus=;weekstart=) for future events.\n:::\nThis session is aimed as an overview of how to perform some statistical modelling with Python. **It is a Python workshop, not a statistics workshop** - if you'd like to better understand the statistical models, or need help deciding what's best for you, please consult a statistics resource or contact a statistician.\n\nIn this session, we'll cover\n\n- Descriptive statistics\n  - Measures of central tendancy\n  - Measures of variability\n  - Measures of correlation\n  \n- Inferential statistics\n  - Linear regressions\n  - T-tests\n  - $\\chi^2$ tests\n  - Generalised regressions\n\n- Visualising statistics\n  - Adding lines to graphs to indicate bounds\n  - Shading regions\n  - Subplots\n  - Boxplots\n\nWe'll use two new modules:\n\n - `scipy.stats`\n - `statsmodels`\n\n## Setup\n\nLet's import all our modules for today:\n\n::: {#eee7cdb0 .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n```\n:::\n\n\nWe'll be working from our \"Players2024\" dataset again. If you don't have it yet,\n\n1. [Download the dataset](data/Players2024.csv).\n2. Create a folder in **in the same location as your script** called \"data\".\n3. Save the dataset there.\n\nTo bring it in and clean it up,\n\n::: {#5965fc35 .cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv(\"data/Players2024.csv\")\ndf = df[df[\"positions\"] != \"Missing\"]\ndf = df[df[\"height_cm\"] > 100]\n```\n:::\n\n\n## Descriptive Statistics\n\nWe'll start with sample size. All dataframes have most descriptive statistics functions available right off the bat which we access via the `.` operator. \n\nTo calculate the number of non-empty observations in a column, say the numeric variable `df[\"height_cm\"]`, we use the `.count()` method\n\n::: {#6dd55c21 .cell execution_count=3}\n``` {.python .cell-code}\ndf[\"height_cm\"].count()\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n5932\n```\n:::\n:::\n\n\n### Measures of central tendancy\nWe can compute measures of central tendancy similarly. The average value is given by\n\n::: {#c29ec3a9 .cell execution_count=4}\n``` {.python .cell-code}\ndf[\"height_cm\"].mean()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n183.04130141604855\n```\n:::\n:::\n\n\nthe median by\n\n::: {#f6753158 .cell execution_count=5}\n``` {.python .cell-code}\ndf[\"height_cm\"].median()\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n183.0\n```\n:::\n:::\n\n\nand the mode by\n\n::: {#c08e5d1c .cell execution_count=6}\n``` {.python .cell-code}\ndf[\"height_cm\"].mode()\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n0    185.0\nName: height_cm, dtype: float64\n```\n:::\n:::\n\n\n> `.mode()` returns a dataframe with the most frequent values as there can be multiple.\n\n#### Visualisation\n\nLet's visualise our statistics as we go. We can start by producing a histogram of the heights with `seaborn`:\n\n::: {#3ed0eca9 .cell execution_count=7}\n``` {.python .cell-code}\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-8-output-1.png){width=470 height=470}\n:::\n:::\n\n\nWe can use matplotlib to annotate the locations of these statistics. Let's save them into variables and then make the plot again. The important function is [`plt.vlines`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.vlines.html), which enables you to create vertical line(s) on your plot. We'll do it once for each so that we get separate legend entries. We'll need to provide the parameters\n\n- `x = `\n- `ymin = `\n- `ymax = `\n- `colors = `\n- `linestyles = `\n- `label = `\n\n::: {#e3d06b04 .cell execution_count=8}\n``` {.python .cell-code}\n# Save the statistics\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", linestyles = \"dashed\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", linestyles = \"solid\", label = \"Mode\")\n\n# Create the legend with the labels\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-9-output-1.png){width=470 height=470}\n:::\n:::\n\n\n### Activity 1: Visualisation\n\nLet's extend this visualisation to include the median and mode, as well as change the median's linestyle to be \"dotted\".\n\n1. Visualise the median and mode, like the average, with different colours.\n2. Go to the documentation for [`plt.vlines()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.vlines.html) to change the median's linestyle.\n\nThen, try to use\n\n* [`plt.legend()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.legend.html) to remove the border on the legend.\n* [`plt.xlabel()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.xlabel.html) to clean up the $x$-label.\n\n\n:::{.callout-note collapse=true}\n# Solution\n\nThe following code is one possible solution.\n\n::: {#d420a9b3 .cell execution_count=9}\n``` {.python .cell-code}\n# Save the statistics\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", label = \"Mode\")\n\n# Create the legend with the labels\nplt.legend(frameon = False)\nplt.xlabel(\"Height (cm)\")\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\nText(0.5, 9.066666666666652, 'Height (cm)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-10-output-2.png){width=470 height=470}\n:::\n:::\n\n\n:::\n\n:::{.callout-tip}\n# Including mathematical symbols with $\\LaTeX$.\n\nYou can tell `matplotlib` to execute certain $\\LaTeX$ commands in the text you plot, enabling inclusion of mathematics in your plots. This is particularly useful for certain units.\n\nLet's imagine we want to use $x_{height}$ for the $x$-label. Using dollar signs,\n\n```python\nplt.xlabel(\"$x_{height}$ (cm)\")\n```\n\n::: {#cbf19916 .cell execution_count=10}\n``` {.python .cell-code}\n# Save the statistics\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", label = \"Mode\")\n\n# Create the legend with the labels\nplt.legend(frameon = False)\nplt.xlabel(\"$x_{height}$ (cm)\")\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nText(0.5, 9.066666666666652, '$x_{height}$ (cm)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-11-output-2.png){width=470 height=474}\n:::\n:::\n\n\n$\\LaTeX$ commands use backslashes `\\`, and sometimes this conflicts with Python string's escape characters. To avoid this, prefix your conflicting strings with `R`, as in\n\n```python\nplt.xlabel(R\"$x_{height}$ (m$\\times 10^{-2}$)\")\n```\n\n::: {#ed12b786 .cell execution_count=11}\n``` {.python .cell-code}\n# Save the statistics\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", label = \"Mode\")\n\n# Create the legend with the labels\nplt.legend(frameon = False)\nplt.xlabel(R\"$x_{height}$ (m$\\times 10^{-2}$)\")\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\nText(0.5, 9.066666666666652, '$x_{height}$ (m$\\\\times 10^{-2}$)')\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-12-output-2.png){width=470 height=476}\n:::\n:::\n\n\n:::\n\n### Measures of variance\n\nWe can also compute measures of variance. The minimum and maximum are as expected\n\n::: {#e8e15a5e .cell execution_count=12}\n``` {.python .cell-code}\ndf[\"height_cm\"].min()\ndf[\"height_cm\"].max()\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n206.0\n```\n:::\n:::\n\n\nThe range is the difference\n\n::: {#d6564593 .cell execution_count=13}\n``` {.python .cell-code}\ndf[\"height_cm\"].max() - df[\"height_cm\"].min()\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n46.0\n```\n:::\n:::\n\n\nQuantiles are given by `.quantile(...)` with the fraction inside. The inter-quartile range (IQR) is the difference between 25% and 75%.\n\n::: {#a814b2d3 .cell execution_count=14}\n``` {.python .cell-code}\nq1 = df[\"height_cm\"].quantile(0.25)\nq3 = df[\"height_cm\"].quantile(0.75)\nIQR = q3 - q1\n```\n:::\n\n\nA column's standard deviation and variance are given by\n\n::: {#0ec2bef0 .cell execution_count=15}\n``` {.python .cell-code}\ndf[\"height_cm\"].std()\ndf[\"height_cm\"].var()\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n46.7683158241558\n```\n:::\n:::\n\n\nAnd the standard error of the mean (SEM) with\n\n::: {#c9e9f975 .cell execution_count=16}\n``` {.python .cell-code}\ndf[\"height_cm\"].sem()\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n0.08879229764682213\n```\n:::\n:::\n\n\nYou can calculate the skewness and kurtosis (variation of tails) of a sample with\n\n::: {#08fc8f7e .cell execution_count=17}\n``` {.python .cell-code}\ndf[\"height_cm\"].skew()\ndf[\"height_cm\"].kurt()\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n-0.4338044567190438\n```\n:::\n:::\n\n\nAll together, you can see a nice statistical summary with\n\n::: {#4858a954 .cell execution_count=18}\n``` {.python .cell-code}\ndf[\"height_cm\"].describe()\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\ncount    5932.000000\nmean      183.041301\nstd         6.838736\nmin       160.000000\n25%       178.000000\n50%       183.000000\n75%       188.000000\nmax       206.000000\nName: height_cm, dtype: float64\n```\n:::\n:::\n\n\n### Activity 2: Visualisation\n\nLet's take our previous visualisation and shade in the IQR using the function [`plt.fill_between()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.fill_between.html). You'll want to complete this activity in a few steps:\n\n1. Copy the code from the previous plot\n2. Save the quartiles Q1 and Q3 in variables\n3. Read the documentation for [`plt.fill_between()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.fill_between.html)\n4. Try to use it.\n\nYou'll first want to use the function with the following parameters\n\n- `x = `\n- `y1 = `\n- `y2 = `\n\ni.e.,\n\n```python\nplt.fill_between(x = ..., y1 = ..., y2 = ...)\n```\n\nOnce you've got it working, try adding the following two\n- `alpha = `  (for the opacity, a number between 0-1)\n- `label = `  (for the legend)\n\n:::{.callout-tip}\n# Hint\n\nThe documentation for `plt.fill_between()` specifies that \n\n- **`x`** should be ***array-like***. For us, this means a `list`.\n- **`y1`** and **`y2`** should be ***array-like or float***. This means either a `list` or a single decminal number.\n:::\n\n:::{.callout-note collapse=true}\n# Solution\n\nThe following code is one possible solution.\n\n```python\n# Save the quartiles\nheight_Q1 = df[\"height_cm\"].quantile(0.25)\nheight_Q3 = df[\"height_cm\"].quantile(0.75)\n\n# Shade in the IQR\nplt.fill_between(x = [height_Q1, height_Q3], y1 = 0, y2 = 500, alpha = 0.2, label = \"IQR\")\n```\n\nAll together with the original plot, this looks like\n\n::: {#aee3c0d4 .cell execution_count=19}\n``` {.python .cell-code}\n# Save the statistics\nheight_tot = df[\"height_cm\"].count()\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Save the quartiles\nheight_Q1 = df[\"height_cm\"].quantile(0.25)\nheight_Q3 = df[\"height_cm\"].quantile(0.75)\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", linestyles = \"dashed\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", linestyles = \"solid\", label = \"Mode\")\n\n# Shade in the IQR\nplt.fill_between(x = [height_Q1, height_Q3], y1 = 0, y2 = 500, alpha = 0.2, label = \"IQR\")\n\n# Create the legend with the labels\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-20-output-1.png){width=470 height=470}\n:::\n:::\n\n\n:::\n\n## Inferential Statistics\n\nInferential statistics requires using the module `scipy.stats`.\n\n### Simple linear regressions\n\nLeast-squares regression for two sets of measurements can be performed with the function `stats.linregress()`\"\n\n::: {#e21b4b7d .cell execution_count=20}\n``` {.python .cell-code}\nstats.linregress(x = df[\"age\"], y = df[\"height_cm\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\nLinregressResult(slope=0.02582749476456191, intercept=182.38260451315895, rvalue=0.01682597901197303, pvalue=0.19506275453364208, stderr=0.01993026652960195, intercept_stderr=0.515991957177263)\n```\n:::\n:::\n\n\nIf we store this as a variable, we can access the different values with the `.` operator. For example, the p-value is\n\n::: {#19407ae8 .cell execution_count=21}\n``` {.python .cell-code}\nlm = stats.linregress(x = df[\"age\"], y = df[\"height_cm\"])\nlm.pvalue\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\n0.19506275453364208\n```\n:::\n:::\n\n\n#### Visualisation\n\nLet's look at implementing the linear regression into our scatter plot from before. Using the scatterplot from before,\n\n::: {#f9b6675b .cell execution_count=22}\n``` {.python .cell-code}\nsns.relplot(df, x = \"age\", y = \"height_cm\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-23-output-1.png){width=470 height=470}\n:::\n:::\n\n\nwe'll need to plot the regression as a line. For reference,\n\n$$ y = \\text{slope}\\times x + \\text{intercept}$$\n\nSo\n\n::: {#523cf0da .cell execution_count=23}\n``` {.python .cell-code}\nsns.relplot(df, x = \"age\", y = \"height_cm\")\n\n# Construct the linear regression\nx_lm = df[\"age\"]\ny_lm = lm.slope*x_lm + lm.intercept\n\n# Plot the line plot\nsns.lineplot(x = x_lm, y = y_lm, color = \"r\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-24-output-1.png){width=470 height=470}\n:::\n:::\n\n\nFinally, we can include the details of the linear regression in the legend by specifying them in the label. We'll need to `round()` them and `str()` them (turn them into strings) so that we can include them in the message.\n\n::: {#ee79ce10 .cell execution_count=24}\n``` {.python .cell-code}\nsns.relplot(df, x = \"age\", y = \"height_cm\")\n\n# Construct the linear regression\nx_lm = df[\"age\"]\ny_lm = lm.slope*x_lm + lm.intercept\n\n# Round and stringify the values\nslope_rounded = str(round(lm.slope, 2))\nintercept_rounded = str(round(lm.intercept, 2))\n\n# Plot the line plot\nlinreg_label = \"Linear regression with\\nslope = \" + slope_rounded + \"\\nintercept = \" + intercept_rounded\nsns.lineplot(x = x_lm, y = y_lm, color = \"r\", label = linreg_label)\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-25-output-1.png){width=470 height=470}\n:::\n:::\n\n\n### $t$-tests\n\nWe can also perform $t$-tests with the `scipy.stats` module. Typically, this is performed to examine the statistical signficance of a difference between two samples' means. Let's examine whether that earlier groupby result for is accurate for heights, specifically, **are goalkeepers taller than non-goalkeepers?**\n\nThe function [`stats.ttest_ind()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html) requires us to send in the two groups as separate columns, so we'll need to do a bit of reshaping.\n\nLet's start by creating a new variable for *goalkeeper status*, and then separate the goalkeepers from the non-goalkeepers in two variables\n\n::: {#42d50ee5 .cell execution_count=25}\n``` {.python .cell-code}\ndf[\"gk\"] = df[\"positions\"] == \"Goalkeeper\"\n\ngoalkeepers = df[df[\"gk\"] == True]\nnon_goalkeepers = df[df[\"gk\"] == False]\n```\n:::\n\n\nThe $t$-test for the means of two independent samples is given by\n\n::: {#a27a7e33 .cell execution_count=26}\n``` {.python .cell-code}\nstats.ttest_ind(goalkeepers[\"height_cm\"], non_goalkeepers[\"height_cm\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\nTtestResult(statistic=35.2144964816995, pvalue=7.551647917141636e-247, df=5930.0)\n```\n:::\n:::\n\n\nYielding a p-value of $8\\times 10^{-247}\\approx 0$, indicating that the null-hypothesis (*heights are the same*) is extremely unlikely.\n\n### Activity 2: Visualisation\n\nWe can also visualise these results with **boxplots**, showing the distributions and their statistical summary. Go to the seaborn documentation for [`sns.catplot()`](https://seaborn.pydata.org/generated/seaborn.catplot.html) to try figure out how.\n\n:::{.callout-note collapse=true}\n# Solution\n\n::: {#078f1714 .cell execution_count=27}\n``` {.python .cell-code}\nsns.catplot(df, x = \"gk\", y = \"height_cm\", kind = \"box\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-28-output-1.png){width=487 height=470}\n:::\n:::\n\n\n:::\n\n### More complex modelling\n\nIf you need to do more advanced statistics, particularly if you need more regressions, you'll likely need to turn to a different package: `statsmodels`. It is particularly useful for **statistical modelling**.\n\nWe'll go through three examples\n\n1. Simple linear regressions (like before)\n2. Multiple linear regressions\n3. Logistic regressions\n\nWhat's nice about `statsmodels` is that it gives an R-like interface and summaries.\n\n#### Simple linear regressions revisited\n\nLet's perform the same linear regression as before, looking at the \"age\" and \"height variables\". Our thinking is that players' heights dictate how long they can play, so we'll make $x = \\text{height}$ and $y = \\text{age}$.\n\nThe first step is to make the set up the variables. We'll use the function `smf.ols()` for ordinary least squares. It takes in two imputs:\n\n* The formula string, in the form `y ~ X1 + X2 ...`\n* The data\n\nWe create the model and compute the fit\n\n::: {#47bac52a .cell execution_count=28}\n``` {.python .cell-code}\nmod = smf.ols(\"height_cm ~ age\", df)\nres = mod.fit()\n```\n:::\n\n\nDone! Let's take a look at the results\n\n::: {#c0039a84 .cell execution_count=29}\n``` {.python .cell-code}\nres.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>        <td>height_cm</td>    <th>  R-squared:         </th> <td>   0.000</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.679</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 30 Oct 2025</td> <th>  Prob (F-statistic):</th>  <td> 0.195</td>  \n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:49:04</td>     <th>  Log-Likelihood:    </th> <td> -19821.</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>  5932</td>      <th>  AIC:               </th> <td>3.965e+04</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>  5930</td>      <th>  BIC:               </th> <td>3.966e+04</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>  182.3826</td> <td>    0.516</td> <td>  353.460</td> <td> 0.000</td> <td>  181.371</td> <td>  183.394</td>\n</tr>\n<tr>\n  <th>age</th>       <td>    0.0258</td> <td>    0.020</td> <td>    1.296</td> <td> 0.195</td> <td>   -0.013</td> <td>    0.065</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>86.537</td> <th>  Durbin-Watson:     </th> <td>   1.995</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td>  56.098</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td>-0.098</td> <th>  Prob(JB):          </th> <td>6.58e-13</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.566</td> <th>  Cond. No.          </th> <td>    151.</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\n\\\nThat's a lot nicer than with scipy. We can also make our plot from before by getting the model's $y$ values with `res.fittedvalues`\n\n::: {#bfacd360 .cell execution_count=30}\n``` {.python .cell-code}\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\nsns.lineplot(x = df[\"age\"], y = res.fittedvalues, color = \"r\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-31-output-1.png){width=470 height=470}\n:::\n:::\n\n\n#### Generalised linear models\n\nThe `statsmodels` module has lots of advanced statistical models available. We'll take a look at one more: Generalised Linear Models. The distributions they include are\n\n* Binomial\n* Poisson\n* Negative Binomial\n* Gaussian (Normal)\n* Gamma\n* Inverse Gaussian\n* Tweedie\n\nWe'll use the *binomial* option to create logistic regressions.\n\nLogistic regressions examine the distribution of binary data. For us, we can compare the heights of **goalkeepers vs non-goalkeepers** again. Let's convert our **gk** column from `True` $\\rightarrow$ `1` and `False` $\\rightarrow$ `0` by converting to an `int`:\n\n::: {#b3dc97c2 .cell execution_count=31}\n``` {.python .cell-code}\ndf[\"gk\"] = df[\"gk\"].astype(int)\n```\n:::\n\n\nNow, we can model this column with height. Specifically,\n\n$$ \\text{gk} \\sim \\text{height}$$\n\nStart by making the model with the function `smf.glm()`. We need to specify the family of distributions; they all live in `sm.families`, which comes from a different submodule that we should import:\n\n::: {#b1447f2a .cell execution_count=32}\n``` {.python .cell-code}\nimport statsmodels.api as sm\nmod = smf.glm(\"gk ~ height_cm\", data = df, family = sm.families.Binomial())\n```\n:::\n\n\nNext, evaluate the results\n\n::: {#97e64510 .cell execution_count=33}\n``` {.python .cell-code}\nres = mod.fit()\n```\n:::\n\n\nLet's have a look at the summary:\n\n::: {#81a002f0 .cell execution_count=34}\n``` {.python .cell-code}\nres.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```{=html}\n<table class=\"simpletable\">\n<caption>Generalized Linear Model Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>gk</td>        <th>  No. Observations:  </th>  <td>  5932</td> \n</tr>\n<tr>\n  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td>  5930</td> \n</tr>\n<tr>\n  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     1</td> \n</tr>\n<tr>\n  <th>Link Function:</th>         <td>Logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n</tr>\n<tr>\n  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -1583.5</td>\n</tr>\n<tr>\n  <th>Date:</th>            <td>Thu, 30 Oct 2025</td> <th>  Deviance:          </th> <td>  3167.0</td>\n</tr>\n<tr>\n  <th>Time:</th>                <td>14:49:07</td>     <th>  Pearson chi2:      </th> <td>4.02e+03</td>\n</tr>\n<tr>\n  <th>No. Iterations:</th>          <td>7</td>        <th>  Pseudo R-squ. (CS):</th>  <td>0.1879</td> \n</tr>\n<tr>\n  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>  -53.2336</td> <td>    1.927</td> <td>  -27.622</td> <td> 0.000</td> <td>  -57.011</td> <td>  -49.456</td>\n</tr>\n<tr>\n  <th>height_cm</th> <td>    0.2745</td> <td>    0.010</td> <td>   26.938</td> <td> 0.000</td> <td>    0.255</td> <td>    0.294</td>\n</tr>\n</table>\n```\n:::\n:::\n\n\nFinally, we can plot the result like before\n\n::: {#b99bbaa9 .cell execution_count=35}\n``` {.python .cell-code}\nsns.relplot(data = df, x = \"height_cm\", y = \"gk\")\nsns.lineplot(x = df[\"height_cm\"], y = res.fittedvalues, color = \"black\")\n```\n\n::: {.cell-output .cell-output-display}\n![](stats_and_vis_files/figure-html/cell-36-output-1.png){width=470 height=470}\n:::\n:::\n\n\n### Activity 3: Digging Deeper\n\nWe've come to the end of the series, so to conclude we'll finish up with an open-ended activity. Like in workshop 2, download the [gapminder](https://uqlibrary.github.io/technology-training/Python/2-data_processing/data/gapminder.csv) dataset.\n\nThen, spend the remaining time analysing and visualising the data! If you're stuck, see if you can produce a visualisation and a statistic that show the same thing.\n\nDon't forget some data transformation tips:\n\n- Use `gapminder = df.read_csv(...)` to load the data\n- Use `gapminder.columns` to see the columns\n- Use `gapminder[\"column_name\"]` to pick out an individual column\n- Use `gapminder[gapminder[\"column_name] == ...]` to filter for a specific value\n\n## Conclusion\n\nPython definitely has powerful tools for statistics and visualisations! If any of\nthe content here was too challenging, you have other related issues\nyou’d like to discuss or would simply like to learn more, we the\ntechnology training team would love to hear from you. You can contact us\nat <training@library.uq.edu.au>.\n\nHere’s a summary of what we’ve covered\n\n| Topic | Description |\n|------------------------------------|------------------------------------|\n| **Descriptive statistics** | Using built-in methods to pandas series (via `df[\"variable\"].___` for a dataframe `df`) we can apply descriptive statistics to our data. |\n| **Using matplotlib to include statistical information** | Using `plt.vlines()` and `plt.fill_between()`, we can annotate the plots with lines showing statistically interesting values. |\n| **Inferential statistics** | Using the `scipy.stats` and `statsmodels` modules, we can perform statistical tests and modelling. |\n\n\n### Resources\n\n- [Official scipy.stats documentation](https://docs.scipy.org/doc/scipy/reference/stats.html)\n- [Official statsmodels documentation](https://www.statsmodels.org/stable/index.html)\n- [Official seaborn documentation](https://seaborn.pydata.org/index.html)\n- [Official matplotlib documentation](https://matplotlib.org/stable/index.html)\n- Our [compilation of useful Python links](https://github.com/uqlibrary/technology-training/blob/master/Python/useful_links.md)\n\n",
    "supporting": [
      "stats_and_vis_files\\figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}