[
  {
    "objectID": "uMap/umap_intro.html",
    "href": "uMap/umap_intro.html",
    "title": "uMap: create quick interactive maps",
    "section": "",
    "text": "This session introduces uMap, an open source, web-based GIS tool useful for quickly creating interactive, shareable maps.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#introduction",
    "href": "uMap/umap_intro.html#introduction",
    "title": "uMap: create quick interactive maps",
    "section": "",
    "text": "This session introduces uMap, an open source, web-based GIS tool useful for quickly creating interactive, shareable maps.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#contents",
    "href": "uMap/umap_intro.html#contents",
    "title": "uMap: create quick interactive maps",
    "section": "Contents",
    "text": "Contents\nWhat this lesson will teach you about:\n\nCreate a map\nChange map background\nCreate different kinds of spatial data\nStyle the data\nManage data into separate layers\nCustomise data interaction\nSave your map, share it and back it up\nModify editing permissions\nImport external data",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#create-a-map",
    "href": "uMap/umap_intro.html#create-a-map",
    "title": "uMap: create quick interactive maps",
    "section": "Create a map",
    "text": "Create a map\n\nGo to https://umap-project.org/\nClick on “Create a map”\nChoose an instance (i.e. a server that offers access to the software)\n\nNo need for account, but it is useful to:\n\nkeep track of all the maps you have created\ndelete maps you don’t need anymore\nfavourite other people’s maps\n\nIf you already have an OpenStreetMap account, you can use it (other providers include GitHub).",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#give-a-title-to-the-map",
    "href": "uMap/umap_intro.html#give-a-title-to-the-map",
    "title": "uMap: create quick interactive maps",
    "section": "Give a title to the map",
    "text": "Give a title to the map\nClick on the “Untitled map” title at the top let, and let’s call this one “My Brisbane” for example.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#change-tilelayers",
    "href": "uMap/umap_intro.html#change-tilelayers",
    "title": "uMap: create quick interactive maps",
    "section": "Change tilelayers",
    "text": "Change tilelayers\n“Tilelayers” are the different backgrounds you can use, also known as “basemaps”.\nClick on the “Change tilelayers” icon in the right-hand-side bar and select a suitable choice. There are detailed ones, but you can also use a minimal one (for example “OSM Dark Matter”) if you want viewers to really focus on your own data.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#centre-the-map",
    "href": "uMap/umap_intro.html#centre-the-map",
    "title": "uMap: create quick interactive maps",
    "section": "Centre the map",
    "text": "Centre the map\nFocus on your area of interest. For that, you can use the Search button in the left toolbar.\nOnce you are happy with the visible range, click the “Save this center and zoom” button (icon with two arrows) in the right toolbar. The map will now always open centred on this location.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#create-data",
    "href": "uMap/umap_intro.html#create-data",
    "title": "uMap: create quick interactive maps",
    "section": "Create data",
    "text": "Create data\nIn the right toolbar, click “add marker” to add point data to the map, and click somewhere on the map. Let’s say this is your place of study or work.\nA “Feature properties” panel opens, in which many options can be changed for the data you placed on the map. For example, try changing the name, description, colour, shape and symbol.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#styling",
    "href": "uMap/umap_intro.html#styling",
    "title": "uMap: create quick interactive maps",
    "section": "Styling",
    "text": "Styling\nNow, place a second point on the map, for example your favourite place to hang out. Notice that it uses the default style again?\nTo make all new points follow a particular style, you can use the “Map advanced properties” button on the right (the cog icon) and go to “Defaut shape properties”. Here, you can define what newly created data looks like on the map. And if you want to, you can reset the first marker to the default style by using the “Clear” buttons in its Shape properties section.\nNow add a different kind of feature: click on “Draw a polygon” on the right toolbar (shortcut is Ctrl + P). Click to create the several nodes that define an area, for example around the UQ lakes, and double-click for the last node. You can drag the polygon to move it, and make it more detailed by adding extra nodes in the middle of segments.\nPolygons can be styled with different properties: notice it has options like opacity, stroke, fill colour and fill opacity that markers did not have.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#layers",
    "href": "uMap/umap_intro.html#layers",
    "title": "uMap: create quick interactive maps",
    "section": "Layers",
    "text": "Layers\nThe points and the polygon you created are all part of the same layer, “Layer 1”. (You can see this information at the top of the Feature properties panel.)\nDepending on the project, you might need several layers to organise your data. This allows to make some layers visible while others are hidden, style them differently, export them as separate files, and define in which order they are rendered.\nClick on “Manage layers” on the right, and then click on the Edit button (pencil icon) for Layer 1. We can give it a name (for example, “Points of interest”), and make the whole layer use a consistent style (just like we did for the whole project before). Any property you set here will be used by default when adding data to this layer.\nNow go back to “Manage layers” (there a left arrow at the top of the panel) and create a new one with “Add a layer”. Name it “Areas”, and modify the UQ Lakes feature so it belongs to this new layer.\nNotes that the “Manage layers” panel allows you to drag and drop the layers to control in which order they are rendered, which is particularly useful for more complex projects.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#save-your-work-and-share-it",
    "href": "uMap/umap_intro.html#save-your-work-and-share-it",
    "title": "uMap: create quick interactive maps",
    "section": "Save your work and share it",
    "text": "Save your work and share it\nClick the “Save” button at the top right of the screen to save your map and give it a unique URL. You can now share this link with others so they can view and explore your map.\nNote that if you don’t have an account, the only way to find your map again if by using this exact URL, so keep it safe! The dialog allows you to send yourself the link as an email.\nWith this URL, the map opens in View mode: we can pan it, click on markers to view more information… Keep in mind that anyone with the link can see your map.\nYou can also click on “Edit” to go back to modifying it: that’s because your session is still active. However, once this link is opened on e.g. another computer, the “Edit” button won’t be there. To go back to editing, either use an account, or save a “secret edit link”.\nOnce the project is saved, you can go back and forth between View and Edit modes.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#permisions",
    "href": "uMap/umap_intro.html#permisions",
    "title": "uMap: create quick interactive maps",
    "section": "Permisions",
    "text": "Permisions\nClick on the “Update permissions and editors” button on the right (the key icon). Here you can see the “secret edit link”. Share this with other editors you want to collaborate with, and save it so you can go back to editing it yourself (if you don’t have an account).\nThis is also were you can change the “Who can edit” value to “Everyone can edit”: in this mode, the map is fully collaborative and can be edited by anyone with the public link.\n\nNote: if your map is saved in your account, the distinction between View link and Edit link does not exist.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#different-feature-types",
    "href": "uMap/umap_intro.html#different-feature-types",
    "title": "uMap: create quick interactive maps",
    "section": "Different feature types",
    "text": "Different feature types\nWe added points and polygons to the map, but spatial data can also take the form of paths.\nTry now to add a “polyline”, to trace a route for example. Make sure it is part of the “Places of interest” layer.\nYou can even create complex “multi” features by using the “Add a polygon” and “Add a polyline” buttons when editing polygon or polyline features.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#import-data",
    "href": "uMap/umap_intro.html#import-data",
    "title": "uMap: create quick interactive maps",
    "section": "Import data",
    "text": "Import data\nWe have created data interactively on the map, but it is possible to import a dataset from a different source.\nThis dataset contains observations of the Australian White Ibis (Threskiornis molucca) around the centre of Brisbane: https://github.com/uqlibrary/technology-training/raw/master/uMap/2024-08-15_ibis-brisbane-filtered.csv\nAfter downloading it, you can import with the following steps:\n\nClick “Import data” button at the bottom of the right toolbar\nBrowse to the CSV file\nGive the new layer a name\nClick “Import data”\n\nAlternatively, you paste the URL directly in the “Choose data” section, which is especially useful if you know the resource will be updated in the future.\nIn the right toolbar, click “Edit map name and caption” (the “i” icon) and add this citation in the Credits:\n\niNaturalist community. Observations of Threskiornis molucca from Brisbane, Australia. Exported from https://www.inaturalist.org on 2024-08-15.\n\nThe default “Drop” marker for the Ibis ocurrence data is not ideal, because of the high number of observations. In the layer’s properties, Shape properties &gt; Icon shape: you can change it to “Circle”, which is already an improvement. However, with a lot of overlapping points, one might like to use alternative visualisations in the “Type of layer” dropdown:\n\nClustered: groups points together by proximity, with a number showing how many are grouped. The groups can be clicked to be expanded.\nHeatmap: a colour gradient depicts the density of points. As expected, the most intensely coloured area is in the City Botanic Gardens. To reveal other areas, we can play with the “Heatmap radius” setting.\n\nNotice in the “Advanced actions” (at the bottom of the Layer panel) that it is possible to clone a layer. For this dataset, it might make sense to overlay a layer of “Categorised” points, so each unique occurence can be seen and interacted with, and one of the properties can be visualised with colours. Let’s name this copy “Ibis observation”, and the other “Ibis density”. “Ibis observation” can now be categorised by e.g. licence or quality.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#interaction-options",
    "href": "uMap/umap_intro.html#interaction-options",
    "title": "uMap: create quick interactive maps",
    "section": "Interaction options",
    "text": "Interaction options\nFor the “Ibis observation” layer, the default interaction popup is not very informative. In the layer’s properties panel, select “Interaction options &gt; Popup content style” and change it to “Table”. Save and view the map: now the popup shows all data associated to the data point, including the direct link to the iNaturalist observation.\nTo go further, one can define a template to use data from the table, for example to build a sentence that adapts to each point. (Layer panel &gt; Interaction options &gt; Popup content template)",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#labels",
    "href": "uMap/umap_intro.html#labels",
    "title": "uMap: create quick interactive maps",
    "section": "Labels",
    "text": "Labels\nWe can add extra polygons to our “Areas” layer to highlight areas of interest, for example around the Mt Coot-tha Botanic Gardens, and the South Bank food court.\nTo always display a label for those features, we can go to: Layer properties panel &gt; Interaction options &gt; Display label &gt; define &gt; always.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#export",
    "href": "uMap/umap_intro.html#export",
    "title": "uMap: create quick interactive maps",
    "section": "Export",
    "text": "Export\nTo guarantee that no data is lost, make sure to export your data and / or project regularly. There are various options depending on how much you want to export:\n\nTo export a single layer’s data, go to: Layer properties panel &gt; Advanced actions &gt; Download. You can copy the text displayed, and save it as a .json file (the format is geojson).\nTo export the visible layers’ data, go to the left toolbar’s “Share and Download” panel. Several formats are available: geojson, gpx, kml, csv. For example, you can import all layers in one go into QGIS with a geojson export.\nTo export the full project, use “full backup” in the same panel, which downloads a “.umap” archive that can later on be imported back into uMap.\nFrom your account dashboard, you can also export all maps at once.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#embed-into-a-website-or-article",
    "href": "uMap/umap_intro.html#embed-into-a-website-or-article",
    "title": "uMap: create quick interactive maps",
    "section": "Embed into a website or article",
    "text": "Embed into a website or article\nThe “Share and Download” panel also provides the code to embed the interactive map into a website or blog article, using an iframe element.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "uMap/umap_intro.html#resources",
    "href": "uMap/umap_intro.html#resources",
    "title": "uMap: create quick interactive maps",
    "section": "Resources",
    "text": "Resources\nTo learn more, consult the uMap guide on the OpenStreetMap wiki. A new version is in development.\nIf you are interested in the technical aspects of hosting the software, use the uMap technical documentation.",
    "crumbs": [
      "Home",
      "![](/images/uMap.svg){width=20} uMap"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UQ Library Technology Training Resources",
    "section": "",
    "text": "This website hosts manuals and resources related to training sessions held at the UQ Library.\nNavigate the left sidebar to find the relevant session, or use the top-right field to search for a topic.\nFurther resources might be listed on the Library website."
  },
  {
    "objectID": "index.html#licence",
    "href": "index.html#licence",
    "title": "UQ Library Technology Training Resources",
    "section": "Licence",
    "text": "Licence\nAll of the information on this website is freely available under the Creative Commons - Attribution 4.0 International Licence unless stated otherwise. You may re-use and re-mix the material in any way you wish, without asking permission, provided you cite the original source. However, we’d love to hear about what you do with it!"
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "UQ Library Technology Training Resources",
    "section": "Contributing",
    "text": "Contributing\nContributions and suggestions are appreciated. Please refer to our README.md for details."
  },
  {
    "objectID": "Voyant/voyant.html",
    "href": "Voyant/voyant.html",
    "title": "Text Analysis with Voyant Tools",
    "section": "",
    "text": "This introductory session focuses on:\n\nimporting a corpus in Voyant Tools\na selection of useful text analysis tools and visualisations\nsome text analysis concepts relevant to the academic setting\nhow to share your insights\n\nYou can use your own files to analyse, or rely on the example corpus provided.",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "Voyant/voyant.html#what-you-will-learn",
    "href": "Voyant/voyant.html#what-you-will-learn",
    "title": "Text Analysis with Voyant Tools",
    "section": "",
    "text": "This introductory session focuses on:\n\nimporting a corpus in Voyant Tools\na selection of useful text analysis tools and visualisations\nsome text analysis concepts relevant to the academic setting\nhow to share your insights\n\nYou can use your own files to analyse, or rely on the example corpus provided.",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "Voyant/voyant.html#voyant-tools",
    "href": "Voyant/voyant.html#voyant-tools",
    "title": "Text Analysis with Voyant Tools",
    "section": "Voyant Tools",
    "text": "Voyant Tools\nVoyant Tools is an Open Source text analysis application that runs in your web browser, originally developed by Stéfan Sinclair (1972-2020) and Geoffrey Rockwell. It can be used to gain insights on a text or collection of texts, by using a combination of tools that look at the data from different angles, using distant reading techniques. It ultimately allows creating a personalised dashboard that highlights the most important aspects of your text analysis, and makes it easy to share those insights with others.\nGiven that it only needs a web browser to run, you can run it on any desktop operating system, offline. However, you also have the option to run it online, on a hosted version like the official website.",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "Voyant/voyant.html#installation",
    "href": "Voyant/voyant.html#installation",
    "title": "Text Analysis with Voyant Tools",
    "section": "Installation",
    "text": "Installation\nYou don’t need to install anything to use Voyant Tools, as it can be used online on the official website. And if this main server is overloaded or not available, there is a list of mirrors you could use instead.\nHowever, if you want to use it offline, on your own computer, you can:\n\nDownload the latest Zip archive\nExtract the archive\nOpen the VoyantServer.jar file\n\nUsing Voyant Tools locally could be useful if you deal with sensitive data, for example, but know that you won’t be able to embed live visualisations on a website for everyone to interact with.",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "Voyant/voyant.html#the-default-dashboard",
    "href": "Voyant/voyant.html#the-default-dashboard",
    "title": "Text Analysis with Voyant Tools",
    "section": "The default dashboard",
    "text": "The default dashboard\nTo load a text files or a corpus (i.e. a collection of texts):\n\nGo to Voyant Tools\nClick the “Open” button\nChoose one of the two example corpora: Jane Austen’s novels or Shakespeare’s plays\nClick “OK”\n\nThis will open the default Voyant Tools dashboard, which includes the following tools:\n\nSummary: some quick statistics about the corpus\nReader: to navigate the text\nCirrus: a word cloud\nTrends: see changes in term use across texts\nContext: see terms in context\n\nLet’s first go through these default tools.",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "Voyant/voyant.html#default-tools",
    "href": "Voyant/voyant.html#default-tools",
    "title": "Text Analysis with Voyant Tools",
    "section": "Default tools",
    "text": "Default tools\nAt any time, you can hover over the question mark to see a short description of the tool or click on it to find out more.\n\n\n\nHovering over the question mark reveals a tool’s short help tooltip.\n\n\nYou can also find extra options by clicking on the options button right next to the question mark. For example, you can change the stopword list used for all or some for the tools.\n\nStopwords are words that are removed before analysing the text. In English, it is common to include words like “the”, “or”, “and”… in the stopword list.\n\n\nSummary\nThe Summary tool gives an overview of the corpus, including:\n\nnumber of documents, words and unique words\ndocument lengths\nvocabulary density: how diverse is the vocabulary of each document\naverage words per sentence\nmost frequent words in the corpus (i.e. overall term frequency)\ndistinctive words: this shows which words make a document differ to others, by a term frequency analysis called TF-IDF.\n\n\n\nReader\nThe Reader tool is not only about reading through the text. You can:\n\nsee the relative size of each document in the bar graph (the area is proportional to the size)\nhover over a word to see the term frequency in the document\nclick on a word to see a distribution graph for the whole corpus\nsearch for particular terms (click the question mark for a more advanced search syntax, e.g. marri* to include terms like “marriage” and “married”)\n\n\nNotice how clicking on a term changes the view in the Trends and Context tools?\n\n\n\nCirrus\nThe Cirrus tool is a word cloud visualisation of the most common terms for the whole corpus.\nYou cant change to a single document by using the “Scale” menu, and change the number of terms shown with the slider.\nBy using the options button, you can change the look of the word cloud, including font and colours.\n\n\nTrends\nThe Trends tool shows how relative term frequencies change between documents. If the documents are ordered chronologically, it can give an idea of a timeline of change in vocabulary.\nBy default, it uses the 5 most common words, but you can change which words are represented by writing them in the search bar and separating them with commas. For example, use this search: prejudic*, trust*, *respect*, pride|proud\n\nIn this syntax, * stands for “any number of characters”, and | stands for “or”.\n\nYou can turn words on and off by clicking on the legend, and change the look of the chart by using the “Display” menu. Hovering over parts of the chart will reveal more information, and clicking will update the view in the Reader and Contexts tools.\nIf you double-click on a part of a plot, you can also “drill down” to see the term frequencies inside the documents, by segmenting them (the number of segments can be changed in the tool’s options).\n\n\nContext\nThe Context tool shows occurences of a term in context, which can be useful to explore how a particular word is used in a document.\nThe “context” and “expand” sliders define how many words are shown around the term, in the single row view and the expanded view respectively. (Click on the + symbol to expand the context.)",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "Voyant/voyant.html#extra-tools",
    "href": "Voyant/voyant.html#extra-tools",
    "title": "Text Analysis with Voyant Tools",
    "section": "Extra tools",
    "text": "Extra tools\nVoyant tools offers more than 20 tools to explore corpora. This tutorial is not designed to have a thorough overview of every single one of the tools, but here are a few extra tools that might come in handy, and that give an idea of the breadth of features Voyant Tools offers.\nTo use extra tools, either click the names above each one of the default tools, or use the tool menu in the toolbar (pictured).\n\n\n\nReplace a tool by another one with this menu.\n\n\n\nLinks\nThe Links tool can be found above the Cirrus, or in the “Tools - Visualization Tools” menu.\nIt represents a network of frequent words and some of their collocates (i.e. terms that appear in proximity, by default 5 words either side). We can change the keywords (shown in blue) with the search box, and their collocates appear in orange.\nHovering over keywords shows the frequency in the corpus, whereas hovering over collocates shows their frequency in proximity.\nYou can double-click on a term to show more collocates, and right-click on it for more options.\n\n\nPhrases\nThe Phrases tool can be found above the Summary, or in the “Tools - Corpus Tools” menu.\nIt allows you to define maximum and minimum lengths of phrases to reveal the most common ones in the corpus. Try for example to search for the most common 6-term phrases in the corpus.",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "Voyant/voyant.html#import-new-data",
    "href": "Voyant/voyant.html#import-new-data",
    "title": "Text Analysis with Voyant Tools",
    "section": "Import new data",
    "text": "Import new data\nInstead of using one of the default corpora, you can import your own documents. TXT, HTML, XML, JSON, XLSX, ODS, CSV are some of the supported formats.\nThe options you have for each format are described in the “Creating a Corpus” documentation page.\nYou can upload several files at once, by either doing a multiple selection or archiving them in one single ZIP file.\n\nChallenge: import a book from Project Gutenberg\n\nGo to the Project Gutenberg website to find Public Domain works\nDownload a book in TXT format\nDoes this file need a bit of cleaning up before analysing?\n“Upload” it in Voyant Tools\nExplore!",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "Voyant/voyant.html#sharing",
    "href": "Voyant/voyant.html#sharing",
    "title": "Text Analysis with Voyant Tools",
    "section": "Sharing",
    "text": "Sharing\nYou can decide to share:\n\nA link to a single view (i.e. one tool)\nA link to the whole dashboard\nA static image (PNG or SVG)",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "Voyant/voyant.html#resources",
    "href": "Voyant/voyant.html#resources",
    "title": "Text Analysis with Voyant Tools",
    "section": "Resources",
    "text": "Resources\n\nOfficial Voyant Tools documentation, including guides and tool descriptions\nHermeneutica: Computer-Assisted Interpretation in the Humanities (Rockwell and Sinclair, 2016), a book using Voyant Tools extensively\nMany tutorials on analysing language and text with R on LADAL\nDiscover hundreds of other text analysis tools with TAPoR",
    "crumbs": [
      "Home",
      "![](/images/voyant.png){width=20} Voyant Tools"
    ]
  },
  {
    "objectID": "R/usefullinks.html",
    "href": "R/usefullinks.html",
    "title": "Further resources for R users",
    "section": "",
    "text": "R and RStudio-related cheatsheets.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "Further resources"
    ]
  },
  {
    "objectID": "R/usefullinks.html#cheatsheets",
    "href": "R/usefullinks.html#cheatsheets",
    "title": "Further resources for R users",
    "section": "",
    "text": "R and RStudio-related cheatsheets.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "Further resources"
    ]
  },
  {
    "objectID": "R/usefullinks.html#tutorials-manuals-and-books",
    "href": "R/usefullinks.html#tutorials-manuals-and-books",
    "title": "Further resources for R users",
    "section": "Tutorials, manuals and books",
    "text": "Tutorials, manuals and books\n\nR for Data Science, excellent free online book by Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund\nTwo introductory materials to R by The Carpentries: R for Reproducible Scientific Analysis and Data Analysis and Visualization in R for Ecologists\nWhy use R over Excel for your analyses? This post provides arguments and a short introduction to R: _Excel vs R: A Brief Introduction to R\nRStudio Education lists useful resources\nCRAN’s basic and advanced manuals\nLots of quality tutorials on STHDA\nDataCamp’s R documentation\nLots of quality tutorials on Cookbook for R\nLinkedIn Learning R courses (use your UQ credentials)\nMany excellent books on Bookdown.org\nVarious tutorials by LADAL, many focused on language analysis",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "Further resources"
    ]
  },
  {
    "objectID": "R/usefullinks.html#practice",
    "href": "R/usefullinks.html#practice",
    "title": "Further resources for R users",
    "section": "Practice",
    "text": "Practice\n\nSolve challenges and learn from other people’s solutions on Exercism\n{swirl} is a package that allows you to learn R interactively in an R session\nThe {learnr} package integrates into RStudio’s “Tutorial” tab\n\n\nData visualisation\n\nThe R Graph Gallery for categorised examples and code\nInteractive web-based data visualization with R, plotly, and shiny, free online book by Carson Sievert",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "Further resources"
    ]
  },
  {
    "objectID": "R/usefullinks.html#questions-and-answers",
    "href": "R/usefullinks.html#questions-and-answers",
    "title": "Further resources for R users",
    "section": "Questions and answers",
    "text": "Questions and answers\n\nQuestions about R programming on StackOverflow\nStatistics questions using R on CrossValidated",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "Further resources"
    ]
  },
  {
    "objectID": "R/usefullinks.html#documentation",
    "href": "R/usefullinks.html#documentation",
    "title": "Further resources for R users",
    "section": "Documentation",
    "text": "Documentation\n\nPackages from CRAN and Bioconductor on Rdocumentation\nPackages from CRAN, Bioconductor, R-Forge and GitHub (and run R code online) on rdrr.io",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "Further resources"
    ]
  },
  {
    "objectID": "R/usefullinks.html#r-news",
    "href": "R/usefullinks.html#r-news",
    "title": "Further resources for R users",
    "section": "R news",
    "text": "R news\n\nDaily news and tutorials on R-bloggers\nWeekly digest of R news on R Weekly\nTidyverse (and Tidymodels) news",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "Further resources"
    ]
  },
  {
    "objectID": "R/usefullinks.html#at-uq",
    "href": "R/usefullinks.html#at-uq",
    "title": "Further resources for R users",
    "section": "At UQ",
    "text": "At UQ\n\nSee the next sessions at the Library\nJoin the monthly UQ R User Group (UQRUG) to collaborate and share with other R users\nAsk questions to other researchers during the weekly Hacky Hour\nContact your unit’s statistician",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "Further resources"
    ]
  },
  {
    "objectID": "R/usefullinks.html#training-outside-the-library",
    "href": "R/usefullinks.html#training-outside-the-library",
    "title": "Further resources for R users",
    "section": "Training outside the Library",
    "text": "Training outside the Library\n\nFind more training providers, at and outside UQ, in our “Training Elsewhere” page",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "Further resources"
    ]
  },
  {
    "objectID": "R/shiny/shiny.html",
    "href": "R/shiny/shiny.html",
    "title": "R advanced: webapps with Shiny",
    "section": "",
    "text": "Shiny is a package that allows to create a web application with R code.\nA Shiny app requires two main elements:\n\na user interface (UI)\na server\n\nLet’s build an app from scratch, using our ACORN data and functions.\nWhat we want to create is a small webapp that visualises Australian temperature data and gives the user a bit of control over the visualisation.\n\n\n\n\nWe will first download our base project that contains custom functions to get our data ready.\n\nDownload the project archive, and extract it wherever you’d like to store your project.\nOpen the .Rproj file\nCreate a new script: “New File &gt; R Script”\n\n\n\n\nWe can source our custom functions that make it easier for us to download the ACORN data and merge all the datasets into one big file:\n\nsource(\"get_acorn.R\")\nsource(\"read_station.R\")\nsource(\"merge_acorn.R\")\nget_acorn(\"acorn_data\")\nlibrary(tidyverse)\nall_stations &lt;- merge_acorn(\"acorn_data\")\n\nWe now have a single object that contains data from 112 weather stations around Australia.\n\n\n\n\nIn our project, let’s create a new app with “File &gt; New File &gt; Shiny Web App…”. We will stick to “single file”, and the current project directory as the location.\nIn our files, we can now see a “myApp” directory that contains an “app.R” script.\nThe app is currently an example app. We can run it with the “Run App” button, and you can see what kind of interaction a basic Shiny app can offer: a slider to change the number of bins in a histogram, for example.\n\n\n\nFor our app to work, we need three sections:\n\ndefine a UI: what users see\ndefine a server: what happens in the background\ndefine how the app is run\n\nBack in the app.R file, we can start with this empty skeleton:\n\n# Load necessary packages\nlibrary(shiny)\n\n# UI\nui &lt;- fluidPage()\n\n# Server\nserver &lt;- function(input, output) {}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nRunning it will show a blank page. Let’s add a title:\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"ACORN data explorer\")\n)\n\n\n\n\nNow, let’s make sure we have the data ready to be used in our app. We don’t want to do the summarising of our data every time we run the app, so let’s save the finished product into an RDS file. Back in our first script, let’s write:\n\n# process for monthly average\nmonthly &lt;- all_stations %&gt;% \n    group_by(month = month(date),\n             year = year(date)) %&gt;% \n    summarise(mean.max = mean(max.temp, na.rm = TRUE))\n\nLet’s save that object into our app directory, so the app can find it:\n\nsaveRDS(monthly, \"myApp/monthly.rds\")\n\nThis dataset will be the base of our Shiny app.\n\n\n\nWe can now read that data file into our app, process it, and present it in an interactive table, using the DT package:\n\n# Import data\nmonthly &lt;- readRDS(\"monthly.rds\")\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\n\n# Define UI\nui &lt;- fluidPage(\n    titlePanel(\"ACORN data explorer\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n}\n\nNotice that we had to define an output in the server section (with a “render” function), and use that output in a UI function (with an “output” function).\n\n\nNow, for a different kind of output, let’s add a plot:\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\n\n# Define UI\nui &lt;- fluidPage(\n    titlePanel(\"ACORN data explorer\"),\n    plotOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n    \n    output$plot &lt;- renderPlot({\n            ggplot(monthly,\n               aes(x = year, y = month, fill = mean.max)) +\n            geom_tile() +\n            scale_fill_distiller(palette = \"RdYlBu\")\n    })\n}\n\nAgain, we have to:\n\nDefine how the plot is generated on the server\nSave the plot as an output, using the right render* function\nShow the plot in the UI with the right *Output function\n\n\n\n\nHow can we add some interaction? We could give the user control over which month they want to visualise by adding a slider:\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Define UI\nui &lt;- fluidPage(\n    titlePanel(\"ACORN data explorer\"),\n    # input slider for months\n    sliderInput(\"month\",\n                \"Pick a month:\",\n                min = 1,\n                max = 12,\n                value = 1),\n    plotOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n    \n    output$plot &lt;- renderPlot({\n        monthly %&gt;% \n            filter(month == input$month) %&gt;% \n            ggplot(aes(x = year, y = month, fill = mean.max)) +\n            geom_tile() +\n            scale_fill_distiller(palette = \"RdYlBu\")\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n\n\nHow could we give the option to go back to the full-year view?\nHint: have a look at ?selectInput, or find other ideas on this list: https://shiny.rstudio.com/tutorial/written-tutorial/lesson3/\nOne solution could be:\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n    titlePanel(\"ACORN data explorer\"),\n    # input slider for months\n    selectInput(\"month\",\n                \"Pick one or more months:\",\n                1:12,\n                multiple = TRUE),\n    plotOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n    \n    output$plot &lt;- renderPlot({\n        monthly %&gt;% \n            filter(month %in% input$month) %&gt;% \n            ggplot(aes(x = year, y = month, fill = mean.max)) +\n            geom_tile() +\n            scale_fill_distiller(palette = \"RdYlBu\")\n    })\n}\n\n\n\n\nTo change the theme of the app, we can use the bslib package, and change the theme argument in fluidPage():\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(bslib)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n    theme = bs_theme(bootswatch = \"solar\"),\n    titlePanel(\"ACORN data explorer\"),\n    # input slider for months\n    selectInput(\"month\",\n                \"Pick one or more months:\",\n                1:12,\n                multiple = TRUE),\n    plotOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\nYou can see the different themes available with the bootswatch_themes() function.\nThis is great to quickly change the general look of our app, but our visualisation looks out of place: how can we also change the theme for ggplot2? Let’s use the convenient thematic package:\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(bslib)\nlibrary(thematic)\nthematic_shiny()\n\nNow, the theme propagates to ggplot2 visualisations.\n\n\n\nUsing the plotly package, how could you make the plot interactive?\nRemember to change the code that generates the plot as well as the render and output functions.\n\n# import data\nmonthly &lt;- readRDS(\"monthly.rds\")\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(bslib)\nlibrary(thematic)\nthematic_shiny()\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n    theme = bs_theme(bootswatch = \"solar\"),\n    titlePanel(\"ACORN data explorer\"),\n    # input slider for months\n    selectInput(\"month\",\n                \"Pick one or more months:\",\n                1:12,\n                multiple = TRUE),\n    plotlyOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n    \n    output$plot &lt;- renderPlotly({\n        p &lt;- monthly %&gt;% \n            filter(month %in% input$month) %&gt;%\n            ggplot(aes(x = year, y = month, fill = mean.max)) +\n            geom_tile() +\n            scale_fill_distiller(palette = \"RdYlBu\")\n        ggplotly(p)\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nThe user can now hover over parts of the plot to see the corresponding data.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "8. Shiny web apps"
    ]
  },
  {
    "objectID": "R/shiny/shiny.html#shiny-webapps",
    "href": "R/shiny/shiny.html#shiny-webapps",
    "title": "R advanced: webapps with Shiny",
    "section": "",
    "text": "Shiny is a package that allows to create a web application with R code.\nA Shiny app requires two main elements:\n\na user interface (UI)\na server\n\nLet’s build an app from scratch, using our ACORN data and functions.\nWhat we want to create is a small webapp that visualises Australian temperature data and gives the user a bit of control over the visualisation.\n\n\n\n\nWe will first download our base project that contains custom functions to get our data ready.\n\nDownload the project archive, and extract it wherever you’d like to store your project.\nOpen the .Rproj file\nCreate a new script: “New File &gt; R Script”\n\n\n\n\nWe can source our custom functions that make it easier for us to download the ACORN data and merge all the datasets into one big file:\n\nsource(\"get_acorn.R\")\nsource(\"read_station.R\")\nsource(\"merge_acorn.R\")\nget_acorn(\"acorn_data\")\nlibrary(tidyverse)\nall_stations &lt;- merge_acorn(\"acorn_data\")\n\nWe now have a single object that contains data from 112 weather stations around Australia.\n\n\n\n\nIn our project, let’s create a new app with “File &gt; New File &gt; Shiny Web App…”. We will stick to “single file”, and the current project directory as the location.\nIn our files, we can now see a “myApp” directory that contains an “app.R” script.\nThe app is currently an example app. We can run it with the “Run App” button, and you can see what kind of interaction a basic Shiny app can offer: a slider to change the number of bins in a histogram, for example.\n\n\n\nFor our app to work, we need three sections:\n\ndefine a UI: what users see\ndefine a server: what happens in the background\ndefine how the app is run\n\nBack in the app.R file, we can start with this empty skeleton:\n\n# Load necessary packages\nlibrary(shiny)\n\n# UI\nui &lt;- fluidPage()\n\n# Server\nserver &lt;- function(input, output) {}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nRunning it will show a blank page. Let’s add a title:\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"ACORN data explorer\")\n)\n\n\n\n\nNow, let’s make sure we have the data ready to be used in our app. We don’t want to do the summarising of our data every time we run the app, so let’s save the finished product into an RDS file. Back in our first script, let’s write:\n\n# process for monthly average\nmonthly &lt;- all_stations %&gt;% \n    group_by(month = month(date),\n             year = year(date)) %&gt;% \n    summarise(mean.max = mean(max.temp, na.rm = TRUE))\n\nLet’s save that object into our app directory, so the app can find it:\n\nsaveRDS(monthly, \"myApp/monthly.rds\")\n\nThis dataset will be the base of our Shiny app.\n\n\n\nWe can now read that data file into our app, process it, and present it in an interactive table, using the DT package:\n\n# Import data\nmonthly &lt;- readRDS(\"monthly.rds\")\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\n\n# Define UI\nui &lt;- fluidPage(\n    titlePanel(\"ACORN data explorer\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n}\n\nNotice that we had to define an output in the server section (with a “render” function), and use that output in a UI function (with an “output” function).\n\n\nNow, for a different kind of output, let’s add a plot:\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\n\n# Define UI\nui &lt;- fluidPage(\n    titlePanel(\"ACORN data explorer\"),\n    plotOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n    \n    output$plot &lt;- renderPlot({\n            ggplot(monthly,\n               aes(x = year, y = month, fill = mean.max)) +\n            geom_tile() +\n            scale_fill_distiller(palette = \"RdYlBu\")\n    })\n}\n\nAgain, we have to:\n\nDefine how the plot is generated on the server\nSave the plot as an output, using the right render* function\nShow the plot in the UI with the right *Output function\n\n\n\n\nHow can we add some interaction? We could give the user control over which month they want to visualise by adding a slider:\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Define UI\nui &lt;- fluidPage(\n    titlePanel(\"ACORN data explorer\"),\n    # input slider for months\n    sliderInput(\"month\",\n                \"Pick a month:\",\n                min = 1,\n                max = 12,\n                value = 1),\n    plotOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n    \n    output$plot &lt;- renderPlot({\n        monthly %&gt;% \n            filter(month == input$month) %&gt;% \n            ggplot(aes(x = year, y = month, fill = mean.max)) +\n            geom_tile() +\n            scale_fill_distiller(palette = \"RdYlBu\")\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n\n\n\n\nHow could we give the option to go back to the full-year view?\nHint: have a look at ?selectInput, or find other ideas on this list: https://shiny.rstudio.com/tutorial/written-tutorial/lesson3/\nOne solution could be:\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n    titlePanel(\"ACORN data explorer\"),\n    # input slider for months\n    selectInput(\"month\",\n                \"Pick one or more months:\",\n                1:12,\n                multiple = TRUE),\n    plotOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n    \n    output$plot &lt;- renderPlot({\n        monthly %&gt;% \n            filter(month %in% input$month) %&gt;% \n            ggplot(aes(x = year, y = month, fill = mean.max)) +\n            geom_tile() +\n            scale_fill_distiller(palette = \"RdYlBu\")\n    })\n}\n\n\n\n\nTo change the theme of the app, we can use the bslib package, and change the theme argument in fluidPage():\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(bslib)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n    theme = bs_theme(bootswatch = \"solar\"),\n    titlePanel(\"ACORN data explorer\"),\n    # input slider for months\n    selectInput(\"month\",\n                \"Pick one or more months:\",\n                1:12,\n                multiple = TRUE),\n    plotOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\nYou can see the different themes available with the bootswatch_themes() function.\nThis is great to quickly change the general look of our app, but our visualisation looks out of place: how can we also change the theme for ggplot2? Let’s use the convenient thematic package:\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(bslib)\nlibrary(thematic)\nthematic_shiny()\n\nNow, the theme propagates to ggplot2 visualisations.\n\n\n\nUsing the plotly package, how could you make the plot interactive?\nRemember to change the code that generates the plot as well as the render and output functions.\n\n# import data\nmonthly &lt;- readRDS(\"monthly.rds\")\n\n# Load necessary packages\nlibrary(shiny)\nlibrary(DT)\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(dplyr)\nlibrary(bslib)\nlibrary(thematic)\nthematic_shiny()\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n    theme = bs_theme(bootswatch = \"solar\"),\n    titlePanel(\"ACORN data explorer\"),\n    # input slider for months\n    selectInput(\"month\",\n                \"Pick one or more months:\",\n                1:12,\n                multiple = TRUE),\n    plotlyOutput(\"plot\"),\n    DTOutput(\"dt\")\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n    output$dt &lt;- renderDT({\n        monthly\n    })\n    \n    output$plot &lt;- renderPlotly({\n        p &lt;- monthly %&gt;% \n            filter(month %in% input$month) %&gt;%\n            ggplot(aes(x = year, y = month, fill = mean.max)) +\n            geom_tile() +\n            scale_fill_distiller(palette = \"RdYlBu\")\n        ggplotly(p)\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\nThe user can now hover over parts of the plot to see the corresponding data.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "8. Shiny web apps"
    ]
  },
  {
    "objectID": "R/shiny/shiny.html#publishing-a-shiny-app",
    "href": "R/shiny/shiny.html#publishing-a-shiny-app",
    "title": "R advanced: webapps with Shiny",
    "section": "Publishing a Shiny app",
    "text": "Publishing a Shiny app\nYou can use ShinyApps.io, which offers free or paid accounts. This is integrated into RStudio to easily deploy and updae your applications.\nWe also have access to ARDC’s Nectar (National eResearch Collaboration Tools and Resources project), in which we can request a virtual machine and deploy a Shiny server.\nOther options exist, see for example this comparison table.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "8. Shiny web apps"
    ]
  },
  {
    "objectID": "R/shiny/shiny.html#useful-links",
    "href": "R/shiny/shiny.html#useful-links",
    "title": "R advanced: webapps with Shiny",
    "section": "Useful links",
    "text": "Useful links\n\nOfficial Shiny tutorial\nGallery of Shiny examples\nHadley Wickham’s book Mastering Shiny\nShiny cheatsheet",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "8. Shiny web apps"
    ]
  },
  {
    "objectID": "R/reports/reports.html",
    "href": "R/reports/reports.html",
    "title": "R reproducible reports with Quarto",
    "section": "",
    "text": "If you don’t have R and RStudio installed already, we have installation instructions\n\n\nIf you are using your own laptop please open RStudio\n\nMake sure you have a working Internet connection\n\nOn the Library’s training computers:\n\nLog in with your UQ username and password\nMake sure you have a working Internet connection\nOpen the ZENworks application\nLook for “RStudio”\nDouble click on RStudio, which will install both R and RStudio\n\n\nRecent versions of RStudio already include Quarto.\nWith RStudio open, let’s make sure we have the necessary packages installed by running this command (this might take a few minutes):\n\ninstall.packages(c(\"tidyverse\", \"plotly\", \"htmltools\"))\n\nThis will install the Tidyverse packages, the plotly package for interactive visualisations, and the htmltools package for having all bases covered for rendering HTML documents.\n\nIf R asks about installing a binary or building from source, pick the binary option: it will be faster!",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#setting-up",
    "href": "R/reports/reports.html#setting-up",
    "title": "R reproducible reports with Quarto",
    "section": "",
    "text": "If you don’t have R and RStudio installed already, we have installation instructions\n\n\nIf you are using your own laptop please open RStudio\n\nMake sure you have a working Internet connection\n\nOn the Library’s training computers:\n\nLog in with your UQ username and password\nMake sure you have a working Internet connection\nOpen the ZENworks application\nLook for “RStudio”\nDouble click on RStudio, which will install both R and RStudio\n\n\nRecent versions of RStudio already include Quarto.\nWith RStudio open, let’s make sure we have the necessary packages installed by running this command (this might take a few minutes):\n\ninstall.packages(c(\"tidyverse\", \"plotly\", \"htmltools\"))\n\nThis will install the Tidyverse packages, the plotly package for interactive visualisations, and the htmltools package for having all bases covered for rendering HTML documents.\n\nIf R asks about installing a binary or building from source, pick the binary option: it will be faster!",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#what-are-we-going-to-learn",
    "href": "R/reports/reports.html#what-are-we-going-to-learn",
    "title": "R reproducible reports with Quarto",
    "section": "What are we going to learn?",
    "text": "What are we going to learn?\nR is a great tool to go from importing to reporting. Today, we focus on the “reporting” part.\nUsing R, RStudio, the Markdown syntax and the Quarto publishing system, we can create reproducible reports that mix code and prose. If the underlying data changes, we only need to replace the original data file and “render” the report once more, which updates all its contents in one click.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#create-a-quarto-project",
    "href": "R/reports/reports.html#create-a-quarto-project",
    "title": "R reproducible reports with Quarto",
    "section": "Create a Quarto project",
    "text": "Create a Quarto project\nUse the project menu (top right) to create a “New project…”. Let’s select the project type “Quarto Project” and name this one “reports”.\nThis kind of project automatically creates a .qmd file for us, but if you have an existing R project and want to add a report to it, you can also create only that file with: “File &gt; New File &gt; Quarto Document…”. If you create a single file, you can use the dialog to choose the title of the report, and add the author. For a generic report, the HTML output is the most versatile and the one we use for this tutorial (and it is Quarto’s default anyway).",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#quarto-document-structure-and-rendering",
    "href": "R/reports/reports.html#quarto-document-structure-and-rendering",
    "title": "R reproducible reports with Quarto",
    "section": "Quarto document structure and rendering",
    "text": "Quarto document structure and rendering\nSee how the document is already populated with a template? Scroll through and have a look at how it is structured. The three main elements are:\n\na YAML header at the top, between the --- tags;\nMarkdown sections, where we can write prose, format text and add headings;\nand code chunks, in between ``` where we can write R code.\n\nBut before we edit this document, let’s go straight to the “Render” button at the top of the source panel. Clicking that button will compile a document from the Quarto file. You should see the process unfolding in the “Background Jobs” tab, and the HTML document pop up in a separate window when it is finished.\nSee how the document contains a title, headers, code input and output, and formatted paragraphs?",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#a-note-on-r-markdown",
    "href": "R/reports/reports.html#a-note-on-r-markdown",
    "title": "R reproducible reports with Quarto",
    "section": "A note on R Markdown",
    "text": "A note on R Markdown\nR Markdown was the predecessor to Quarto. If you have existing .Rmd files, you should be able to move them to Quarto with no or very minimal modification. That being said, R Markdown is here to stay and you can keep using it independently from Quarto.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#editing-the-document",
    "href": "R/reports/reports.html#editing-the-document",
    "title": "R reproducible reports with Quarto",
    "section": "Editing the document",
    "text": "Editing the document\nLet’s remove everything below our YAML header, and start writing our own report!\nNote that by default, the editor uses the “Visual” mode. We can change it to “Source” for now, using the editor toolbar, to familiarise ourselves with the syntax.\n\nMarkdown syntax\nMarkdown is a markup language that allows us to format text with simple tags, similarly to HTML or LaTeX.\nTo add a heading, we can start a line with ##: this will be a heading of level 2. The number of hash symbols corresponds to the level of the header.\nWe are going to deal with greenhouse gas emissions for Australia, so let’s add a header and some text about the source of the data and how to import it. For example:\n## National Greenhouse Gas data\n\nOur data is extracted from the _National Inventory by Economic Sector_ dataset, which is part of the _National Greenhouse Accounts_, and is released under a [CC-BY](https://creativecommons.org/licenses/by/4.0/) licence. The latest release can be found on [this page](https://www.dcceew.gov.au/climate-change/publications/national-greenhouse-accounts-2022/national-inventory-by-economic-sector-2022).\n\nThe values are reported in Mt CO&lt;sub&gt;2&lt;/sub&gt;-e.\nNotice how we used a [text](link) syntax to add a link to a website?\n\nChallenge 1\nWe can also style our text by surrounding with other tags:\n\n** for bold\n* for italic\n\nTry to style your text, and add a heading of level 3 for a section on “importing the data”. Render the document to see if it works!\n\n\n\nR code chunks\nWe can now add a code chunk to include some R code inside our reproducible document. To add a code chunk, click the “Insert a new code chunk” button at the top of the source panel, and click “R”. You can see that the language of the code chunk is defined at the top, with {r} (but other languages, like Python, are also supported in Quarto).\n```{r}\ncode_goes_here\n```\nLet’s import the Tidyverse, by including this code in the chunk:\n\nlibrary(tidyverse)\n\nNotice that you can run your chunks of code one by one by clicking the green “play” button at the right of the chunk: you don’t have to render the whole document every time you want to test your code.\nNow, try to render the document and see what it looks like.\n\nChallenge 2\nInside a new chunk, add some code to import the dataset located here into an object called ghg.\n\nghg &lt;- read_csv(\"https://raw.githubusercontent.com/uqlibrary/technology-training/master/R/reports/aus_ghg_2019.csv\")\n\n\nClicking “Render” will automatically save your .qmd file as well as the HTML output.\n\nNow, we can add a chunk to show the data, by including this code in it:\n\nghg\n\n\n  \n\n\n\n\n\n\nWorking directory\nNote that the working directory for a Quarto document will be the .qmd file’s location by default (and not necessarily the working directory of the R project your are in). That is why it is a good idea to save your Quarto file at the top of your project directory if you want consistency between your scripts and your Quarto file.\nIn our example, we load a CSV file from the Internet, but if we had a data file stored locally, it is important to keep that in mind.\n\n\nChunk options\nNotice how our two first chunks show some messages as an output? We might want to remove that if it is not important and we don’t want to include it in the report. At the top of your chunk, you can use code chunk options like so:\n```{r}\n#| option1: value\n#| option2: value\ncode_goes_here\n```\nFor example, include this option to hide messages in the output:\n#| message: false\nThe code will be executed and the output (if there is any) will be shown, but the messages won’t!\nThere are various options to choose from, depending on what you want to do and show with your chunk of code. For example, to hide both messages and warnings, and only show the output of the code (without showing the underlying code), you can use these options, separated by commas:\n#| message: false\n#| warning: false\n#| echo: false\nIt also is a good idea to label your chunks, especially in longer documents, so you can spot issues more easily. It won’t be shown in the report, but will be used in the console and can be used to navigate your script (with the dropdown menu at the bottom of the source panel). For example, for our first chunk:\n#| label: load-packages\n#| message: false\nThese labels also allow you to use cross-references when generating plots.\nIt is also possible to include options in the YAML header at the top of the document, to set default options you want to use for all you chunks. That is particularly useful if you want to define a default size for all your figures, for example.\nHere is an example of “execute” options you might use in your YAML header:\n---\ntitle: \"My Report\"\nexecute:\n  echo: true\n  message: false\n  warning: false\n---\nThat would make sure that, by default:\n\nThe code is shown, but\nthe messages and warnings are hidden.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#errors-when-rendering",
    "href": "R/reports/reports.html#errors-when-rendering",
    "title": "R reproducible reports with Quarto",
    "section": "Errors when rendering",
    "text": "Errors when rendering\nIt should be straight forward to find where an issue comes from when rendering a report does not work.\n\nChallenge 3\n\nTry changing a chunk code so the code is not valid.\nWhat can you see in the console? Which parts are helpful to identify the issue?",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#tidy-the-data",
    "href": "R/reports/reports.html#tidy-the-data",
    "title": "R reproducible reports with Quarto",
    "section": "Tidy the data",
    "text": "Tidy the data\nLet’s keep populating our report with more code. Our data is not respecting the tidy data principles, so let’s fix that first with a tidyr function:\n\nghg_tidy &lt;- pivot_longer(ghg,\n                         -year,\n                         names_to = \"sector\",\n                         values_to = \"emissions\")",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#inline-code",
    "href": "R/reports/reports.html#inline-code",
    "title": "R reproducible reports with Quarto",
    "section": "Inline code",
    "text": "Inline code\nWe can also include code that will be executed inside Markdown text. For example, you can write the following sentence:\n\nThe dataset contains GHG emissions for the period `{r} min(ghg$year)` to `{r} max(ghg$year)`. The maximum GHG emissions recorded for the mining sector is `{r} round(max(ghg$Mining), 2)` Mt CO2-e.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#visualisation",
    "href": "R/reports/reports.html#visualisation",
    "title": "R reproducible reports with Quarto",
    "section": "Visualisation",
    "text": "Visualisation\nWe can also include a visualisation using, for example, ggplot2:\n\nggplot(ghg_tidy, aes(x = year, y = emissions, colour = sector)) +\n  geom_line() +\n  ylab(\"emissions (Mt CO2e)\")\n\n\n\n\n\n\n\n\n\nIf you want to hide the code that created an output, like for this plot, you can add the option #| echo: false to it.\n\nFinally, let’s create an interactive version of our plot:\n\nlibrary(plotly)\np &lt;- ggplot(ghg_tidy, aes(x = year, y = emissions, colour = sector)) +\n  geom_line() +\n  ylab(\"emissions (Mt CO2e)\")\nggplotly(p)\n\nThis will work in a HTML document, but will most likely fail in other output formats.\nIf you want to change the size of your visualisations, you can tweak the width and height with chunk options like fig-width: 8. However, to make that consistent for all your figures, better use an extra default option in the YAML header. For example:\nformat: \n  html:\n    fig-width: 8\n    fig-height: 6",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#update-the-report",
    "href": "R/reports/reports.html#update-the-report",
    "title": "R reproducible reports with Quarto",
    "section": "Update the report",
    "text": "Update the report\nWe have an updated version of the dataset. The only thing we need to do to update the whole report is point the data import code to the new file, at the top of our document, changing the year to “2022”:\n\nghg &lt;- read_csv(\"https://raw.githubusercontent.com/uqlibrary/technology-training/master/R/reports/aus_ghg_2022.csv\")\n\nRendering again will update all the objects and visualisations for us! This is the power of reproducible reports in R.\nWith reproducible reports, you can potentially structure and write (most of) a report even before you have your research project’s final dataset. (Well, at least the data analysis part, maybe not so much the conclusions!)",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#output-formats",
    "href": "R/reports/reports.html#output-formats",
    "title": "R reproducible reports with Quarto",
    "section": "Output formats",
    "text": "Output formats\nThe benefits of using HTML documents are multiple:\n\nfigures won’t break the flow of the document by jumping to the next page and leaving a large blank space;\nyou can include interactive visualisations making use of the latest HTML features;\nthey can be directly integrated into a website.\n\nHowever, other output formats are available. Here are some examples for the format value in your YAML header:\n\npdf for a non-editable, widespread, portable format\ndocx and odt to open and edit with Microsoft Word and LibreOffice Writer\ngfm for a Markdown file that can easily be published on GitHub or GitLab\ndashboard for a card-based dashboard\nand more, including for creating slides.\n\n\nRendering to PDF\nIn some cases, you might be required to share your report as a PDF. Rendering your document to PDF can generate very professional-looking reports, but it will require having extra software on your computer.\nYou can install the necessary LaTeX packages with an R package called TinyTeX, which is a great alternative to very big LaTeX distributions that can be several gigabytes-big.\nIn a terminal, run the following:\nquarto install tinytex\nAfter this, try to change your YAML header’s format value to pdf and render it.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/reports/reports.html#useful-links",
    "href": "R/reports/reports.html#useful-links",
    "title": "R reproducible reports with Quarto",
    "section": "Useful links",
    "text": "Useful links\nRelated to R Markdown and knitr:\n\nOfficial Quarto website\n\nTutorial\nGuide\nOptions Reference\nBlog\n\nQuarto Cheatsheet\n\nWe also have a general list of recommended R resources.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "6. Quarto: reproducible publishing"
    ]
  },
  {
    "objectID": "R/installation.html",
    "href": "R/installation.html",
    "title": "R and RStudio installation instructions",
    "section": "",
    "text": "Participants are welcome to bring and use their own laptops with the software already installed (with a working Internet connection), to ensure that you can continue using what you learn once you leave the workshop. However, if you need, you can use one of the Library’s training computers, as they all offer the necessary software.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "0. Installing R and RStudio"
    ]
  },
  {
    "objectID": "R/installation.html#install-r-and-rstudio",
    "href": "R/installation.html#install-r-and-rstudio",
    "title": "R and RStudio installation instructions",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nFor the workshop, we will use RStudio. RStudio is a handy interface to use the programming language R. To use RStudio, you need to install both R and RStudio.\nPlease go through the installation steps below to install R and RStudio before the start of the workshop. Please also make sure that the software runs as expected once it is installed.\nIf you use a UQ computer and don’t have administrator rights, you might need to contact ITS for assistance.\n\nWindows\nTo use RStudio, you need to install both R and RStudio\n\nDownload R from the official website\nRun the .exe file that was just downloaded\nGo to the RStudio Download page\nSelect the Windows installer.\nDouble click the file to install it\nOnce it’s installed, open RStudio to make sure it works and there is no error messages.\n\n\n\nmacOS\nTo use RStudio, you need to install both R and RStudio\n\nGo to the R website\nSelect the .pkg file for the version of macOS that you have\nDouble-click on the file that was downloaded and R will install\nGo to the RStudio Download page\nSelect the macOS installer\nOnce it’s downloaded, double click the file to install it\nOnce it’s installed, open RStudio to make sure it works and there is no error messages.\n\n\n\nLinux\nTo use RStudio, you need to install both R and RStudio\n\nDownload R by following the instructions for your distribution from CRAN. For most distributions, you can use your package manager (e.g. for Debian/Ubuntu run sudo apt-get install r-base, and for Fedora run sudo yum install R)\nGo to the RStudio Download page\nSelect the installer for your distribution.\nOnce it’s downloaded, double click the file to install it, or use a command like sudo dpkg -i rstudio-x.yy.zzz-amd64.deb in the terminal\nOnce it’s installed, open RStudio to make sure it works and there is no error messages.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "0. Installing R and RStudio"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html",
    "href": "R/ggplot2_intro/ggplot2_intro.html",
    "title": "R ggplot2: introductory data visualisation",
    "section": "",
    "text": "TipUpcoming workshop(s) available!\n\n\n\nThe next workshop is on Mon Mar 09 at 01:00 PM.\nBook in to the next offering now.\nAlternatively, check our calendar for future events.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#open-rstudio",
    "href": "R/ggplot2_intro/ggplot2_intro.html#open-rstudio",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Open RStudio",
    "text": "Open RStudio\n\nIf you are using your own laptop please open RStudio\n\nIf you need theme, we have installation instructions\nMake sure you have a working Internet connection\n\nOn the Library computers (the first time takes about 10 min):\n\nLog in with your UQ username and password (use your student account if you have both a staff and student account)\nMake sure you have a working Internet connection\nOpen the ZENworks application\nLook for RStudio\nDouble-click on RStudio which will install both R and RStudio, and open RStudio",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#essential-shortcuts",
    "href": "R/ggplot2_intro/ggplot2_intro.html#essential-shortcuts",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Essential shortcuts",
    "text": "Essential shortcuts\nRemember some of the most commonly used RStudio shortcuts:\n\nfunction or dataset help: press F1 with your cursor anywhere in a function name.\nexecute from script: Ctrl + Enter\nassignment operator (&lt;-): Alt + -",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#installing-ggplot2",
    "href": "R/ggplot2_intro/ggplot2_intro.html#installing-ggplot2",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Installing ggplot2",
    "text": "Installing ggplot2\nWe first need to make sure we have the ggplot2 package available on our computer. We can use the “Install” button in the “Packages” pane, or we can execute this command in the console: install.packages(\"ggplot2\")\nYou only need to install a package once, but you need to load it every time you start a new R session.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#setting-up-a-project",
    "href": "R/ggplot2_intro/ggplot2_intro.html#setting-up-a-project",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Setting up a project",
    "text": "Setting up a project\n\nEverything we write today will be saved in your R project. Please remember to save it on your H drive or USB if you are using a Library computer.\n\nLet’s create a new R project to keep everything tidy:\n\nClick the “File” menu button (top left corner), then “New Project”\nClick “New Directory”\nClick “New Project”\nIn “Directory name”, type the name of your project, e.g. “ggplot2_intro”\nSelect the folder where to locate your project: e.g. a Documents/RProjects folder, which you can create if it doesn’t exist yet.\nClick the “Create Project” button\nCreate a folder to store our plots:\n\ndir.create(\"plots\")\n\n\nWe will write ggplot2 code more comfortably in a script:\n\nMenu: Top left corner, click the green “plus” symbol, or press the shortcut (for Windows/Linux) Ctrl+Shift+N or (for Mac) Cmd+Shift+N. This will open an “Untitled1” file.\nGo to “File &gt; Save” or press (for Windows/Linux) Ctrl+S or (for Mac) Cmd+S. This will ask where you want to save your file and the name of the new file.\nCall your file “process.R”\n\nWe can straight away load the package by adding this command to our script and executing it:\n\nlibrary(ggplot2)\n\n\nRemember to use Ctrl+Enter to execute a command from the script.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#finding-help",
    "href": "R/ggplot2_intro/ggplot2_intro.html#finding-help",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Finding help",
    "text": "Finding help\nWe are going to work with different datasets that come with the ggplot2 package. For any dataset or function doubts that you might have, don’t forget the two main ways to bring up a help page:\n\nthe command: ?functionname\nthe keyboard shortcut: press F1 after writing a function name\n\n\nIntroducing ggplot2\nThe R package ggplot2 was developed by Hadley Wickham with the objective of creating a grammar of graphics for categorical data (in 2007). It is based on the book The Grammar of Graphics Developed by Leland Wilkinson (first edition published in 1999).\nIt is now part of the group of data science packages called Tidyverse.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#the-components-of-the-grammar-of-graphics",
    "href": "R/ggplot2_intro/ggplot2_intro.html#the-components-of-the-grammar-of-graphics",
    "title": "R ggplot2: introductory data visualisation",
    "section": "The components of the Grammar of Graphics",
    "text": "The components of the Grammar of Graphics\nThe Grammar of Graphics is based on the idea that you can build every graph from the same few components.\nThe components are:\n\nData\nMapping\nStatistics\nScales\nGeometries\nFacets\nCoordinates\nTheme\n\nIn this introductory session, we will mainly focus on the data, the mapping, the statistics, the geometries and the theme.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#ggplot2s-three-essential-components",
    "href": "R/ggplot2_intro/ggplot2_intro.html#ggplot2s-three-essential-components",
    "title": "R ggplot2: introductory data visualisation",
    "section": "ggplot2’s three essential components",
    "text": "ggplot2’s three essential components\nIn ggplot2, the 3 main components that we usually have to provide are:\n\nWhere the data comes from,\nthe aesthetic mappings, and\na geometry.\n\nFor our first example, let’s use the msleep dataset (from the ggplot2 package), which contains data about mammals’ sleeping patterns.\n\nYou can find out about the dataset with ?msleep.\n\nLet’s start with specifying where the data comes from in the ggplot() function:\n\nggplot(data = msleep)\n\n\n\n\n\n\n\n\nThis is not very interesting. We need to tell ggplot2 what we want to visualise, by mapping aesthetic elements (like our axes) to variables from the data. We want to visualise how common different conservations statuses are, so let’s associate the right variable to the x axis:\n\nggplot(data = msleep,\n       mapping = aes(x = conservation))\n\n\n\n\n\n\n\n\nggplot2 has done what we asked it to do: the conservation variable is on the x axis. But nothing is shown on the plot area, because we haven’t defined how to represent the data, with a geometry_* function:\n\nggplot(data = msleep,\n       mapping = aes(x = conservation)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNow we have a useful plot: we can see that a lot of animals in this dataset don’t have a conservation status, and that “least concern” is the next most common value.\nWe can see our three essential elements in the code:\n\nthe data comes from the msleep object;\nthe variable conservation is mapped to the aesthetic x (i.e. the x axis);\nthe geometry is \"bar\", for “bar chart”.\n\nHere, we don’t need to specify what variable is associated to the y axis, as the “bar” geometry automatically does a count of the different values in the conservation variable. That is what statistics are applied automatically to the data.\n\nIn ggplot2, each geometry has default statistics, so we often don’t need to specify which stats we want to use. We could use a stat_*() function instead of a geom_*() function, but most people start with the geometry (and let ggplot2 pick the default statistics that are applied).",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#line-plots",
    "href": "R/ggplot2_intro/ggplot2_intro.html#line-plots",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Line plots",
    "text": "Line plots\nLet’s have a look at another dataset: the economics dataset from the US. Learn more about it with ?economics, and have a peak at its structure with:\n\nstr(economics)\n\nspc_tbl_ [574 × 6] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ date    : Date[1:574], format: \"1967-07-01\" \"1967-08-01\" ...\n $ pce     : num [1:574] 507 510 516 512 517 ...\n $ pop     : num [1:574] 198712 198911 199113 199311 199498 ...\n $ psavert : num [1:574] 12.6 12.6 11.9 12.9 12.8 11.8 11.7 12.3 11.7 12.3 ...\n $ uempmed : num [1:574] 4.5 4.7 4.6 4.9 4.7 4.8 5.1 4.5 4.1 4.6 ...\n $ unemploy: num [1:574] 2944 2945 2958 3143 3066 ...\n\n\nDo you think that unemployment is stable over the years? Let’s have a look with a line plot, often used to visualise time series:\n\nggplot(data = economics,\n       mapping = aes(x = date,\n                     y = unemploy)) + \n    geom_line()\n\n\n\n\n\n\n\n\nLet’s go through our essential elements once more:\n\nThe ggplot() function initialises a ggplot object. In it, we declare the input data frame and specify the set of plot aesthetics used throughout all layers of our plot;\nThe aes() function groups our mappings of aesthetics to variables;\nThe geom_&lt;...&gt;() function specifies what geometric element we want to use.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#scatterplots",
    "href": "R/ggplot2_intro/ggplot2_intro.html#scatterplots",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Scatterplots",
    "text": "Scatterplots\nScatterplots are often used to look at the relationship between two variables. Let’s try it with a new dataset: mpg (which stands for “miles per gallon”), a dataset about fuel efficiency of different models of cars.\n\n?mpg\nstr(mpg)\n\nDo you think that big engines use fuel more efficiently than small engines?\nWe can focus on two variables:\n\ndispl: a car’s engine size, in litres.\nhwy: a car’s fuel efficiency on the highway, in miles per gallon.\n\nFor the geometry, we now have use “points”:\n\nggplot(data = mpg,\n       mapping = aes(x = displ,\n                     y = hwy)) +\n    geom_point()\n\n\n\n\n\n\n\n\nNotice how the points seem to be aligned on a grid? That’s because the data was rounded. If we want to better visualise the density of points, we can use the “count” geometry, which makes the dots bigger when data points have the same x and y values:\n\nggplot(data = mpg,\n       mapping = aes(x = displ,\n                     y = hwy)) +\n    geom_count()\n\n\n\n\n\n\n\n\nAlternatively, we can avoid overlapping of points by using the “jitter” geometry, which gives the points a little shake:\n\nggplot(data = mpg,\n       mapping = aes(x = displ,\n                     y = hwy)) +\n    geom_jitter()\n\n\n\n\n\n\n\n\nEven though the position of the dots does not match exactly the original x and y values, it does help visualise densities better.\nThe plot shows a negative relationship between engine size (displ) and fuel efficiency (hwy). In other words, cars with big engines use more fuel. Does this confirm or refute your hypothesis about fuel efficiency and engine size?\nHowever, we can see some outliers. We need to find out more about our data.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#adding-aesthetics",
    "href": "R/ggplot2_intro/ggplot2_intro.html#adding-aesthetics",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Adding aesthetics",
    "text": "Adding aesthetics\nWe can highlight the “class” factor by adding a new aesthetic:\n\nggplot(data = mpg,\n       mapping = aes(x = displ,\n                     y = hwy,\n                     colour = class)) +\n    geom_jitter()\n\n\n\n\n\n\n\n\nIt seems that two-seaters are more fuel efficient than other cars with a similar engine size, which can be explained by the lower weight of the car. The general trend starts to make more sense!\nWe now know how to create a simple scatterplot, and how to visualise extra variables. But how can we better represent a correlation?",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#trend-lines",
    "href": "R/ggplot2_intro/ggplot2_intro.html#trend-lines",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Trend lines",
    "text": "Trend lines\nA trend line can be created with the geom_smooth() function:\n\nggplot(mpg,\n       aes(x = displ,\n           y = hwy)) +\n    geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nWe stopped using the argument names because we know in which order they appear: first the data, then the mapping of aesthetics. Let’s save ourselves some typing from now on!\n\nThe console shows you what function / formula was used to draw the trend line. This is important information, as there are countless ways to do that. To better understand what happens in the background, open the function’s help page and notice that the default value for the method argument is “NULL”. Read up on how it automatically picks a suitable method depending on the sample size, in the “Arguments” section.\nWant a linear trend line instead? Add the argument method = \"lm\" to your function:\n\nggplot(mpg,\n       aes(x = displ,\n           y = hwy)) +\n    geom_smooth(method = \"lm\")\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#layering",
    "href": "R/ggplot2_intro/ggplot2_intro.html#layering",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Layering",
    "text": "Layering\nA trend line is usually displayed on top of the scatterplot. How can we combine several layers? We can string them with the + operator:\n\nggplot(mpg,\n       aes(x = displ,\n           y = hwy)) + \n    geom_point() +\n    geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nThe order of the functions matters: the points will be drawn before the trend line, which is probably what you’re after.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#the-colour-aesthetic",
    "href": "R/ggplot2_intro/ggplot2_intro.html#the-colour-aesthetic",
    "title": "R ggplot2: introductory data visualisation",
    "section": "The colour aesthetic",
    "text": "The colour aesthetic\nWe can once again add some information to our visualisation by mapping the class variable to the colour aesthetic:\n\nggplot(mpg,\n       aes(x = displ,\n           y = hwy)) + \n    geom_point(aes(colour = class)) + \n    geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nChallenge 1 – where should aesthetics be defined?\nTake the last plot we created:\n\nggplot(mpg,\n       aes(x = displ,\n           y = hwy)) + \n    geom_point(aes(colour = class)) + \n    geom_smooth()\n\nWhat would happen if you moved the colour = class aesthetic from the geometry function to the ggplot() call?\nDifferent geometries can also have their own mappings that overwrite the defaults. If you place mappings in a geom function, ggplot2 will treat them as local mappings for the layer. It will use these mappings to extend or overwrite the global mappings for that layer only. This makes it possible to display different aesthetics in different layers.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#saving-a-plot",
    "href": "R/ggplot2_intro/ggplot2_intro.html#saving-a-plot",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Saving a plot",
    "text": "Saving a plot\nLike your visualisation? You can export it with the “Export” menu in the “Plots” pane.\n\nBuilding a document or a slideshow? You can copy it straight to your clipboard, and paste it into it.\nA PDF is a good, quick option to export an easily shareable file with vector graphics. Try for example the “A5” size, the “Landscape” orientation, and save it into your “plots” directory.\nMore options are available in the “Save as image…” option. PNG is a good compressed format for graphics, but if you want to further customise your visualisation in a different program, use SVG or EPS, which are vector formats. (Try to open an SVG file in Inkscape for example.)\n\nTo save the last plot with a command, you can use the ggsave() function:\n\nggsave(filename = \"plots/fuel_efficiency.png\")\n\nThis is great to automate the export process for each plot in your script, but ggsave() also has extra options, like setting the DPI, which is useful for getting the right resolution for a specific use. For example, to export a plot for your presentation:\n\nggsave(filename = \"plots/fuel_efficiency.png\", dpi = \"screen\")\n\n\nSaving a .svg file with requires installing the svglite package. This packages seems so work best installing in a fresh R session (Session &gt; Restart R) from source install.packages(\"svglite\", type = \"source\"). Then load the library library(svglite) rerun your code including loading previous libraries (ggplot2 etc.) and now saving a plot with a .svg extension should work!\n\nChallenge 2 – add a variable and a smooth line\nLet’s use a similar approach to what we did with the mpg dataset.\nTake our previous unemployment visualisation, but represented with points this time:\n\nggplot(economics,\n       aes(x = date,\n           y = unemploy)) + \n    geom_point()\n\nHow could we:\n\nAdd a smooth line for the number of unemployed people. Are there any interesting arguments that could make the smoother more useful?\nColour the points according to the median duration of unemployment (see ?economics)\n\n\nggplot(economics,\n       aes(x = date,\n           y = unemploy)) + \n    geom_point(aes(colour = uempmed)) +\n    geom_smooth()\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nSee how the legend changes depending on the type of data mapped to the colour aesthetic? (i.e. categorical vs continuous)\n\nThis default “trend line” is not particularly useful. We could make it follow the data more closely by using the span argument. The closer to 0, the closer to the data the smoother will be:\n\nggplot(economics,\n       aes(x = date,\n           y = unemploy)) + \n    geom_point(aes(colour = uempmed)) +\n    geom_smooth(span = 0.1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nYou can now see why this is called a “smoother”: we can fit a smooth curve to data that varies a lot.\nTo further refine our visualisation , we could visualise the unemployment rate rather than the number of unemployed people, by calculating it straight into our code:\n\nggplot(economics,\n       aes(x = date,\n           y = unemploy / pop)) + \n    geom_point(aes(colour = uempmed)) +\n    geom_smooth(span = 0.1)\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe early 1980s recession now seems to have had a more significant impact on unemployment than the Global Financial Crisis of 2007-2008.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#bar-charts-and-ordered-factors",
    "href": "R/ggplot2_intro/ggplot2_intro.html#bar-charts-and-ordered-factors",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Bar charts and ordered factors",
    "text": "Bar charts and ordered factors\nLet’s use the diamonds dataset now. The diamonds dataset comes with ggplot2 and contains information about ~54,000 diamonds, including the price, carat, colour, clarity, and cut quality of each diamond.\nLet’s have a look at the data:\n\ndiamonds\nsummary(diamonds)\n?diamonds\n\nBack to bar charts. Consider a basic bar chart, as drawn with geom_bar(). The following chart displays the total number of diamonds in the diamonds dataset, grouped by cut:\n\nggplot(diamonds,\n       aes(x = cut)) + \n    geom_bar()\n\n\n\n\n\n\n\n\nThe chart shows that more diamonds are available with high quality cuts than with low quality cuts.\ncut is an ordered factor, which you can confirm by printing it to the console:\n\nhead(diamonds$cut)\n\n[1] Ideal     Premium   Good      Premium   Good      Very Good\nLevels: Fair &lt; Good &lt; Very Good &lt; Premium &lt; Ideal\n\n\nSee how ggplot2 respects that order in the bar chart?\n\nCustomising a plot\nLet’s see how we can customise our bar chart’s look.\n\nChange a geometry’s default colour\nFirst, we can pick our favourite colour in geom_bar():\n\nggplot(diamonds,\n       aes(x = cut)) + \n    geom_bar(fill = \"tomato\")\n\n\n\n\n\n\n\n\nIf you are curious about what colour names exist in R, you can use the colours() function.\n\n\n\nChange labels\nWe can also modify labels with the labs() function to make our plot more self-explanatory:\n\nggplot(diamonds,\n       aes(x = cut)) + \n    geom_bar(fill = \"tomato\") +\n    labs(title = \"Where are the bad ones?\",\n         x = \"Quality of the cut\",\n         y = \"Number of diamonds\")\n\n\n\n\n\n\n\n\nLet’s have a look at what labs() can do:\n\n?labs\n\nIt can edit the title, the subtitle, the x and y axes labels, and the caption.\n\nRemember that captions and titles are better sorted out in the publication itself, especially for accessibility reasons (e.g. to help with screen readers).\n\n\n\nHorizontal bar charts\nFor a horizontal bar chart, we can map the cut variable to the y aesthetic instead of x. But remember to also change your labels around!\n\nggplot(diamonds,\n       aes(y = cut)) + # switch here...\n  geom_bar(fill = \"tomato\") +\n  labs(title = \"Where are the bad ones?\",\n       y = \"Quality of the cut\", # ...but also here!\n       x = \"Number of diamonds\") # ...and here!\n\n\n\n\n\n\n\n\nThis is particularly helpful when long category names overlap under the x axis.\n\n\nBuilt-in themes\nThe theme() function allows us to really get into the details of our plot’s look, but some theme_*() functions make it easy to apply a built-in theme, like theme_bw():\n\nggplot(diamonds,\n       aes(y = cut)) + \n  geom_bar(fill = \"tomato\") +\n  labs(title = \"Where are the bad ones?\",\n       y = \"Quality of the cut\",\n       x = \"Number of diamonds\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nTry theme_minimal() as well, and if you want more options, install the ggthemes package!",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#play-time",
    "href": "R/ggplot2_intro/ggplot2_intro.html#play-time",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Play time!",
    "text": "Play time!\nChallenge 3: explore geometries\nWhen creating a new layer, start typing geom_ and see what suggestions pop up. Are there any suggestions that sound useful or familiar to you?\nModify your plots, play around with different layers and functions, and ask questions!",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#close-project",
    "href": "R/ggplot2_intro/ggplot2_intro.html#close-project",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Close project",
    "text": "Close project\nClosing RStudio will ask you if you want to save your workspace and scripts. Saving your workspace is usually not recommended if you have all the necessary commands in your script.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intro/ggplot2_intro.html#useful-links",
    "href": "R/ggplot2_intro/ggplot2_intro.html#useful-links",
    "title": "R ggplot2: introductory data visualisation",
    "section": "Useful links",
    "text": "Useful links\n\nFor ggplot2:\n\nggplot2 cheatsheet\nOfficial ggplot2 documentation\nOfficial ggplot2 website\nChapter on data visualisation in the book R for Data Science\nFrom Data to Viz, a website to explore different visualisations and the code that generates them\nSelva Prabhakaran’s r-statistics.co section on ggplot2\nCoding Club’s data visualisation tutorial\nSTHDA’s ggplot2 essentials\n\nOur compilation of general R resources",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "3. ggplot2: introduction to visualisation"
    ]
  },
  {
    "objectID": "R/dplyr/dplyr.html",
    "href": "R/dplyr/dplyr.html",
    "title": "R dplyr: preparing data for analysis",
    "section": "",
    "text": "TipUpcoming workshop(s) available!\n\n\n\nThe next workshop is on Mon Mar 02 at 01:00 PM.\nBook in to the next offering now.\nAlternatively, check our calendar for future events.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "2. dplyr: data transformation"
    ]
  },
  {
    "objectID": "R/dplyr/dplyr.html#what-are-we-going-to-learn",
    "href": "R/dplyr/dplyr.html#what-are-we-going-to-learn",
    "title": "R dplyr: preparing data for analysis",
    "section": "What are we going to learn?",
    "text": "What are we going to learn?\nIn this hands-on session, you will use R, RStudio and the dplyr package to transform your data.\nSpecifically, you will learn how to explore, filter, reorganise and process a table of data with the following verbs:\n\nselect(): pick variables\nfilter(): pick observations\narrange(): reorder observations\nmutate(): create new variables\nsummarise(): collapse to a single summary\ngroup_by(): change the scope of function",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "2. dplyr: data transformation"
    ]
  },
  {
    "objectID": "R/dplyr/dplyr.html#keep-in-mind",
    "href": "R/dplyr/dplyr.html#keep-in-mind",
    "title": "R dplyr: preparing data for analysis",
    "section": "Keep in mind",
    "text": "Keep in mind\n\nEverything we write today will be saved in your project. Please remember to save it in your H drive or USB if you are using a Library computer.\nR is case sensitive: it will tell the difference between uppercase and lowercase.\nRespect the naming rules for objects (no spaces, does not start with a number…)\n\n\nHelp\nFor any dataset or function doubts that you might have, don’t forget the three ways of getting help in RStudio:\n\nthe shortcut command: ?functionname\nthe help function: help(functionname)\nthe keyboard shortcut: press F1 after writing a function name",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "2. dplyr: data transformation"
    ]
  },
  {
    "objectID": "R/dplyr/dplyr.html#open-rstudio",
    "href": "R/dplyr/dplyr.html#open-rstudio",
    "title": "R dplyr: preparing data for analysis",
    "section": "Open RStudio",
    "text": "Open RStudio\n\nIf you are using your own laptop please open RStudio\n\nIf you need them, we have installation instructions\n\nMake sure you have a working internet connection\nOn Library computers (the first time takes about 10 min.):\n\nLog in with your UQ credentials (student account if you have two)\nMake sure you have a working internet connection\nGo to search at bottom left corner (magnifiying glass)\nOpen the ZENworks application\nLook for RStudio\nDouble click on RStudio which will install both R and RStudio",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "2. dplyr: data transformation"
    ]
  },
  {
    "objectID": "R/dplyr/dplyr.html#setting-up",
    "href": "R/dplyr/dplyr.html#setting-up",
    "title": "R dplyr: preparing data for analysis",
    "section": "Setting up",
    "text": "Setting up\n\nInstall the dplyr package\nIf you don’t have it already, you can install dplyr with the command: install.packages(\"dplyr\")\n\n\n\n\n\n\nNote\n\n\n\nAt home, you can install the whole “tidyverse”, a meta-package useful for data science: install.packages(\"tidyverse\")\n\n\n\n\nNew project\n\nClick the “File” menu button (top left corner), then “New Project”\nClick “New Directory”\nClick “New Project” (“Empty project” if you have an older version of RStudio)\nIn “Directory name”, type the name of your project, e.g. “dplyr_intro”\nSelect the folder where to locate your project: for example, the Documents/RProjects folder, which you can create if it doesn’t exist yet.\nClick the “Create Project” button\n\n\n\nCreate a script\nWe will use a script to write code more comfortably.\n\nMenu: Top left corner, click the green “plus” symbol, or press the shortcut (for Windows/Linux) Ctrl+Shift+N or (for Mac) Cmd+Shift+N. This will open an “Untitled1” file.\nGo to “File &gt; Save” or press (for Windows/Linux) Ctrl+S or (for Mac) Cmd+S. This will ask where you want to save your file and the name of the new file.\nCall your file “process.R”\n\n\n\nIntroducing our data\nLet’s import and explore our data.\n\nread the data into an object called “gapminder”, using read.csv():\n\n\ngapminder &lt;- read.csv(\"https://raw.githubusercontent.com/resbaz/r-novice-gapminder-files/master/data/gapminder-FiveYearData.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nRemember you can use Ctrl+Enter to execute a command from the script.\n\n\n\nExplore the gapminder dataset using dim() and str()\n\nHow can we get the dataframe’s variable names? There are two ways: names(gapminder) returns the names regardless of the object type, such as list, vector, data.frame etc., whereas colnames(gapminder) returns the variable names for matrix-like objects, such as matrices, dataframes…\nTo return one specific column in the dataframe, you can use the dollar syntax: gapminder$year. For example, try these:\n\nclass(gapminder$country) # what kind of data?\n\n[1] \"character\"\n\nrange(gapminder$year) # what is the time range?\n\n[1] 1952 2007",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "2. dplyr: data transformation"
    ]
  },
  {
    "objectID": "R/dplyr/dplyr.html#basic-dplyr-verbs",
    "href": "R/dplyr/dplyr.html#basic-dplyr-verbs",
    "title": "R dplyr: preparing data for analysis",
    "section": "Basic dplyr verbs",
    "text": "Basic dplyr verbs\nThe R package dplyr was developed by Hadley Wickham for data manipulation.\nThe book R for Data Science introduces the package as follows:\n\n\n\n\n\n\nNotedplyr essentials\n\n\n\nYou are going to learn the five key dplyr functions that allow you to solve the vast majority of your data manipulation challenges:\n\nPick variables by their names with select()\nPick observations by their values with filter()\nReorder the rows with arrange()\nCreate new variables with functions of existing variables with mutate()\nCollapse many values down to a single summary with summarise()\n\nThese can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. These six functions provide the main verbs for a language of data manipulation.\n\n\nTo use the verbs to their full extent, we will need pipes and logical operators, which we will introduce as we go.\nLet’s load the dplyr package to access its functions:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou only need to install a package once (with install.packages()), but you need to reload it every time you start a new R session (with library()).\n\n\n\n1. Pick variables with select()\nselect() allows us to pick variables (i.e. columns) from the dataset. For example, to only keep the data about year, country and GDP per capita:\n\ngap_small &lt;- select(gapminder, year, country, gdpPercap)\n\nThe first argument refers to the dataframe that is being transformed, and the following arguments are the columns you want to keep. Notice that it keeps the order you specified?\nYou can also rename columns in the same command:\n\ngap_small &lt;- select(gapminder, year, country, gdpPerPerson = gdpPercap)\n\nIf you have many variables but only want to remove a small number, it might be better to deselect instead of selecting. You can do that by using the - character in front of a variable name:\n\ngap_remove &lt;- select(gapminder, -continent)\n\nYou can also select a range of columns, by specifying the first and last column names with the : character in between:\n\ngap_range &lt;- select(gapminder, country:continent)\n\nAnd if you know the position of the columns but not the names, you can also use indices:\n\ngap_indices &lt;- select(gapminder, 1:3, 5)\n\nThere are many helper functions to select columns according to a logic. For example, to only keep the columns that have “a” in their names:\n\ngap_a &lt;- select(gapminder, contains(\"a\"))\n\nTo see more helper operators and functions, look at the select() help page: ?select\n\n\n2. Pick observations with filter()\nThe filter() function allows use to pick observations depending on one or several conditions. But to be able to define these conditions, we need to learn about logical operators.\nLogical operators allow us to compare things. Here are some of the most important ones:\n\n==: equal\n!=: different or not equal\n&gt;: greater than\n&lt;: smaller than\n&gt;=: greater or equal\n&lt;=: smaller or equal\n\n\n\n\n\n\n\nWarning\n\n\n\nRemember: = is used to pass on a value to an argument, whereas == is used to check for equality. Using = instead of == for a logical statement is one of the most common errors and R will give you a reminder in the console when this happens.\n\n\nYou can compare any kind of data For example:\n\n1 == 1\n\n[1] TRUE\n\n1 == 2\n\n[1] FALSE\n\n1 != 2\n\n[1] TRUE\n\n1 &gt; 0\n\n[1] TRUE\n\n\"money\" == \"happiness\"\n\n[1] FALSE\n\n\nWhen R executes these commands, it answers TRUE of FALSE, as if asked a yes/no question. These TRUE and FALSE values are called logical values.\nTo see if a value is part of a set of values, you can use the %in% operator. For example, to confirm that I have “coffee” in my grocery list, but not “mangoes”:\n\ngroceries &lt;- c(\"tea\", \"coffee\", \"milk\")\n\"coffee\" %in% groceries\n\n[1] TRUE\n\n\"mangoes\" %in% groceries\n\n[1] FALSE\n\n\nNote that we can compare a single value to many. For example, compare one value to five others:\n\n1 == c(1, 2, 3, 1, 3)\n\n[1]  TRUE FALSE FALSE  TRUE FALSE\n\n\nThis kind of operation results in a logical vector with a logical value for each element. This is exactly what we will use to filter our rows.\nFor example, to filter the observations for Australia, we can use the following condition:\n\naustralia &lt;- filter(gapminder, country == \"Australia\")\naustralia\n\n\n  \n\n\n\nThe function compares the value “Australia” to all the values in the country variable, and only keeps the rows that have TRUE as an answer.\nNow, let’s filter the rows that have a life expectancy lifeExp greater than 81 years:\n\nlife81 &lt;- filter(gapminder, lifeExp &gt; 81)\ndim(life81)\n\n[1] 7 6\n\n\n\n\n3. Reorder observations with arrange()\narrange() will reorder our rows according to a variable, by default in ascending order:\n\narrange(life81, lifeExp)\n\n\n  \n\n\n\nIf we want to have a look at the entries with highest life expectancy first, we can use the desc() function (for “descending”):\n\narrange(life81, desc(lifeExp))\n\n\n  \n\n\n\nWe could also use the - shortcut, which only works for numerical data:\n\narrange(life81, -lifeExp)\n\n\nThe pipe operator\nWhat if we wanted to get that result in one single command, without an intermediate life81 object?\nWe could nest the commands into each other, the first step as the first argument of the second step:\n\narrange(filter(gapminder, lifeExp &gt; 81), -lifeExp)\n\n… but this becomes very hard to read, very quickly. (Imagine with 3 steps or more!)\nWe can make our code more readable and avoid creating useless intermediate objects by piping commands into each other. The pipe operator %&gt;% strings commands together, using the left side’s output as the first argument of the right side function.\nFor example, this command:\n\nround(1.23, digits = 1)\n\n[1] 1.2\n\n\n… is equivalent to:\n\n1.23 %&gt;% round(digits = 1)\n\n[1] 1.2\n\n\nHere’s another example with the filter() verb:\n\ngapminder %&gt;%\n  filter(country != \"France\")\n\n\n  \n\n\n\n… becomes:\n\nfilter(gapminder, country != \"France\")\n\n\n  \n\n\n\nTo do what we did previously in one single command, using the pipe:\n\ngapminder %&gt;% \n  filter(lifeExp &gt; 81) %&gt;% \n  arrange(-lifeExp)\n\n\n  \n\n\n\nThe pipe operator can be read as “then” and makes the code a lot more readable than when nesting functions into each other, and avoids the creation of several intermediate objects. It is also easier to troubleshoot as it makes it easy to execute the pipeline step by step.\nFrom now on, we’ll use the pipe syntax as a default.\n\n\n\n\n\n\nNote\n\n\n\nThis material uses the magrittr pipe. The magrittr package is the one that introduced the pipe operator to the R world, and dplyr automatically imports this useful operator when it is loaded. However, the pipe being such a widespread and popular concept in programming and data science, it ended up making it into Base R (the “native” pipe) in 2021 with the release of R 4.1, using a different operator: |&gt;. You can switch your pipe shortcut to the native pipe in Tools &gt; Global options &gt; Code &gt; Use native pipe operator.\n\n\n\n\nChallenge 1 – a tiny dataset\nUse some of the functions we learned about to keep the 2002 life expectancy observation for Eritrea and remove the superfluous variables.\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\neritrea_2002 &lt;- gapminder %&gt;%\n    select(year, country, lifeExp) %&gt;%\n    filter(country == \"Eritrea\", year == 2002)\n\n\n\n\n\n\n\n4. Create new variables with mutate()\nHave a look at what the verb mutate() can do with ?mutate.\nLet’s see what the two following variables can be used for:\n\ngapminder %&gt;%\n    select(gdpPercap, pop)\n\n\n  \n\n\n\nHow do you think we could combine them to add something new to our dataset?\nWe could use mutate() to create a gdp variable, by multiplying the population and the GDP per capita:\n\ngap_gdp &lt;- gapminder %&gt;%\n    mutate(gdp = gdpPercap * pop)\n\nYou can reuse a variable computed by ‘mutate()’ straight away. For example, we also want a more readable version of our new variable, in billion dollars:\n\ngap_gdp &lt;- gapminder %&gt;%\n    mutate(gdp = gdpPercap * pop,\n           gdpBil = gdp / 1e9)\n\n\n\n5. Collapse to a single value with summarise()\nsummarise() collapses many values down to a single summary. For example, to find the mean life expectancy for the whole dataset:\n\ngapminder %&gt;%\n  summarise(meanLE = mean(lifeExp))\n\n\n  \n\n\n\nHowever, a single-value summary is not particularly interesting. summarise() becomes more powerful when used with group_by().\n\n\n6. Change the scope with group_by()\ngroup_by() changes the scope of the following function(s) from operating on the entire dataset to operating on it group-by-group.\nSee the effect of the grouping step:\n\ngapminder %&gt;%\n    group_by(continent)\n\n\n  \n\n\n\nThe data in the cells is the same, the size of the object is the same. However, the dataframe was converted to a tibble, because a dataframe is not capable of storing grouping information.\nUsing the group_by() function before summarising makes things more interesting. Let’s re-run the previous command, with the intermediate grouping step:\n\ngapminder %&gt;%\n  group_by(continent) %&gt;% \n  summarise(meanLE = mean(lifeExp))\n\n\n  \n\n\n\nWe now have the summary computed for each continent.\nSimilarly, to find out the total population per continent in 2007, we can do the following:\n\ngapminder %&gt;% \n    filter(year == 2007) %&gt;%\n    group_by(continent) %&gt;%\n    summarise(pop = sum(pop))\n\n\n  \n\n\n\n\nChallenge 2 – max life expectancy per country\nGroup by country, and find out the maximum life expectancy ever recorded for each one.\nHint: ?max\n\n\n\n\n\n\nTipSolution\n\n\n\n\n\n\ngapminder %&gt;% \n    group_by(country) %&gt;%\n    summarise(maxLE = max(lifeExp))\n\n\n  \n\n\n\n\n\n\nAnd we could build on this to find out which countries have had the largest variation in life expectancy:\n\ngapminder %&gt;% \n  group_by(country) %&gt;%\n  summarise(minLE = min(lifeExp), maxLE = max(lifeExp)) %&gt;%\n  mutate(difLE = maxLE - minLE) %&gt;%\n  arrange(-difLE)",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "2. dplyr: data transformation"
    ]
  },
  {
    "objectID": "R/dplyr/dplyr.html#more-examples",
    "href": "R/dplyr/dplyr.html#more-examples",
    "title": "R dplyr: preparing data for analysis",
    "section": "More examples",
    "text": "More examples\nAnother example of a summary, with a the starwars data set that dplyr provides:\nGrouping by species, summarise the number of characters per species and find the mean mass. Only for species groups with more than 1 character.\n\nstarwars %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    n = n(), # this counts the number of rows in each group\n    mass = mean(mass, na.rm = TRUE)\n  ) %&gt;%\n  filter(n &gt; 1) # the mean of a single value is not worth reporting\n\n\n  \n\n\n\n\n\n\n\n\n\nNoteCombining dplyr with ggplot2\n\n\n\n\n\nIf you are already familiar with ggplot2, the two example below illustrate how you can process data with dplyr and pipe the result directly into a ggplot2 visualisation, as often done when using various Tidyverse packages.\nThe following example summarises the gapminder population data into total population per continent per year and plots the results coloured by continent.\n\n# increase in population per continent\nlibrary(ggplot2)\ngapminder %&gt;% \n  group_by(continent, year) %&gt;% \n  summarise(pop = sum(pop)) %&gt;% \n  ggplot(aes(x = year,\n             y = pop,\n             colour = continent)) +\n  geom_line()\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\n\n\n\n\n\n\n\n\nAnd another example, using using our gapminder dataset:\nLet’s say we want to calculate the variation (range) in life expectancy per country and plot the top and bottom 10 countries?\n\ngapminder %&gt;% \n  group_by(country) %&gt;% \n  summarise(maxLifeExp = max(lifeExp),\n            minLifeExp = min(lifeExp)) %&gt;% \n  mutate(dif = maxLifeExp - minLifeExp) %&gt;%  # new col with difference between max/min lifeExp\n  arrange(desc(dif)) %&gt;%  # arrange by dif, descending order for the next step\n  slice(1:10, (nrow(.)-10):nrow(.)) %&gt;%  # slice top 10 rows and bottom 10 rows\n  ggplot(aes(x = reorder(country, dif), y = dif)) +\n  geom_col() +\n  coord_flip() + # flip the x and y axis for a horizontal bar chart\n  labs(x = \"Country\",\n       y = \"Difference in Life Expectancy\") + # prettier labels for axes (which have been flipped) \n  annotate(\"segment\", x = 11.5, xend = 21.5, y = 39, yend = 39, colour = \"purple\", linewidth = 1, alpha = 0.6) +\n  annotate(\"segment\", x = 0.5, xend = 11, y = 39, yend = 39, colour = \"green\", linewidth = 1, alpha = 0.6) +\n    annotate(\"text\", x = c(5, 16), y = c(40, 40), \n           label = c(\"Smallest 10\", \"Largest 10\") ,\n           color=\"black\", size= 5 , angle=90) # add labels to coloured lines",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "2. dplyr: data transformation"
    ]
  },
  {
    "objectID": "R/dplyr/dplyr.html#close-project",
    "href": "R/dplyr/dplyr.html#close-project",
    "title": "R dplyr: preparing data for analysis",
    "section": "Close project",
    "text": "Close project\nIf you want to close RStudio, make sure you save your script first.\nYou can then close the window, and if your script contains all the steps necessary for your data processing, it is safer to not save your workspace at the prompt. It should only take a second to execute all the commands stored in your script when you re-open your project.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "2. dplyr: data transformation"
    ]
  },
  {
    "objectID": "R/dplyr/dplyr.html#what-next",
    "href": "R/dplyr/dplyr.html#what-next",
    "title": "R dplyr: preparing data for analysis",
    "section": "What next?",
    "text": "What next?\nMore on dplyr:\n\ndplyr cheatsheet\nR for Data Science, chapter about dplyr",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "2. dplyr: data transformation"
    ]
  },
  {
    "objectID": "Qualtrics/qualtrics.html",
    "href": "Qualtrics/qualtrics.html",
    "title": "Qualtrics",
    "section": "",
    "text": "Qualtrics is an “experience management” plaftform that can be used for running surveys and analysing the collected responses.\nFind below the resources for our Qualtrics sessions.",
    "crumbs": [
      "Home",
      "![](/images/XM.svg){width=20} Qualtrics"
    ]
  },
  {
    "objectID": "Qualtrics/qualtrics.html#using-qualtrics-for-surveys",
    "href": "Qualtrics/qualtrics.html#using-qualtrics-for-surveys",
    "title": "Qualtrics",
    "section": "Using Qualtrics for surveys",
    "text": "Using Qualtrics for surveys\n\nManual\nFiles",
    "crumbs": [
      "Home",
      "![](/images/XM.svg){width=20} Qualtrics"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html",
    "href": "QGIS/reports/QGIS_reports.html",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "",
    "text": "This is an intermediate level tutorial. Before completing this tutorial, we recommend our QGIS: Introduction to Mapping tutorial. This tutorial is designed for QGIS 3.40. If you need to install it on your computer, go to the QGIS website.\nWe will start a little bit differently today, as we’re going to get straight into a project. The data and project for this session can be directly downloaded from this link. This highlights one of the big benefits of creating a good folder structure for your project – it’s much easier to share!\nPlease extract the folder from the zip file.\nAs always we have our classic setup:\n\n“data” - for all the data we will use to make our maps, split into:\n\n“raw” - raw data from your research or the internet\n“processed” - any data you’ve modified\n\n“output” - for any maps or images we export\n“temp” - this folder isn’t necessary, but when you’re playing around and testing, it can stop things getting messy.\n\n\n\nFinally, we have our .qgz project file named “qgis_reports_map.qgz” inside the “qgis_reports” folder (the highest level folder) so it’s only looking down into folders for data, not back out.\n\nLet’s open our qgis_reports_map.qgz file and get started.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#setting-up",
    "href": "QGIS/reports/QGIS_reports.html#setting-up",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "",
    "text": "This is an intermediate level tutorial. Before completing this tutorial, we recommend our QGIS: Introduction to Mapping tutorial. This tutorial is designed for QGIS 3.40. If you need to install it on your computer, go to the QGIS website.\nWe will start a little bit differently today, as we’re going to get straight into a project. The data and project for this session can be directly downloaded from this link. This highlights one of the big benefits of creating a good folder structure for your project – it’s much easier to share!\nPlease extract the folder from the zip file.\nAs always we have our classic setup:\n\n“data” - for all the data we will use to make our maps, split into:\n\n“raw” - raw data from your research or the internet\n“processed” - any data you’ve modified\n\n“output” - for any maps or images we export\n“temp” - this folder isn’t necessary, but when you’re playing around and testing, it can stop things getting messy.\n\n\n\nFinally, we have our .qgz project file named “qgis_reports_map.qgz” inside the “qgis_reports” folder (the highest level folder) so it’s only looking down into folders for data, not back out.\n\nLet’s open our qgis_reports_map.qgz file and get started.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#todays-learning-goals",
    "href": "QGIS/reports/QGIS_reports.html#todays-learning-goals",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Today’s Learning Goals",
    "text": "Today’s Learning Goals\nMain aims:\n\nCreate a pretty map for output\nAutomate our map description\nUse templates\n\nIf we have time:\n\nLearn to use Web Mapping Services\nGeoreference an Image",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#the-brief",
    "href": "QGIS/reports/QGIS_reports.html#the-brief",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "The Brief",
    "text": "The Brief\nWe need to use our spatial files to make a map that:\n\nShows the project outline\nShow the access paths\nCategorises the artefacts\n\nThe current map in our project does accomplish these goals, but it’s not inviting to look at.\nTo fix this, we need to ask some questions:\nWho is the intended audience for our map?\n\nA good looking map for a Report map may look different to an Academic Paper or a Technical map or to a map for the Public.\n\nWhat story am I trying to tell?\n\nDo you want to highlight a particular pattern in the data?\n\nYou can use shapes and colour (esp. saturation) to bring your viewer to focus on certain elements of the map.\n\nDoes it need to look friendly or more professional?\n\nYour choice of a colour palette can bring cohesion to your map.\n\nDo you have a lot of dense information you need to convey?\n\nPerhaps you need to make more than one map.\n\nDo you need to show what the land surface looks like, or not?\n\nThat brings us to our next question:\n\n\nWhat basemap does my map need?\nThe basemap sets the tone for the rest of the layers in the map, as they can greatly change the visual complexity of the map.\n\nSatellite image - Geographically informative, but visually busy\nStreet map - Geographically informative, but visually busy\nHistorical image - Informative and busy, but black and white imagery can be less visually busy\nSimple topo map - Provides some information, but less visually busy\nNo basemap - Centres focus on data, but provides no geographic context",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#basemaps",
    "href": "QGIS/reports/QGIS_reports.html#basemaps",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Basemaps",
    "text": "Basemaps\nLet’s start by making a map we might use in an Academic Paper or in an Article for the Public, where we want to focus their attention on our data. Unless we really need Satellite imagery, or everything on the OpenStreetMap, it is too busy and distracting.\n\nLet’s load in a new basemap.\n\nThere are many ways of doing this, in the past we’ve used the QuickMapServices plugin to load in new basemaps, now we can go to the next step and source them externally. We can do this with WMTS (Web Map Tile Service), WCS (Web Coverage Service), ArcGIS REST Servers, and XYZ Tiles.\nToday we’re going to use an XYZ Tile from CARTODB.\n\nScroll down the Browser panel until your see XYZ Tiles\nRight click XYZ Tiles and select New Connection...\n\nIn Name type Voyager (no labels)\nIn URL paste:\nhttps://a.basemaps.cartocdn.com/rastertiles/voyager_nolabels/{z}/{x}/{y}@2x.png\nIncrease the Max. Zoom Level to 20\n\nThis value depends on what is available from the given service in different parts of the world. Increasing that value beyond 20 for this map in Brisbane will show “Map data not yet available” when you zoom in very close.\n\nClick OK\n\nIn the Browser panel, expand XYZ Tiles and double click on Voyager (no labels)\nNow do the same for this Light Gray Basemap from ESRI, but set the Max. Zoom Level to 16:\nhttps://services.arcgisonline.com/arcgis/rest/services/Canvas/World_Light_Gray_Base/MapServer/tile/{z}/{y}/{x}\n\nArrange the layers so that the Voyager (no labels) layer is under your other layers (you can hide the Light Gray Basemap and OpenStreetMap for now).\nIsn’t that much less visually busy?!",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#colours",
    "href": "QGIS/reports/QGIS_reports.html#colours",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Colours",
    "text": "Colours\nThe random colours that QGIS gives you aren’t always pretty, so let’s change the colours.\nChoosing a colour palette can be difficult and overwhelming, but once you have one, you can keep using those colours to form a more harmonious looking map. What story are you telling? Do you want your map to be bright and saturated, or warm and earthy?\nThe built in color ramps provide some good options I quite like rainfall (in Precipitation), Set1 (in QGIS), and wiki-knutux (in Topography).\nToday we’re being fancy, we’re going to import and build our own Color Ramp and Color Palette.\n\nIn your internet browser, navigate to colorbrewer2.org\n\nSet “Number of data classes” to 7\nSet “Nature of data classes” to “qualitative”\nChoose Set2 (should be the second from the right)\nClick and drag to highlight the 7 HEX codes listed (they should look like this #66c2a5)\n\nCopy those with Ctrl + C (or Cmd + C)\n\n\nBack in QGIS navigate to Settings &gt; Options...\n\nClick the Colors from the left menu\nFrom the top right of the window click the ... button and select New Palette..., name it Report Palette\nAdd your copied colours by clicking Paste colors on the right\nClick the ... button again and tick the Show in Color Buttons box\nClick OK\n\n\nWe now have a custom palette we can use throughout our projects!\nUnfortunately we need to set up the Color Ramp separately.\n\n\nNavigate to Settings &gt; Style Manager...\n\nClick the  Add item button at the bottom and select Color Presets...\n\nSelect the colour that appeared there and remove it by pressing  Remove color at the bottom\nAdd your copied colours by clicking Paste colors\nClick OK\n\nChange the Destination to Default\nGive it a meaningful Name like Report Colour Ramp\nAdd the Tag Palettes\nTick Add to favourites (this means it will appear in the color ramp dropdown, and not just in the All Color Ramps section)\nClick Save\n\nClick Close\n\nYou can also use the Style Manager to edit the default options and Favorites that appear in the Layer Styling window.\nNow we’ve set up our colours, we can start using them!",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#change-the-symbology",
    "href": "QGIS/reports/QGIS_reports.html#change-the-symbology",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Change the Symbology",
    "text": "Change the Symbology\n\nPoint Geometry\n\nCategories\n\nSelect the Artefacts layer from the Layer Styling panel\nIn the Layer Styling panel, change Single Symbol to Categorized\nFrom Value select Artefact Type\nFrom Color ramp choose our Report Colour Ramp\nClick the Classify button\n\n\n\nSymbols\nWith the Voyager (no labels) basemap, we can reduce the complexity of our symbology to make a cleaner looking map.\nIf you want, you can change the properties of all symbols at once:\n\nSimply click in the white space below your symbol classes so that none are selected.\nIf you click on the button next to Symbol, it will take you to Symbol SettingsWARNING if you edit any individual symbols before this step, it will reset any changes you made to those individual symbols.\n\nTry increasing the size of the symbols to 3mm\nClick Simple Marker and change the Symbol layer type to Filled Marker\n\nNow click Simple Fill and change its Symbol layer type to Outline: Simple Line\nChange the Stroke width to 0.4\nThis gives our points a very clean and simple appearance\n\n\nClick the back arrow button next to Symbol Settings\n\nNow that we have a nice colour scheme, let’s give it a new shape.\n\nDouble click on the symbol of the Artefact Scatter\n\nClick Filled Marker - this will allow us to change many things including the shape of the marker\n\nFrom the bottom of the window select a shape, let’s go with the Triangle\n\n\nClick the back arrow button next to Artefact Scatter\n\n\n\n\nPolygon Geometry\nDepending on your map, you may want your polygon to remain solid, to be transparent, have hatched lines, or simply be an outline. As our project area has items inside it, but an unimportant basemap, we can use an outline with a slightly transparent background to bring focus.\n\nOpen the Layer Styling panel by pressing F7 (or fn+F7)\nSelect the Project_area layer from either the Layer Styling panel, or the Layers panel.\nUnder Fill, click Simple Fill\n\nSet the Stroke width to 0.8mm\nClick the dropdown next to Fill color and select orange from our Report Palette\n\nClick the colour again and change the Opacity to 15%\n\n\n\nThat’s nicer, but let’s make it fancy. Let’s add a dropshadow.\n\nClick the  Add symbology layer next to Fill to add another fill symbol\n(You can add as many of these as you like, you can even change this with Symbol layer type to Arrows, Hashed lines, Filled lines, and more.)\nClick the new Simple Fill\n\nChange Symbol layer type from Simple Fill to Outline: Simple Line\nSet the Stroke width to 3\nScroll to the bottom, tick Draw Effects, then click the  Customise effects button (The one next to “Enable symbol layer”, not the one in the “Layer Rendering” section!)\n\nUntick Source\nTick and select Drop Shadow\n\nChange the Blur Radius to 6mm\nChange the Opacity to 30%\n\n\nClick the back arrow button next to Effects Properties\n\n\n\n\nLine Geometry\nWe can make our lines stand out. Sometimes it can be as simple as picking a contrasting colour, changing the width, or adding more symbol layers (with dashes, or arrows, or horizontal lines).\nIn this case, we want to keep the attention on the point data, let’s keep the paths, but reduce their contrast.\n\nSelect the Paths layer from the Layer Styling panel\nUnder Line, click Simple Line\n\nChange the Color to a colour in your Report Palette that is different to your points, I’m going to use green\nIncrease the Stroke width to 0.4mm\n\nTo reduce the focus on the paths, in the Layers panel, drag the Paths layer below the Project_area layer",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#themes",
    "href": "QGIS/reports/QGIS_reports.html#themes",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Themes",
    "text": "Themes\nWe can use themes to create multiple looks for our map that we can easily swap between. Let’s create a theme for our Data Map\n\nClick the Eye icon (Manage Map Themes) at the top of the Layers panel, and click Add theme...\n\nLet’s name our new theme Data Main, and click OK\n\n\nNothing exciting has happened yet, let’s change the layers a little to see the power of Themes.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#styles",
    "href": "QGIS/reports/QGIS_reports.html#styles",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Styles",
    "text": "Styles\nStyles work similarly to Themes, but instead of changing which layers are present, it changes between different Layer Styling options.\nIf you want to try different looks for a feature, but not have to undo everything, you can save them as Styles\n\nIn the Layers panel, Right click on Project_area &gt; Styles &gt; Rename Current...\n\nLet’s start by changing the name of our current style from default to Data Main\n\nLet’s create a new style, Right click on Project_area &gt; Styles &gt; Add...\n\nName the style Data Inset\n\nNow any Layer Style edits we do will be attached to the Data Inset style\n\n\nSelect the Project_area layer from the Layer Styling panel\n\nUnder Fill, click Simple Fill\nClick the Arrow Up on the right to move it above Simple Line\nChange the Fill color to an Opacity of 100%\nChange the Stroke width to 0.4mm\n\n\nNow we have a slightly different style to our Project_area that will look better in an inset map. We can edit this style and revert back to the original Data Main style whenever we like, we will explore how to use that with Themes next.\n\nThemes (again)\nNow that we’ve made some style changes, we can see what themes do.\n\nIn the Layers panel, hide the Voyager (no labels), Paths and Artefacts layers by unticking the boxes next to them.\nUnhide the Light Gray Basemap\n\n\n\nClick the Eye icon (Manage Map Themes) at the top of the Layers panel, and click Add theme...\n\nLet’s name our new theme Data Inset, and click OK\n\nNow, click the Eye icon again\n\nYou will see that there are two check box options at the bottom: Data Inset and Data Main\nYou can cycle between these two themes by choosing one or the other\nIf you want to update a theme, simply hide/unhide layers, change the layer Styles, go to Eye icon &gt; Replace theme... and then select the theme you want to overwrite.\n\nSave your project",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#layout-manager",
    "href": "QGIS/reports/QGIS_reports.html#layout-manager",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Layout Manager",
    "text": "Layout Manager\nNow we can prepare our map for export.\n\nNavigate to Project &gt; Layout Manager...\n\nUnder New from Template click Create, name the map Data Map\n\nThis will open the Layout Manager\n\n\nPage Properties\nBefore we do anything, let’s set up out environment.\n\nNavigate to Layout &gt; Page Properties..\n\nHere we can choose the size and orientation of our map.\nSize: A4 is often sensible, as it means you know the map will fit neatly into a report.\n\nHowever you may need to make the size suit the data, or the requirements of where you’re submitting the map\nWe’ll make ours 200x200\n\nOrientation: Portrait means you can have a full page map without rotating the page in your report. But some spatial layouts make more sense in Landscape. Use your judgement.\n\nToday we have manually changed the size, so this isn’t needed.\n\n\n\n\n\nGuides\nGuides seem unnecessary, but they make lining things up much much easier.\nWe will use a guide to place our map details.\n\nIn the right panel, click Guides\nClick  Add new guide under Vertical Guides (if your canvas is a Landscape, or you want your legend at the bottom, choose Horizontal Guides for this step)\nA new row should appear, click on the 0 and change it to 140\n\n\n\nMap\n\nAdd your map by clicking the map icon in the left menu, or navigating Add Item &gt; Add Map\n\nYour mouse will now be a +\nIf you hover over the top corner of the canvas, the mouse should snap to the corner\nClick and drag until your mouse snaps to the corner created by the bottom of the canvas, and the guide you added\n\nIn the Items panel on the right, double click on Map 1 and rename it Main\n\nGiving Items clear names makes this process much more straightforward\n\n\n\n\nScale\nOkay, we have our map, but it’s not zoomed in right. This is always a little tricky.\nYou could click the Move item content button on the left menu(or press C on your keyboard), as this would allow you to drag the map around internally, and zoom in with your scroll wheel. But there is an easier way!\n\nNavigate back to your qgis_reports window\n\nRight click on your Project_area layer and select Zoom to Layer(s)\n\nNavigate back to your Project Map window\n\nIn the right menu, click Item Properties\nYou will see these options: \nClick the first orange arrow button to Set Map Extent to Match Main Canvas Extent\nClick the third orange arrow button to Set Map Scale to Match Main Canvas Scale\n\nThe other buttons do the reverse\n\nI like to then round the Scale up to a nice round number from something like 3814 to 4000\n\nWhile you’re here, scroll to the bottom of the Item Properties, and tick the box for Frame, it adds a simple border to our map\n\n\n\nInset Map\nYou can add your inset map to the blank space at the side of you map canvas, or you can put it on top of you main map in unused space. We don’t have a lot of space around our map, so we’ll put it to the right. Guides are useful here too.\n\nIn the right menu, click Guides\n\nClick  Add new guide under Horizontal Guides twice\n\nIn the new row changethe 0 to 155\nIn the other new row change the 0 to 95\n\n\nAdd your map by clicking the map icon in the left menu, or navigating Add Item &gt; Add Map\n\nHover over the corner created by the top horizontal guide and the vertical guide, the mouse should snap to the corner\n\nClick and drag until your mouse snaps to the corner created by the side of the canvas, and the lower guide you added\n\n\nZoom out on the inset map, by selecting it from the Items tab and following the same approach as the Main map\n\nOr by using Move item content button on the left menu (or press C on your keyboard), clicking on the inset and using the scroll wheel\n\nIn the Items panel on the right, double click on Map 2 and rename it Inset\n\nNow we have our maps added, we can bring our themes into play again.\n\n\nThemes (again!)\n\nSelect Main from the Items panel\n\nIn Item Properties, you will see Layers section\nTick Follow map theme and from the dropdown change none to Main\nNow this map is linked to the Main theme and will only update when we change that theme, or the layers attributed to the theme\n\nRepeat this step for Inset\n\nThis is also a big advantage for using Styles, as the point is a better representation of our site on the inset map.\n\n\nText\nBefore we add any text, it’s worth thinking about what you want the font to be. You can manually set the font for each item in your Layout, or you can set the default font.\n\nNavigate to Settings &gt; Options\n\nFrom the left menu, select Layouts\nChange the default font to your preferred font. I’m going to select Sitka Text Semibold\n\n\n\n\nTitle\n\nAdd a text box to your map by clicking the Add Label icon in the left menu, or navigating Add Item &gt; Add Label\nClick and drag in the empty canvas below the map where you want to put your title\n\nIn the Item Properties panel remove the Lorem ipsum text, and add Brisbane Botanic Gardens\nUnder Appearance click the Font box to open the Label Font window\n\nChange the text size to 18, you can make other font changes here too\nClick the back arrow button next to Label Font\n\n\nIn the Items panel on the right, double click on Brisbane Botanic Gardens and rename it Title\n\nOkay, that’s a some fairly basic text, let’s do something interesting with the descriptive text.\n\n\nDynamic Text\n\nAdd another text box to your map by clicking the Add Label icon in the left menu, or navigating Add Item &gt; Add Label\n\nClick and drag in the empty canvas below the inset map\nIn the Items panel on the right, double click on “Lorem ipsum” and rename it Description\n\nIn the Item Properties panel remove the “Lorem ipsum” text and then enter:\n\nCreated by:\nDate:\nScale: 1:\nDatum:\nNormally we’d have to enter the details for each of these manually, but we can use Dynamic Text to do it for us\n\n\n\nIn the Item Properties panel, click next to Date:, so the cursor is sitting there\n\nUnder the Main Properties text box, click Dynamic Text &gt; Current Date &gt; Day/Month/Year (26/09/2025)\nThis will insert code that looks like this: [%format_date(now(), 'dd/MM/yyyy')%] which automatically adds today’s date\n\nNow repeat those steps for the rest:\n\nClick next to Scale: 1:\n\nClick Dynamic Text &gt; Map Properties &gt; Main &gt; Scale (4000)\n\nThe code will look like this: [%format_number(item_variables('Main')['map_scale'] which automatically adds the main map’s scale\n\n\nClick next to Datum:\n\nClick Dynamic Text &gt; Map Properties &gt; Main &gt; CRS Name (GDA2020 / MGA zone 56)\n\nThe code will look like this: [%item_variables('Main')['map_crs_description']%] which automatically adds the main map’s CRS\n\n\n\nUnder Appearance:\n\nSet the Horizontal margin and Vertical margin to 2mm\n\n\nLet’s add another text box for our sources, but we’ll make the font smaller\n\nAdd another text box to your map by clicking the Add Label icon in the left menu, or navigating Add Item &gt; Add Label\n\nClick and drag in the empty canvas below the inset map\nIn the Items panel on the right, double click on “Lorem ipsum” and rename it Sources\nIn the Item Properties panel remove the “Lorem ipsum” text and then enter:\n\nSources:\n\n\nClick next to Sources:\n\nClick Dynamic Text &gt; Layer Credits\n\nThe code will look like this: [%array_to_string(map_credits())%] which automatically adds the sources built in to some of the data used\nNote that this information lives in the Metadata (layer Properties &gt; Metadata &gt; Access under Rights) of the baselayers, and the layers we downloaded. Not all layers will necessarily have this information, and sometimes it’s very detailed and clunky. You can add this information in the layer’s metadata if it is missing.\nIt may be simpler to manually enter this to your map.\n\n\nUnder Appearance:\n\nSet the Horizontal margin and Vertical margin to 2mm\nClick Font\n\nSet the Size to 8mm\n\n\n\nYou could also add any extra text details such as the locality, disclaimers, or any other details needed.\n\n\nShapes\nWe can add a border around our layout. This isn’t necessary, but it looks nice! It also has greater customisability than the map frame options.\n\nAdd a rectangle to your map by clicking the Add Shape icon in the left menu, or navigating Add Item &gt; Add Shape &gt; Add Rectangle\n\nHover over the top left corner of the canvas, the mouse should snap to the corner\nClick and drag until your mouse snaps to the opposite corner of the canvas\n\nIn the Item Properties panel, click the box next to Style, this will take you to the Symbol Settings window\n\nClick Simple Fill\n\nChange Symbol layer type from Simple Fill to Outline: Simple Line\nIncrease the Stroke width to 1mm\nScroll to the bottom, tick Draw Effects, then click the  Customise effects button\n\nUntick Source\nTick and select Outer Glow\n\nChange the Spread to 0.8mm\nChange the Blur Radius to 2.5mm\nChange the Single color to Black\n\n\n\n\nIn the Items panel, change the name from &lt;Rectangle&gt; to Outline\n\nThen tick the lock box next to the Outline\nThis means it stays on top of everything else, but you won’t select it every time you click the map canvas\n\n\n\n\nLegend\n\nAdd a Legend your map by clicking the Add Legend icon in the left menu, or navigating Add Item &gt; Add Legend\n\nThe legend always has too much detail to start with. Let’s clean it up.\n\nIn the Item Properties panel, under Legend Items untick Auto update\n\nClick the drop down next to Artefacts, select the empty item, and then click the red – to remove it\nRemove the Imagery layers too\nRight click on Artefacts and select Hidden, we don’t need to see the group name\nDouble click on Project_area and rename it to Project Area\nDouble click on Paths – paths and rename it to Footpaths\n\nScroll down to Fonts and Text Formatting, under Item Labels click the Item font button\n\nChange the Size to 15\n\nScroll down to Symbol\n\nChangeSymbol width to 10\nChangeSymbol height to 12\n\n\n\nCustom Legend Symbols\nOkay, we have a legend, but let’s get really fancy with a custom polygon legend.\n\nSave your Layout\n\nLayout &gt; Save Project\n\nNavigate back to QGIS (it’s still open in another window)\n\nSelect the Project_area in the Layers Panel\nFrom the top toolbars open the  Field Calculator\n\nIn the Expression window paste geom_to_wkt($geometry)\nUnder the Expression window, right click on the text next to Preview: (it should start with “MultiSurface”\n\nSelect Copy Expression Value\n\nClose the Field Calculator\n\n\nBack in the Layout Manager, in the Item Properties panel, double click on Project Area\n\nUnder Patch, click the Shape box\n\nThere should be a large Shape box with some “Polygon” details inside. Delete this.\nPaste what you copied from the Field Calulator - it should be many lines\nBe careful to delete the single quotes ’ at the start and end of what you’ve just pasted in\nTick the Preserve aspect ratio box\n\nClick the back arrow button\n\nClick the back arrow button again\n\n\n\n\nNorth Arrow\n\nAdd a North Arrow to your map by clicking the Add North Arrow icon in the left menu, or navigating Add Item &gt; Add North Arrow\n\nClick and drag in the blank space under the map until you’re happy with the size of the arrow\nIn the Item Properties panel, under the SVG Images box, type arrow in the search bar\n\nChoose a North arrow you like\nScroll down to SVG Parameters\n\nSet the Fill color and Stroke color to Black\n\n\n\n\n\n\nScalebar\n\nAdd a scale bar to your map by clicking the Add Scale Bar icon in the left menu, or navigating Add Item &gt; Add Scale Bar\n\nYou can click and drag in the blank space under the map, although the scale bar size will be set in a moment\n\nIn the Item Properties panel:\n\nChange the Style from Single Box to Line Ticks Up\nScroll down to Appearance:\n\nChange the Label margin to 0mm\nChange Distance label placement from Above Segment Edges to Below Segment Edges\n\n\n\n\n\nFinessing\n\nGo to the Layout panel and scroll down to Resize Layout to Content\n\nSet the Top, Right, Left and Bottom Margin to 2mm\nClick Resize layout\nNow we can see the dark glow we added to the Outline, without interfering with our neat 200x200 layout\n\n\nSave your project.\n\nLayout &gt; Save Project\n\n\n\nExport options\nNow that we’re done we can export. Generally to PDF or Image. Today we will use image.\n\nLayout &gt; Export as Image...\n\nYou will see a popup warning about WMS servers. Some tile services have limits to the size of file you can export like this. If you exceed it, your basemap will be blank.\nClick OK\n\nNavigate to your project’s output folder, and give the file a meaningful name “Brisbane Botanic Gardens Map”, click Save\nThe Export resolution should automatically be a print quality 300dpi, and the width and height should match the page size we set at the beginning.\nClick Save\nNavigate to and open your file to make sure it worked!",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#save-a-map-template",
    "href": "QGIS/reports/QGIS_reports.html#save-a-map-template",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Save a Map Template",
    "text": "Save a Map Template\nThis is crucial! Save your hard work for next time!\n\nLayout &gt; Save as Template...\nNavigate to a folder you can easily find again, such as a broader QGIS folder.\nGive it a meaningful name “Report_Map_Template”\nClick Save",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#load-a-map-template",
    "href": "QGIS/reports/QGIS_reports.html#load-a-map-template",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Load a Map Template",
    "text": "Load a Map Template\nNext time you need to make an export you can load your template in.\nOpen an old QGIS Project.\n\nNavigate to Project &gt; Layout Manager...\n\nUnder New from Template change Empty Layout to Specific, click the three dots and navigate to your Report_Map_Template, select it, and click Open\nClick Create\nGive the layout an appropriate name, and click OK\n\nThis will open the Layout Manager\n\nYou will see that the inset and main maps are in the wrong location, the title is wrong, the scale bar is off, and the Legend is empty. But you know how to fix these!\n\nMap Scale\n\nNavigate back to your QGIS window\n\nRight click on the most suitable layer and select Zoom to Layer(s)\n\nNavigate back to your Layout Manager window\n\nClick the Main from the Item panel, and click the Item Properties panel\n\nClick the first orange arrow button to Set Map Extent to Match Main Canvas Extent\nClick the third orange arrow button to Set Map Scale to Match Main Canvas Scale\n\nThe other buttons do the reverse\n\nRound the Scale up to a nice round number\n\nFix the inset map too\n\n\n\nScalebar\n\nClick the Scalebar from the Item panel\nIn the Item Properties panel, change Scalebar units to Metres or Kilometres - Change Fixed width to an appropriate unit\n\n\n\nLegend\n\nClick the Legend from the Item panel\nIn the Item Properties panel, under the Legend Items box and click the green + to add a layer\n\nDouble click on and rename the layer if needed\nRepeat for all layers you need to list\n\n\n\n\nTitle\n\nClick the Title from the Item panel\nIn the Item Properties panel, under the Main Properties rename the title\n\nAnd that’s it, you now have another map looking the way you want it, without too much hassle.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#feedback",
    "href": "QGIS/reports/QGIS_reports.html#feedback",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "Feedback",
    "text": "Feedback\nPlease visit our website to provide feedback and find upcoming training courses we have on offer.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/reports/QGIS_reports.html#more-data-sources",
    "href": "QGIS/reports/QGIS_reports.html#more-data-sources",
    "title": "QGIS: Refine Your Maps for Reporting and Publication",
    "section": "More Data Sources",
    "text": "More Data Sources\n\nAerial Imagery\nThere are a few places you can aquire aerial photography, today we will look at two sources, one is freely available Government Data from QImagery, the other is accessible from using your UQ credentials.\n\nGo to QImagery\nRead and tick the “I acknowledge I have read and agree to the Terms & Conditions” box, and click Get Started\nClick the Search button, select ‘locality, town or city’ and search for “St Lucia” in the “Enter search term” search box and select the first result\nIt will zoom to your selected location then click the newly appeared Search button\nFrom here you can select from a wide array of images of QLD over many years.\nClick one of the drop-downs and hover over the options to see where those images are located. Preview the image by clicking View.\nYou can then download your desired images by clicking “Download” and selecting TIFF (georeferenced)\nMove the TIFF file(s) to your project folder, and open them in QGIS\n\nYou can also get historical images from Geoscience Australia’s Historical Aerial Photo (HAP) Collection\n\n\nGeoreferencing\nI will be happy to go over this in person, but if you need a guide, here is the QGIS how to\n\n\nMore Web Mapping Services\nAdd any of these URLs via ArcGIS REST Servers in QGIS\nQLD Gov Aerial Basemap\nQLD Gov Roads\nQLD Heritage Areas\nMore QLD Gov Services\nBrisbane/SEQ Utilities\nBCC Data\nRandom User’s Data\n10000+ Searchable Feature Layers from ArcGIS\nNearMap API",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "2. Refined Maps for Publication"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html",
    "href": "QGIS/intro/QGIS_intro.html",
    "title": "QGIS: Introduction to map making",
    "section": "",
    "text": "This tutorial is designed for QGIS 3.40. If you need to install it on your computer, go to the QGIS website.\n\n\nWe will start by creating a good folder structure to work within. This folder is where our project, our data, and creations will live. Folder structure is very important for keeping your data tidy, as well as for ease of sharing your project with others. You simply need to zip the project folder if you need to share the whole thing.\n\nOpen QGIS and create a new project with Project &gt; New.\nLet’s now save our project: Project &gt; Save.\nNavigate to where you want to save your project, and create a new folder, let’s call it “qgis_intro”.\n\n\n\n\nInside that folder, create these folders:\n\n“data” - for all the data we will use to make our maps, split into:\n\n“raw” - raw data from your research or the internet\n“processed” - any data you’ve modified\n\n“output” - for any maps or images we export\n“temp” - this folder isn’t necessary, but when you’re playing around and testing, it stops things getting messy.\n\nFinally, let’s save our .qgz project file inside the “qgis_intro” folder, named “qgis_intro_map.qgz”\n\n\n\n\n\n\n\n\n\nYour .qgz file should always be in the highest level folder, so it’s only looking down into folders for data, not back out. This might feel unnecessary now, but things quickly get out of control and hard to find if you don’t have a good folder structure.\n\n\n\nThe data for this session can be directly downloaded from this link.\nOnce downloaded, extract the archive into your raw folder within your data folder.\n\nSome of our data comes from Natural Earth: http://www.naturalearthdata.com/",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#setting-up",
    "href": "QGIS/intro/QGIS_intro.html#setting-up",
    "title": "QGIS: Introduction to map making",
    "section": "",
    "text": "This tutorial is designed for QGIS 3.40. If you need to install it on your computer, go to the QGIS website.\n\n\nWe will start by creating a good folder structure to work within. This folder is where our project, our data, and creations will live. Folder structure is very important for keeping your data tidy, as well as for ease of sharing your project with others. You simply need to zip the project folder if you need to share the whole thing.\n\nOpen QGIS and create a new project with Project &gt; New.\nLet’s now save our project: Project &gt; Save.\nNavigate to where you want to save your project, and create a new folder, let’s call it “qgis_intro”.\n\n\n\n\nInside that folder, create these folders:\n\n“data” - for all the data we will use to make our maps, split into:\n\n“raw” - raw data from your research or the internet\n“processed” - any data you’ve modified\n\n“output” - for any maps or images we export\n“temp” - this folder isn’t necessary, but when you’re playing around and testing, it stops things getting messy.\n\nFinally, let’s save our .qgz project file inside the “qgis_intro” folder, named “qgis_intro_map.qgz”\n\n\n\n\n\n\n\n\n\nYour .qgz file should always be in the highest level folder, so it’s only looking down into folders for data, not back out. This might feel unnecessary now, but things quickly get out of control and hard to find if you don’t have a good folder structure.\n\n\n\nThe data for this session can be directly downloaded from this link.\nOnce downloaded, extract the archive into your raw folder within your data folder.\n\nSome of our data comes from Natural Earth: http://www.naturalearthdata.com/",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#interface",
    "href": "QGIS/intro/QGIS_intro.html#interface",
    "title": "QGIS: Introduction to map making",
    "section": "Interface",
    "text": "Interface\nThe main elements of the interface are:\n\nthe Browser panel, to navigate our file system and data sources\nthe Layers panel, to organise the different layers that compose our map\nthe Canvas, where we see the map.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#load-layers",
    "href": "QGIS/intro/QGIS_intro.html#load-layers",
    "title": "QGIS: Introduction to map making",
    "section": "Load layers",
    "text": "Load layers\n\nBasemap\nWe can straight away load an OpenStreetMap basemap to have a background: in the Browser panel, XYZ Tiles &gt; OpenStreetMap.\n\n\nControls\nThe default mouse mode is Pan Map, which allows to drag the map around. We can use the mouse wheel to zoom in and out, and press the Ctrl key to use more precision.\n\n\nVector data\nTo add our own data: Layer &gt; Add Layer &gt; Add Vector Layer..., choose the “ne_50m_admin_1_states_provinces” file, and click “Add”. It is now in your Layers panel.\nYou can also use the Project Home directory in the browser to add another layer: try loading the “populated places” and “rivers” shapefiles (format .shp), by drag-and-dropping or double-clicking.\nAll layers are visible in the “Layers” panel, with the most recently loaded by default at the top. Layers are rendered as they are listed, which means the top ones will cover the bottom ones. Make sure you optimise the order for your map!\nYou can also turn the layer visibility on and off from this panel with the tick box.\n\n\nLearn about the data\nYou can open the attribute table to see tabular data contained in each layer. (Right-click on a layer and use “Open Attribute Table”. Alternatively, use the top toolbar button or use F6.)\nYou can also use the Identify Features tool in the top toolbar. This allows you to learn about specific features included in the currently selected layer.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#subset-the-data",
    "href": "QGIS/intro/QGIS_intro.html#subset-the-data",
    "title": "QGIS: Introduction to map making",
    "section": "Subset the data",
    "text": "Subset the data\nWe currently have data from all around the World, which you can confirm by using the Zoom Full button. However, we want to focus on Australia exclusively.\nTo remove the useless regions, we can do the following:\n\nSelect the “admin” layer\nUse the Select Features button, and draw a rectangle around Australia, so all the region appear in yellow\nRight-click on the layer and use Export &gt; Save Selected Features As...\nName the new file “australia_admin” in your project directory, and click OK.\n\nThe new layer is loaded to the map by default.\nNow that we have we have a layer with only the administrative regions for Australia, we can use that as an overlay layer to clip the rivers. Follow these steps:\n\nSelect the layer you want to clip, in this case ne_10m_rivers_lake_centerlines\nUse the Vector &gt; Geoprocessing Tools &gt; Clip... tool\nUse the “australia_admin” layer as the overlay.\nClick “Run”\n\n\nIf you get an error about invalid geometries, you might have to use the Fix geometries tool on the Australian shapefile first (found in the Processing Toolbox).\n\nNotice how the clipped layers don’t have a descriptive name? Make sure you rename them. Let’s changed Clipped to australia_rivers_10m\nYou can now hide the original layers and confirm that you have only Australian data visible in the canvas by using the Zoom Full button.\n\nChallenge: can you do the same for the Cities? Do you notice anything strange? How can we fix that?",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#saving-your-project",
    "href": "QGIS/intro/QGIS_intro.html#saving-your-project",
    "title": "QGIS: Introduction to map making",
    "section": "Saving your project",
    "text": "Saving your project\nIt’s always a good idea to regularly save your progress.\nAnd! Notice the little icon next to some of your layers? We previously created “temporary scratch layers”, they’re basically temp files that will disappear when we close QGIS. This is useful if you keep processing data and creating new layers that you want to discard afterwards. In our case, we do want to keep the “cities” and “rivers” layers, so we need to save them to a file. If we try to close QGIS with scratch layers loaded, it will give you a warning that they will be lost in the process.\nYou can click on the scratch layer icon to save the file. In the dialog, we can give the layers a File name (in our project’s home directory) and click OK.\nYou can save your project with the floppy disk icon, or using Ctrl + S, and the project should be visible in a list when you re-open QGIS again.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#change-the-symbology",
    "href": "QGIS/intro/QGIS_intro.html#change-the-symbology",
    "title": "QGIS: Introduction to map making",
    "section": "Change the symbology",
    "text": "Change the symbology\nOur vector layers are assigned a random colour when we load them. We now want to make them look better.\nDouble-click on the “admin” layer to open the Layer Properties click the  Symbology tab, and use Color to change to a better colour. Click Apply. We can also go into more details about the border (colour, width, type of line…) by clicking on Simple fill.\nWe can make more complex fills by adding an extra “symbol layer”, and layering different kinds of fill styles. Try this with both polygons and lines. For example, style the rivers so they look like water (the topo hydrology preset is a good start).\nNow try to change the point symbology of our “populated places” layer: we don’t have to stick to Simple marker. Try for example the different icons available with Symbol layer type &gt; SVG marker or Font marker selected.\n\nThe Properties dialog is quite large and makes it difficult to see the changes in symbology. You can use the “Layer Styling Panel” (button in the Layers panel, or F7 shortcut) to use a side panel instead, with the “Live update” option on by default.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#classify-vector-data",
    "href": "QGIS/intro/QGIS_intro.html#classify-vector-data",
    "title": "QGIS: Introduction to map making",
    "section": "Classify vector data",
    "text": "Classify vector data\n\nAdd labels\nHave a look at the attribute table for our “populated places” layer. What could we use to label our points?\nDouble-click on the “populated places” layer:  Labels and then click No Labels &gt; Single labels. You can also use the Layer Labelling Options button to open the sidebar, which allows you to “Live update” the map when you change a setting.\nChoose Value &gt; Name to label with the city names. In the “Text” tab, we can change the look of our labels: amend the size, the colour, the font… We can also add a text buffer with the “Buffer tab” to make them more readable.\nThe placement tab allows us to fine-tune the placement of the labels in reference to the points. The default mode is “Cartographic”, and we can increase the distance to the symbol used. If we want the label to cover the point, we can use Mode &gt; Offset from point and use the middle quadrant. We might also want to set the layer’s Symbology to “No Symbols”.\n\nLabel Lines\nWhat if you want to label lines? It could be a good idea to change the default Placement from Parallel to Curved so the labels follow the shape of the lines. You can try it on the rivers. If the labels don’t show, increasing the Overrun Distance by a small amount might help.\n\nIf labels appear several times on the same line, you can try using Rendering &gt; Merge connected lines to avoid duplicate labels. You can also make it neater with the feature Suppress labelling of features smaller than, so QGIS doesn’t try to render labels for tiny features.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#tabular-data",
    "href": "QGIS/intro/QGIS_intro.html#tabular-data",
    "title": "QGIS: Introduction to map making",
    "section": "Tabular data",
    "text": "Tabular data\nQGIS can deal with plain tabular data. For example, try importing the file HDI.ods: it doesn’t contain any coordinates, but we can still store it as a layer and use it in our project. You can see the data it contains by opening its attribute table.\nHow can we add the Human Development Index (HDI) data to our existing “admin” shapefile?\n\nJoining tabular data\nTo add the HDI data to our region shapefile, we can go the the “admin” layer’s properties, use the  Joins tab and click the Add new join button  to create a new join. We can then choose the HDI data as the Join layer, and define what common field between the two tables we will merge on: the ISO_3166_2 code in our case.\nYou can now see the joined data highlighted in green in the Fields tab, click “OK”, and check the actual values in the “admin” layer’s attribute table.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#save-styles",
    "href": "QGIS/intro/QGIS_intro.html#save-styles",
    "title": "QGIS: Introduction to map making",
    "section": "Save styles",
    "text": "Save styles\nWe can save styles with right-click &gt; Styles &gt; Add, which creates a new one and saves the previous version.\n\nClassifying the symbology\nWhat if we want to colour according to a variable? Depending on the type of variable, we can choose “Categorised” (for categorical data) or “Graduated” (for continuous data).\nWe can now use the joined data to change the symbology of our regions according to the HDI:\n\nClick on the Layer Styling Panel button\nChange from “Single Symbol” to “Graduated”\nUse the HDI column\nPick a relevant colour ramp\nClick “classify”\n\nDepending on the data that you deal with, different “Modes” will be more or less useful to create a good colour key. Try “Natural Breaks (Jenks)” for example.\n\nData Defined Symbology with the Assistant Tool\nStill using a continuous numerical variable, we can change the size of the “populated places” symbols according to population. Use the Data defined override dropdown next to the Size box, and use the Assistant... to define the scale: set the “Source” to pop_max and click the “Fetch value range from layer” button. If the “Size from” value is too small, we can amend it.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#add-a-raster-layer",
    "href": "QGIS/intro/QGIS_intro.html#add-a-raster-layer",
    "title": "QGIS: Introduction to map making",
    "section": "Add a raster layer",
    "text": "Add a raster layer\n\n\n\n\n\n\nNoteIt’s likely worth skipping this Solaran raster data section, but expand this if you want to include it in today’s session\n\n\n\n\n\nThe solaran file is average solar exposure data from the Australian Bureau of Meteorology. It is a text file that contains data organised in a grid: it is raster data.\nTry loading the solaran.txt file: data about average solar exposure. What is the difference with the other files? What kind of layer is it imported as? What can you do to make it easier to read?\nWhen importing data, QGIS might find everything it needs in the file(s) and add the layer as expect with only a drag-and-drop. Other times, we might have to be more specific by using the proper data type in the Layer &gt; Data Source Manager and using the right settings.\nIn our case, we can import the solaran file with Data Source Manager &gt; Raster, provide the path to the file in Source, and click Add. The file does not contain CRS information, which means we have to specify it in the following dialogue: WGS 84 (EPSG:4326).\n\nStyle a raster layer\nThis layer is a single-band raster layer, which gets coloured automatically in greyscale. We can change that in the symbology, set the Render type as “Singleband pseudocolor”, use the Color ramp called “Spectral”, and use the Color ramp drop-down menu to “Invert Color Ramp” in order to make the colouring more intuitive (i.e. red for high values and blue for low values).\n\nTo learn more about rasters, have a look at our Raster Analysis course",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#plugins",
    "href": "QGIS/intro/QGIS_intro.html#plugins",
    "title": "QGIS: Introduction to map making",
    "section": "Plugins",
    "text": "Plugins\nA wealth of plugins is available: Plugins &gt; Manage and Install Plugins.... Try for example to install QuickMapServices, to easily access many sources of raster data.\nThe plugin should automatically open a side panel in which you can search for terms. For example, search for “satellite” and add the ESRI satellite imagery to your project.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#create-a-layout-to-export",
    "href": "QGIS/intro/QGIS_intro.html#create-a-layout-to-export",
    "title": "QGIS: Introduction to map making",
    "section": "Create a layout to export",
    "text": "Create a layout to export\nClick on Show Layout Manager in the toolbar. Create a new layout called “Default”. We can now see the Layout window.\nWe can now add elements to our layout: the map, a legend, a scale bar, a north arrow, text boxes for a title and data sources… In order to create a nice printout, or export in a variety of formats. You can select these from the left-hand-side toolbar, or by going to the Add Item menu up the top.\nThe layout manager allows us to save several different layouts, one for each output we want to generate. Even if we change our map data, we can reuse the same layouts and export an updated map.\n\nIf you need to zoom in on the Map you’ve added, you’ll need to change your edit tool the the Move item content tool by clicking it in the sidebar, going to Edit &gt; Move content, or by pressing C on your keyboard. If you’re having trouble zooming in exactly where you want to on the map, you can try the following: * Go back to your project window * Right click on the layer that covers the full extent of your area of interest and select Zoom to Layer(s) * Navigate back to the layout manager window, select your map from the Items tab, and click ont he Item Properties tab. * Click the Set Map Extent to Match Main Canvas Extent button  * Your map should zoom to match the extent your zoomed to in the project window.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/intro/QGIS_intro.html#feedback",
    "href": "QGIS/intro/QGIS_intro.html#feedback",
    "title": "QGIS: Introduction to map making",
    "section": "Feedback",
    "text": "Feedback\nPlease visit our website to provide feedback and find upcoming training courses we have on offer.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "1. Introduction"
    ]
  },
  {
    "objectID": "QGIS/data_sources/QGIS_data_sources.html",
    "href": "QGIS/data_sources/QGIS_data_sources.html",
    "title": "QGIS: Data Sources",
    "section": "",
    "text": "This is a list of Spatial data sources that you might find useful. Some are used in our workshops, others didn’t fit, but are still very valuable. Some of the data sources will have instructions for extracting the data, some will not. This is primarily a repository to refer to and use to find data.\n\n\nSunshine Coast MyMaps\n\n\n\nCSIRO’s Norfolk Island Data Portal CSIRO spatial products.\n\n\n\n\n\nA Digital Elevation Model (DEM) is a common example of raster data, i.e. grid data that contains a value in each cell (a bit like the pixels in a coloured picture).\nFor this tutorial, we are using a DEM sourced from ELVIS - Geoscience Australia’s ELeVation Information System.\n\nGo to http://www.ga.gov.au/elvis/\nSearch for “St Lucia” in the Location Search search box and select the first result\nClick Order Data\nChoose “Draw” and “Box” and then click the Draw button\nClick on the map to start drawing a rectangle around your desired area\nThen click Search\nThe right panel will show you all the different datasets available in that area\nWe want the QLD Government Digital Elevation Model 1 Metre, click the down arrow on the right\nAs you hover over the different options, they will highlight a red box on the map, click the tick box and select all that overlap the area you’re interested in (note that there may be data from different years here)\nWhen you have the data you want, scroll to the bottom of the Order Data window\nSelect your industry, enter your email, tick the reCAPTCHA, and click the Order Datasets button\nYou should receive an email within 5 minutes, download the files from the link in the email, extract the folder to your project folder, and add them to your map.\n\n\n\n\nThere are a few places you can aquire aerial photography, today we will look at two sources, one is freely available Government Data, the other is accessible from using your UQ credentials.\nAs a UQ student, you also have access to very high resoltuion imagery from NearMap. You can even access an array of imagery going back in time.\n\nGo to the UQ NearMap Portal\nEnter your UQ Student (@student.uq.edu.au) or Staff (@uq.edu.au) email address, with the appropriate domain selected. Click “Invite me”.\nYou should receive an email soon after, click the “Accept Invitation” button, and go through the account setup process.\nGo to Login and enter your email address, click “Next” and enter your password.\nFrom the top right select MapBrowser.\nType “St Lucia, QLD” in the search box, press enter\nYou can click the date down the bottom to look at different snapshots in time, and even compare maps side-by-side.\nTo save imagery from NearMap, simply click the “Exports” button on the left handside (it is an image icon)\nFrom the menu that appears change the “Export type” to “Georeferenced image”\nFrom “Projection” choose GDA2020 / MGA zone 56\nYou can increase and decrease the size of the bounding box by adjusting its corners, a smaller box means you can have a finer resolution, down to 0.075m. If we select all of UQ St Lucia in one go, the highest resolution we can have is 0.299m.\nOnce you’re happy with your selection click Download Files\nMove the downloaded zip file to your project folder, and open them in QGIS\n\n\n\n\n\n\n\nYou can access a wide variety of QLD Government Data, including Spatial Data such as lot plans and vegetation maps, from QLD Spatial. There are three ways to access this data. You can download all of it, you can select a portion for download, or you can live load it into your project. Today we download the files covering our project area.\n\nTo access data from QLD Spatial go to https://qldspatial.information.qld.gov.au/\nSearch for “property boundaries”\nScroll down to “Property boundaries Queensland” and click Add to my list\nClick My List from the top menu\nClick View/extract in map\nUnder Extractable Data, click the box next to Property boundaries Queensland, it will become green\nClick Extract/download\nClick Choose an area\nSelect BRISBANE CITY from the Select LGA option\nChoose GeoPackage 1.0 from Select output format\nEnter your Email Address\nAccept the Terms and Conditions\nClick Extract/Download\nYou can can also just download the preprepared data here, just press the three dots and download.\nMove the downloaded zip file to your project raw data folder, and open the shapefile in QGIS\n\n\n\n\nAnother way to access QSpatial Data is using the Queensland Globe portal.\n\nTo access data from Queensland Globe go to https://qldglobe.information.qld.gov.au\nAccept the T&Cs.\nClick Search, Select Locality (Suburb) within a Local Government Area, and search for the location you want and select it from the list.\nClick Layers, click Add Layers + here you can scroll through and filter different layer types\nWe want to tick the box next to Planning Cadastre &gt; Land Parcels &gt; Land Parcel (you may need to zoom in to see certain layers)\nTo export data click the Wrench Icon in the top right, and then click the Identify icon (i)\nUse the triangular Identify Polygon tool to select and area of interest. Double click when you’ve finished selecting your area.\nThe Layers menu will now show your selection. You can download all, or sections, of your selection.\nI will choose Land Parcel and then Download as a shp file.\nNote: You may need to disable other layers for this to work correctly. I needed to turn off the Transportation layer to prevent roads from being included in my selection."
  },
  {
    "objectID": "QGIS/data_sources/QGIS_data_sources.html#council-data",
    "href": "QGIS/data_sources/QGIS_data_sources.html#council-data",
    "title": "QGIS: Data Sources",
    "section": "",
    "text": "Sunshine Coast MyMaps"
  },
  {
    "objectID": "QGIS/data_sources/QGIS_data_sources.html#federal-data",
    "href": "QGIS/data_sources/QGIS_data_sources.html#federal-data",
    "title": "QGIS: Data Sources",
    "section": "",
    "text": "CSIRO’s Norfolk Island Data Portal CSIRO spatial products."
  },
  {
    "objectID": "QGIS/data_sources/QGIS_data_sources.html#raster-data",
    "href": "QGIS/data_sources/QGIS_data_sources.html#raster-data",
    "title": "QGIS: Data Sources",
    "section": "",
    "text": "A Digital Elevation Model (DEM) is a common example of raster data, i.e. grid data that contains a value in each cell (a bit like the pixels in a coloured picture).\nFor this tutorial, we are using a DEM sourced from ELVIS - Geoscience Australia’s ELeVation Information System.\n\nGo to http://www.ga.gov.au/elvis/\nSearch for “St Lucia” in the Location Search search box and select the first result\nClick Order Data\nChoose “Draw” and “Box” and then click the Draw button\nClick on the map to start drawing a rectangle around your desired area\nThen click Search\nThe right panel will show you all the different datasets available in that area\nWe want the QLD Government Digital Elevation Model 1 Metre, click the down arrow on the right\nAs you hover over the different options, they will highlight a red box on the map, click the tick box and select all that overlap the area you’re interested in (note that there may be data from different years here)\nWhen you have the data you want, scroll to the bottom of the Order Data window\nSelect your industry, enter your email, tick the reCAPTCHA, and click the Order Datasets button\nYou should receive an email within 5 minutes, download the files from the link in the email, extract the folder to your project folder, and add them to your map.\n\n\n\n\nThere are a few places you can aquire aerial photography, today we will look at two sources, one is freely available Government Data, the other is accessible from using your UQ credentials.\nAs a UQ student, you also have access to very high resoltuion imagery from NearMap. You can even access an array of imagery going back in time.\n\nGo to the UQ NearMap Portal\nEnter your UQ Student (@student.uq.edu.au) or Staff (@uq.edu.au) email address, with the appropriate domain selected. Click “Invite me”.\nYou should receive an email soon after, click the “Accept Invitation” button, and go through the account setup process.\nGo to Login and enter your email address, click “Next” and enter your password.\nFrom the top right select MapBrowser.\nType “St Lucia, QLD” in the search box, press enter\nYou can click the date down the bottom to look at different snapshots in time, and even compare maps side-by-side.\nTo save imagery from NearMap, simply click the “Exports” button on the left handside (it is an image icon)\nFrom the menu that appears change the “Export type” to “Georeferenced image”\nFrom “Projection” choose GDA2020 / MGA zone 56\nYou can increase and decrease the size of the bounding box by adjusting its corners, a smaller box means you can have a finer resolution, down to 0.075m. If we select all of UQ St Lucia in one go, the highest resolution we can have is 0.299m.\nOnce you’re happy with your selection click Download Files\nMove the downloaded zip file to your project folder, and open them in QGIS"
  },
  {
    "objectID": "QGIS/data_sources/QGIS_data_sources.html#vector-data",
    "href": "QGIS/data_sources/QGIS_data_sources.html#vector-data",
    "title": "QGIS: Data Sources",
    "section": "",
    "text": "You can access a wide variety of QLD Government Data, including Spatial Data such as lot plans and vegetation maps, from QLD Spatial. There are three ways to access this data. You can download all of it, you can select a portion for download, or you can live load it into your project. Today we download the files covering our project area.\n\nTo access data from QLD Spatial go to https://qldspatial.information.qld.gov.au/\nSearch for “property boundaries”\nScroll down to “Property boundaries Queensland” and click Add to my list\nClick My List from the top menu\nClick View/extract in map\nUnder Extractable Data, click the box next to Property boundaries Queensland, it will become green\nClick Extract/download\nClick Choose an area\nSelect BRISBANE CITY from the Select LGA option\nChoose GeoPackage 1.0 from Select output format\nEnter your Email Address\nAccept the Terms and Conditions\nClick Extract/Download\nYou can can also just download the preprepared data here, just press the three dots and download.\nMove the downloaded zip file to your project raw data folder, and open the shapefile in QGIS\n\n\n\n\nAnother way to access QSpatial Data is using the Queensland Globe portal.\n\nTo access data from Queensland Globe go to https://qldglobe.information.qld.gov.au\nAccept the T&Cs.\nClick Search, Select Locality (Suburb) within a Local Government Area, and search for the location you want and select it from the list.\nClick Layers, click Add Layers + here you can scroll through and filter different layer types\nWe want to tick the box next to Planning Cadastre &gt; Land Parcels &gt; Land Parcel (you may need to zoom in to see certain layers)\nTo export data click the Wrench Icon in the top right, and then click the Identify icon (i)\nUse the triangular Identify Polygon tool to select and area of interest. Double click when you’ve finished selecting your area.\nThe Layers menu will now show your selection. You can download all, or sections, of your selection.\nI will choose Land Parcel and then Download as a shp file.\nNote: You may need to disable other layers for this to work correctly. I needed to turn off the Transportation layer to prevent roads from being included in my selection."
  },
  {
    "objectID": "Python/5-python_toolkit/python_toolkit.html",
    "href": "Python/5-python_toolkit/python_toolkit.html",
    "title": "Building your Python Toolkit",
    "section": "",
    "text": "TipUpcoming workshop(s) available!\n\n\n\nThe next workshop is on Fri Mar 27 at 09:30 AM.\nBook in to the next offering now.\nAlternatively, check our calendar for future events.\nIn this standalone workshop, we take a look at the Python building blocks that you’ll want to have in your toolkit. This includes:\nWe’ll work by procedurally building up a bigger and bigger program, containing all the features we cover. That way you’ve got them all in context\nAlong the way, you’ll encounter callouts like these:\nThese crop up every time a new feature or concept is introduced.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "5. Building your Python toolkit"
    ]
  },
  {
    "objectID": "Python/5-python_toolkit/python_toolkit.html#preparation",
    "href": "Python/5-python_toolkit/python_toolkit.html#preparation",
    "title": "Building your Python Toolkit",
    "section": "Preparation",
    "text": "Preparation\nBefore we begin, please follow the following instructions\n\nOpen your preferred IDE (e.g. Spyder).\nDownload the files for today’s session.\nExtract them to an accessible location (you’ll work from here).",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "5. Building your Python toolkit"
    ]
  },
  {
    "objectID": "Python/5-python_toolkit/python_toolkit.html#part-0-refresher",
    "href": "Python/5-python_toolkit/python_toolkit.html#part-0-refresher",
    "title": "Building your Python Toolkit",
    "section": "Part 0: Refresher",
    "text": "Part 0: Refresher\nBefore we get into the content for this workshop, let’s just have a brief refresher on what Python is and how it works.\nPython is a program on your computer, just like any other. When you want to run Python code, or a .py file, the Python program (e.g. python.exe) runs. It takes your code as an input and evaluates it.\nPython is also the name of the programming language which the Python program interprets. So, we refer to the language as Python, and the executable program which interprets it the interpreter.\nThe language is similar to other high-level, object-oriented languages like R, MATLAB, Julia, JavaScript etc. It is versatile and reads easily.\nThere are various ways to run Python, this workshop assumes you’re using an IDE like Spyder. Here, we’ll write just one script, containing our program.\nIn Python we work with two types of objects: variables and functions.\n\nVariables\nVariables allow you to store information in a named object. We use the = operator to created them, with the syntax\n\n&lt;name&gt; = &lt;value&gt;\n\nFor example, the script below creates two objects, example_number and example_string, by assigning values to both.\n\nexample_number = 100.5\nexample_string = \"Hello!\"\n\n\n\nFunctions\nFunctions enable you to run predefined Python code. Functions can be built-in, from a module, or written yourself.\nTo call (use) a function, the syntax is\n\n&lt;function&gt;(&lt;input_1&gt;, &lt;input_2&gt;, ...)\n\nThe parentheses () are essential, even if you don’t have any inputs.\nFor example, let’s use the round() function to round our example_number:\n\nround(example_number)\n\n100\n\n\n\n\nKeywords and Modules\nFinally, you will encounter a third type of command in Python: keywords. These are special commands which alter your code in a predetermined way, and you can’t use them for variable names. You also can’t write your own - they’re baked into Python itself.\nKeywords must always be followed by a space (or a colon in special cases). One example is the import command, which loads a module. Modules are collections of Python code that other people have written, containing lots of functions, variables and submodules.\nFor example, the following code imports the os module.\n\nimport os",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "5. Building your Python toolkit"
    ]
  },
  {
    "objectID": "Python/5-python_toolkit/python_toolkit.html#part-1-verifying-your-setup",
    "href": "Python/5-python_toolkit/python_toolkit.html#part-1-verifying-your-setup",
    "title": "Building your Python Toolkit",
    "section": "Part 1: Verifying your setup",
    "text": "Part 1: Verifying your setup\nIn the first part of this workshop, we’ll learn to use Python’s building blocks by verifying our setup is working. We’ll need to use\n\nThe os module and functions therein\nThe print() function\nConditionals if and else\nError handling with raise\n\nTo begin, create a new script called toolkit.py. This should be in the same location as the /texts/ folder you downloaded.\n\nUsing os to check the working directory\nWe’ll begin by diving into an unusual starting point: the os module. Python comes with a built in module called os (short for operating system) which allows you to interact with your computer. Let’s start by importing it\n\nimport os\n\n\n\n\n\n\n\nTipThe os module\n\n\n\n\n\nThe os module is a built-in Python module that enables interfacing with the operating system. Some popular uses, which aren’t looked at in this workshop, are\n\nos.chdir(...): Change the current working directory to ...\nos.system(...): Send the command ... to a terminal\nos.walk(...): Recursively walk through the files in ... (requires looping, see later)\n\n\n\n\nThis links our Python environment to the os module so we can now access code from it. The reason that we’re starting here is because we can use the os module to interact with our computer.\nNext, we’ll use the print() function to send a message to the user:\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\n\nRunning the Python Toolkit Program\n\n\n\n\n\n\n\n\nTipThe print() function\n\n\n\n\n\nThe function print(...) is a built-in function which sends the message ... to the console. Note that the output of the function is technically None (this is different to the console message).\n\n\n\nAs a first application of os, we’ll use it to print the current working directory. This is a location on your computer, considered by the program to be home - all files are relative to the current working directory. We’ll need this later when we deal with file paths.\nYou can access the current working directory with the getcwd() function within the os module. Because the function lives inside the module, we need to use the . operator.\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(\"The current working directory is\")\nprint(os.getcwd())\n\nRunning the Python Toolkit Program\nThe current working directory is\n/home/runner/work/technology-training/technology-training/Python/5-python_toolkit\n\n\n\n\n\n\n\n\nTipThe . operator\n\n\n\n\n\nThe . operator allows you to access objects that exist within other objects. Here, we access the getcwd() function which lives inside the os module.\nAll objects in Python have methods (functions) and attributes (variables) attached to them, which you use the . operator to access.\n\n\n\nWe can simplify this process by using f-strings. Including the letter f directly before a string’s first quotation mark tells Python to execute any code within curly brackets:\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\n\n\n\n\n\n\n\n\nWarningSetting the correct working directory\n\n\n\nWhen you work with other files, you’ll want to set the working directory. Here, we’ll be loading data that we store in the same folder as your Python script, so let’s move the working directory there.\n\nFirst, copy your script’s file path. On Windows, find the file in File Explorer and copy the path address (up the top).\nNext, use the os.chdir() function to change the current working directory as follows\n\nos.chdir(R\"path/to/script/\")\nMake sure you include the R before the path. This tells Python it’s a raw string and won’t misinterpret the backslashes.\n\n\n\n\n\n\n\n\nTipf-strings\n\n\n\n\n\nYou can include executed code within strings by prepending them with the letter f. The code needs to be placed within curly brackets inside the string, and the output of the code will be directly inserted there.\n\nf\"This is an f-string, showing that 1 + 1 = {1 + 1}\"\n\n'This is an f-string, showing that 1 + 1 = 2'\n\n\n\n\n\n\n\nManaging folders and paths with conditionals\nNow that we’ve printed a welcome message, let’s use another function, to ensure that we’ve got the folder “texts” in our working directory. The function lives in a submodule of os, called os.paths, and the function is exists().\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nprint(os.path.exists(\"texts\"))\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nTrue\n\n\n\n\n\n\n\n\nTipSubmodules\n\n\n\n\n\nMost big modules, including os, contain submodules. These are also accessed via the . notation:\n\nmodule.submodule.function()\n\nSubmodules could also have submodules:\n\nmodule.submodule.submodule_of_submodule.function()\n\nThis helps keep the scope clear. For example, there might be lots of submodules with the function exists(), but because we call os.path.exists(), we know that this one relates to filepaths.\n\n\n\nWhat should we do if the folder doesn’t exist? We should probably stop the program, fix our setup, and then try again.\nLet’s tell Python to print a message if the folder exists. For this, we need to use conditionals.\nThe keyword if indicates that a block of indented code should only be run if a condition is True. Because the function os.path.exists() returns a True or False, we can use it as the condition.\nLet’s set up our conditional and test it by printing a message. We have to be careful with the syntax:\n\nif &lt;condition&gt;:\n    code_to_run_if_True\n\ncode_outside_will_always_run\n\nIncluding it in our code, we want to substitute:\n\n&lt;condition&gt; \\(\\rightarrow\\) os.path.exists(\"texts\")\ncode_to_run_if_True \\(\\rightarrow\\) print(\"The folder /texts/ exists\")\n\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nif os.path.exists(\"texts\"):\n    print(\"The folder /texts/ exists\")\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nThe folder /texts/ exists\n\n\nWhat if it doesn’t exist? We can use the keyword else to catch anything that fails the condition.\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nif os.path.exists(\"texts\"):\n    print(\"The folder /texts/ exists.\")\nelse:\n    print(\"The folder /texts/ does not exist.\")\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nThe folder /texts/ exists.\n\n\n\n\n\n\n\n\nTipConditionals: if, elif and else\n\n\n\n\n\nThe keywords if, elif, and else together constitute the conditionals that Python supports. These allow you to only run code based on certain conditions.\nThe first, if, is always required. The syntax is,\n\nThe if keyword\nA condition, which must become either True or False\nA colon, :\nThe code to run if the condition is True. It must be indented if on a new line.\n\n\nif &lt;condition&gt;:\n    code_to_run_if_True\n\ncode_outside_will_always_run\n\nThe keyword elif allows you to check an additional condition, only if the previous condition(s) failed.\n\nif &lt;condition&gt;:\n    code_to_run_if_True\nelif &lt;condition_2&gt;:\n    code_to_run_if_condition2_True\nelif &lt;condition_3&gt;:\n    code_to_run_if_condition3_True\n\ncode_outside_will_always_run\n\nFinally, the keyword else catches anything that failed all conditions\n\nif &lt;condition&gt;:\n    code_to_run_if_True\nelif &lt;condition_2&gt;:\n    code_to_run_if_condition2_True\nelif &lt;condition_3&gt;:\n    code_to_run_if_condition3_True\nelse:\n    code_to_run_if_all_failed\n\ncode_outside_will_always_run\n\n\n\n\nRealistically, we don’t want our program to run unless the setup is correct. This means we should stop the program if it can’t find the folder.\nYou’ve probably already encountered errors in your code. Now it’s your chance to code them in manually. To raise an error,\n\nUse the raise keyword\nFollow with a valid error, e.g. ValueError(), KeyError() etc.\nPlace a useful message inside the brackets.\n\nIn this case, we should use the FileNotFoundError(). Let’s replace the else section:\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nif os.path.exists(\"texts\"):\n    print(\"The folder /texts/ exists.\")\nelse:\n    raise FileNotFoundError(\"Cannot find the folder /texts/.\")\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nThe folder /texts/ exists.\n\n\n\n\n\n\n\n\nTipRaising exceptions with raise\n\n\n\n\n\nThe raise keyword allows you to raise exceptions (errors) in Python. These stop the execution and print an error message.\nThe syntax is\n\nraise SomeError(\"Appropriate error message\")\n\nand there are lots of built in exceptions.\n\n\n\n\n\n\n\n\n\nWarning&lt;Error retrieving source code with stack_data see ipython/ipython#13598&gt;\n\n\n\nYour traceback might have the error\n&lt;Error retrieving source code with stack_data see ipython/ipython#13598&gt;\nWhile this isn’t a major issue, it’s a bug that will prevent you from seeing tracebacks in your error messages. It’s because Anaconda has shipped an old version of a behind-the-scenes module that it uses; we can fix this.\nIf you’re using Anaconda,\n\nOpen an Anaconda Prompt\nRun\n\nconda update executing\nIf you’re not, then you might have a more serious bug going on. You can try running the following command in a terminal\npip install -U executing\nbut it may not solve the problem.\n\n\n\n\nActivity 1\nTo make sure that you’ve set things up correctly, you should raise an error if the number of files in the texts folder is not five.\nTo set things up, let’s use the os.listdir() function to get a list of the files, storing them in a variable.\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nif os.path.exists(\"texts\"):\n    print(\"The folder /texts/ exists.\")\nelse:\n    raise FileNotFoundError(\"Cannot find the folder /texts/.\")\n\n# Check that there are five files in the folder\nfiles_in_texts = os.listdir(\"texts\")\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nThe folder /texts/ exists.\n\n\nFor this activity, you can use the len() built-in function to determine the size of files_in_texts. Then, use a conditional to raise an error if it’s not five.\nIn sum,\n\nUse len() to determine the number of objects in files_in_texts\nUse an if statement to check if this is not equal to five. You’ll need the != (not equal to) operator.\nUse the raise keyword to raise an error.\n\n\n\n\n\n\n\nTipLogical operators\n\n\n\n\n\nTo check (in)equalities, you can use logical operators. For example,\n\n1 == 1  # Equal to\n\nTrue\n\n\n\n1 != 2  # Not equal to\n\nTrue\n\n\n\n2 &gt; 1   # Greater than\n\nTrue\n\n\n\n1 &lt;= 1  # Less than or equal to\n\nTrue\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe new code is\n\nfiles_in_texts = os.listdir(\"texts\")\n\n# Check that there are five files within texts\nif len(files_in_texts) != 5:\n    raise FileNotFoundError(\"Incorrect number of files in /texts/.\")\n\nNote that this doesn’t produce an output message. Generally, if everything is fine, we don’t need a message.\nThe whole program has now become\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nif os.path.exists(\"texts\"):\n    print(\"The folder /texts/ exists.\")\nelse:\n    raise FileNotFoundError(\"Cannot find the folder /texts/.\")\n\nfiles_in_texts = os.listdir(\"texts\")\n\n# Check that there are five files within texts\nif len(files_in_texts) != 5:\n    raise FileNotFoundError(\"Incorrect number of files in /texts/.\")\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nThe folder /texts/ exists.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "5. Building your Python toolkit"
    ]
  },
  {
    "objectID": "Python/5-python_toolkit/python_toolkit.html#part-2-analysing-the-data",
    "href": "Python/5-python_toolkit/python_toolkit.html#part-2-analysing-the-data",
    "title": "Building your Python Toolkit",
    "section": "Part 2: Analysing the data",
    "text": "Part 2: Analysing the data\nIn the second part of this session, we’ll learn to use Python’s input/output and looping features to analyse the files within texts. We’ll use\n\nThe open() function and with ... as ... keywords for reading\nString methods and the set variable type to analyse the texts\nfor loops to automate the process\n\n\n\n\n\n\n\nNoteCode from Part 1\n\n\n\n\n\nBefore we begin, ensure that your code looks like this. Continue from the bottom throughout part 2.\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nif os.path.exists(\"texts\"):\n    print(\"The folder /texts/ exists.\")\nelse:\n    raise FileNotFoundError(\"Cannot find the folder /texts/.\")\n\nfiles_in_texts = os.listdir(\"texts\")\n\n# Check that there are five files within texts\nif len(files_in_texts) != 5:\n    raise FileNotFoundError(\"Incorrect number of files in /texts/.\")\n\n#### Part 2 ####\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nThe folder /texts/ exists.\n\n\n\n\n\n\nReading and manipulating text files\nWe’ll start by reading the first file, Macbeth.txt into Python. Make sure to include the snippets in part 2 below the code from part 1.\nTo read a file in Python,\n\nCreate a with ... as ... block to make sure the file connection closes properly\nAt the first ..., use the open(&lt;filepath&gt;, encoding = \"utf-8\") function to open the file. Use encoding = \"utf-8\" because your operating system might not have this as the default.\nAt the second ..., use a placeholder variable to store the file connection. Something like file.\nInside the block (like an if statement), use file.read() to access its contents and store that in a variable.\n\nWe’ll use that final variable to perform our analysis. It is completely disconnected from the actual file.\n\n#### Part 2 ####\n\nwith open(\"texts/Macbeth.txt\", encoding = \"utf-8\") as file:\n    contents = file.read()\n\n\n\n\n\n\n\nTipFile input/output\n\n\n\n\n\nReading and writing files in Python takes a few steps. Essentially, Python forms a connection to a file with the open() function which automatically closes if we do this inside a with ... as ... block.\nThe syntax is\n\nwith open(\"path_to_file\", encoding = \"...\") as &lt;placeholder&gt;:\n    code_with_file_connection_open\n\ncode_once_connection_has_closed\n\nNote that whatever you put at &lt;placeholder&gt; will store the file connection. All files have the method read(), which parses the contents. A method is a function that you access with ..\nTypically, you want to store the contents in a variable, like in our contents example, which is disconnected from the actual file.\nThe encoding refers to how the file stored text. Generally, you should use encoding = \"utf-8\", because the default is based on your operating system and varies from machine to machine.\n\n\n\nNext, let’s perform some analysis of the text. Our goal is to compare the total number of words with the total unique number of words.\nFirst, we need to apply a string method to separate the words in the text. Methods are functions that all variables of a particular type have access to, and we use them with the dot operator .. In this case, the .split() method will create a list by dividing the string every time there is a whitespace.\n\n#### Part 2 ####\n\nwith open(\"texts/Macbeth.txt\", encoding = \"utf-8\") as file:\n    contents = file.read()\n\nwords = contents.split()\n\n\n\n\n\n\n\nTipMethods\n\n\n\n\n\nEvery variable has common methods (functions) and attributes (variables) associated with them, accessible via the . operator. For example, all strings have the .lower() method which makes them lowercase:\n\nexample_string = \"THIS WAS IN CAPS\"\nexample_string.lower()\n\n'this was in caps'\n\n\nOther variables have their own methods. Numbers have the .as_integer_ratio() method, which turns the number into a fraction\n\nexample_int = 5.5\nexample_int.as_integer_ratio()\n\n(11, 2)\n\n\nand lists have the .append() method, which adds another element to the list\n\nexample_list = [\"a\", \"b\"]\nexample_list.append(\"c\")\nprint(example_list)\n\n['a', 'b', 'c']\n\n\n\n\n\nWe can then use the len() function again to determine the total number of words and print a message. We can also use print() by itself to make an empty line.\n\n#### Part 2 ####\n\nwith open(\"texts/Macbeth.txt\", encoding = \"utf-8\") as file:\n    contents = file.read()\n\nwords = contents.split()\nword_count = len(words)\n\nprint()\nprint(f\"There are {word_count} words in Macbeth.\")\n\n\nThere are 21428 words in Macbeth.\n\n\nTo work out the unique number of words, we can convert our list to a different variable type: the set. Sets are like lists, but they only contain unique values.\nWe can convert a variable to another type by using its type as a function, e.g. int(), str(), list(). Here, we’ll need set(). Then we can use len() again to determine its size.\n\nCreate a set of unique words with set(words)\nFind the count of unique words with len()\nPrint an additional message\n\n\n#### Part 2 ####\n\nwith open(\"texts/Macbeth.txt\", encoding = \"utf-8\") as file:\n    contents = file.read()\n\nwords = contents.split()\nunique_words = set(words)\n\nword_count = len(words)\nunique_word_count = len(unique_words)\n\nprint()\nprint(f\"There are {word_count} words in Macbeth.\")\nprint(f\"There are {unique_word_count} different words in Macbeth.\")\n\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\n\n\n\n\n\n\n\n\nTipContainers: list, tuple, dict and set\n\n\n\n\n\nPython has four built-in variables which are ‘containters’: they store multiple values.\nLists\nLists simply a collection of Python objects. They are ordered, so you can access them by their index, and they are mutable, so you can change individual elements.\nCreate a list with square brackets:\n\nexample_list = [1, \"a\", 5.5]\n\n# Mutable - can change specific elements\n# Ordered - access elements by position\nexample_list[0] = \"first\"\nprint(example_list)\n\n['first', 'a', 5.5]\n\n\nTuples\nTuples are like lists, but you can’t modify its elements. That makes it ordered and immutable.\nCreate a tuple with parentheses:\n\nexample_tuple = (1, \"a\", 5.5)\n\n# Immutable - attempting to change specific element gives error\nexample_tuple[0] = \"first\"\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[29], line 4\n      1 example_tuple = (1, \"a\", 5.5)\n      3 # Immutable - attempting to change specific element gives error\n----&gt; 4 example_tuple[0] = \"first\"\n\nTypeError: 'tuple' object does not support item assignment\n\n\n\nDictionaries\nDictionaries are like lists, but they are unordered. Instead of using position to identify elements, you use keywords.\nCreate a dictionary with curly brackets and key: value pairs:\n\nexample_dict = {\"a\": 1, \"b\": 2, \"c\": 3}\n\n# Can create new elements in dictionary by 'accessing' them\nexample_dict[\"d\"] = 4\nprint(example_dict)\n\n{'a': 1, 'b': 2, 'c': 3, 'd': 4}\n\n\nSets\nSets are like lists but the elements are unique. Duplicates will always be removed. They are also unordered, so you can’t access individual elements unless you loop through the set.\n\nexample_set = {\"a\", \"a\", 2, 2, \"c\"}\n\nprint(example_set)\n\n{'a', 2, 'c'}\n\n\n\n\n\nFinally, let’s determine the ratio of unique words to total words\n\\[\\text{ratio} = \\frac{\\text{unique words}}{\\text{total words}}\\]\n\n#### Part 2 ####\n\nwith open(\"texts/Macbeth.txt\", encoding = \"utf-8\") as file:\n    contents = file.read()\n\nwords = contents.split()\nunique_words = set(words)\n\nword_count = len(words)\nunique_word_count = len(unique_words)\nratio = unique_word_count / word_count\n\nprint()\nprint(f\"There are {word_count} words in Macbeth.\")\nprint(f\"There are {unique_word_count} different words in Macbeth.\")\nprint(f\"The unique word ratio is {unique_word_count / word_count}\")\n\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\n\n\n\nUsing loops to automate the process\nNow that we’ve analysed one of the texts, let’s do the same for all five.\nThe brute force approach is to copy the code five times and adjust it.\nHowever, we can do one better with a for loop. This enables us to repeat a section of code for each element in an object.\n\nfor &lt;placeholder&gt; in &lt;object&gt;:\n    code_to_repeat\n\ncode_after_loop\n\nWe’ll start just by printing out the names of each file. We need the list of file names, which we get from os.listdir(\"texts\").\n\n#### Part 2 ####\n\nwith open(\"texts/Macbeth.txt\", encoding = \"utf-8\") as file:\n    contents = file.read()\n\nwords = contents.split()\nunique_words = set(words)\n\nword_count = len(words)\nunique_word_count = len(unique_words)\nratio = unique_word_count / word_count\n\nprint()\nprint(f\"There are {word_count} words in Macbeth.\")\nprint(f\"There are {unique_word_count} different words in Macbeth.\")\nprint(f\"The unique word ratio is {unique_word_count / word_count}\")\n\nfiles_in_texts = os.listdir(\"texts\")\n\nfor text_path in files_in_texts:\n    print(text_path)\n\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\nThe_Great_Gatsby.txt\nPride_and_Prejudice.txt\nThe_Adventures_of_Huckleberry_Finn.txt\nThe_Count_of_Monte_Cristo.txt\nMacbeth.txt\n\n\n\n\n\n\n\n\nTipfor loops\n\n\n\n\n\nTo iterate through an object, running the same code on each element, Python offers the for loop.\n\nfor &lt;placeholder&gt; in &lt;object&gt;:\n    code_to_repeat\n\ncode_after_loop\n\nWhatever you name in &lt;placeholder&gt; will store an element of the &lt;object&gt; for each iteration of the loop.\nFor example, the following loop prints each element of example_list. Each time the loop runs, letter stores one of the list’s elements, in order.\n\nexample_list = [\"a\", \"b\", \"c\"]\n\nfor letter in example_list:\n    print(letter)\n\na\nb\nc\n\n\nThere are a few important keywords you can use to help with loops.\nbreak\nThe keyword break tells Python to finish the loop immediately. This is often used with conditionals. For example,\n\nexample_list = [\"a\", \"b\", \"c\"]\n\nfor letter in example_list:\n    if letter == \"b\":\n        break\n    print(letter)\n\na\n\n\ncontinue\nThe keyword continue tells Python to skip the rest of the current iteration and start the next.\n\nexample_list = [\"a\", \"b\", \"c\"]\n\nfor letter in example_list:\n    if letter == \"b\":\n        continue\n    print(letter)\n\na\nc\n\n\n\n\n\nWe can use our for loop to run the whole analysis on each file.\nFirst, let’s just place the analysis inside the loop. This will run once for each file, but because we haven’t changed the path from \"texts/Macbeth.txt\", it will still read Macbeth each time.\n\n#### Part 2 ####\nfiles_in_texts = os.listdir(\"texts\")\n\nfor text_path in files_in_texts:\n    with open(\"texts/Macbeth.txt\", encoding = \"utf-8\") as file:\n        contents = file.read()\n\n    words = contents.split()\n    unique_words = set(words)\n\n    word_count = len(words)\n    unique_word_count = len(unique_words)\n    ratio = unique_word_count / word_count\n\n    print()\n    print(f\"There are {word_count} words in Macbeth.\")\n    print(f\"There are {unique_word_count} different words in Macbeth.\")\n    print(f\"The unique word ratio is {unique_word_count / word_count}\")\n\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\n\nNotice that the final files_in_texts = os.listdir(\"texts\") has been removed because it’s superfluous.\nNow we can use the text variable, which changes on each iteration of the loop, in place of the file path. Specifically, we’ll make the change\n\"texts/Macbeth.txt\" \\(\\rightarrow\\) f\"texts/{text_path}\"\n\n#### Part 2 ####\nfiles_in_texts = os.listdir(\"texts\")\n\nfor text_path in files_in_texts:\n    with open(f\"texts/{text_path}\", encoding = \"utf-8\") as file:\n        contents = file.read()\n\n    words = contents.split()\n    unique_words = set(words)\n\n    word_count = len(words)\n    unique_word_count = len(unique_words)\n    ratio = unique_word_count / word_count\n\n    print()\n    print(f\"There are {word_count} words in Macbeth.\")\n    print(f\"There are {unique_word_count} different words in Macbeth.\")\n    print(f\"The unique word ratio is {unique_word_count / word_count}\")\n\n\nThere are 51257 words in Macbeth.\nThere are 10206 different words in Macbeth.\nThe unique word ratio is 0.19911426731958562\n\nThere are 130410 words in Macbeth.\nThere are 14702 different words in Macbeth.\nThe unique word ratio is 0.11273675331646346\n\nThere are 114125 words in Macbeth.\nThere are 14307 different words in Macbeth.\nThe unique word ratio is 0.12536254107338446\n\nThere are 464023 words in Macbeth.\nThere are 40030 different words in Macbeth.\nThe unique word ratio is 0.0862672755445312\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\n\nIf you look closely, it has worked - the numbers are changing each time. We need to update our messages though, to make it dynamic.\n\n#### Part 2 ####\nfiles_in_texts = os.listdir(\"texts\")\n\nfor text_path in files_in_texts:\n    with open(f\"texts/{text_path}\", encoding = \"utf-8\") as file:\n        contents = file.read()\n\n    words = contents.split()\n    unique_words = set(words)\n\n    word_count = len(words)\n    unique_word_count = len(unique_words)\n    ratio = unique_word_count / word_count\n\n    print()\n    print(f\"There are {word_count} words in {text_path}.\")\n    print(f\"There are {unique_word_count} different words in {text_path}.\")\n    print(f\"The unique word ratio is {unique_word_count / word_count}\")\n\n\nThere are 51257 words in The_Great_Gatsby.txt.\nThere are 10206 different words in The_Great_Gatsby.txt.\nThe unique word ratio is 0.19911426731958562\n\nThere are 130410 words in Pride_and_Prejudice.txt.\nThere are 14702 different words in Pride_and_Prejudice.txt.\nThe unique word ratio is 0.11273675331646346\n\nThere are 114125 words in The_Adventures_of_Huckleberry_Finn.txt.\nThere are 14307 different words in The_Adventures_of_Huckleberry_Finn.txt.\nThe unique word ratio is 0.12536254107338446\n\nThere are 464023 words in The_Count_of_Monte_Cristo.txt.\nThere are 40030 different words in The_Count_of_Monte_Cristo.txt.\nThe unique word ratio is 0.0862672755445312\n\nThere are 21428 words in Macbeth.txt.\nThere are 6207 different words in Macbeth.txt.\nThe unique word ratio is 0.2896677244726526\n\n\nFinally, let’s remove the trailing .txt on the messages by extracting the text’s title from its path. To do this, slice the string with square brackets: title = text_path[:-4]. In this case, we slice from the start of the path up to the fourth last character.\nWe can also replace the underscores with spaces using the .replace() string method.\n\n\n\n\n\n\nTipIndexing and Slicing\n\n\n\n\n\nIndexing\nTo extract a substring from a string (or a subset of a list) use square brackets and specify the position of the elements you want. For example, to pick out the first letter in the following string,\n\nexample_string = \"apple\"\n\nyou specify the position of the first element, which is 0 (in Python, count from 0):\n\nexample_string[0] # First element\n\n'a'\n\n\nIf you want the second element, use 1:\n\nexample_string[1] # Second element\n\n'p'\n\n\nIf you want to count from the end, use negatives:\n\nexample_string[-1] # Last element\n\n'e'\n\n\nSlicing\nWhat if you want multiple elements? You slice by specifying the start and end indices between a colon:\n\nexample_string[1:3] # Elements 1 and 2\n\n'pp'\n\n\nNotice that it includes the first index but excludes the second.\nTo start at the beginning, just leave the first index out:\n\nexample_string[:3] # Elements 0, 1 and 2\n\n'app'\n\n\nTo go to the end, leave the second index out:\n\nexample_string[2:] # Elements 2, 3, ..., -1\n\n'ple'\n\n\nFinally, you can combine negative indexing with slicing. For example, to go up to the last element:\n\nexample_string[:-1]\n\n'appl'\n\n\nIn sum:\n\n\n\nCode\n0\n1\n2\n3\n4\n\n\n\n\nexample_string\n\"a\"\n\"p\"\n\"p\"\n\"l\"\n\"e\"\n\n\nexample_string[0]\n\"a\"\n\n\n\n\n\n\nexample_string[2]\n\n\n\"p\"\n\n\n\n\nexample_string[-1]\n\n\n\n\n\"e\"\n\n\nexample_string[1:3]\n\n\"p\"\n\"p\"\n\n\n\n\nexample_string[:3]\n\"a\"\n\"p\"\n\"p\"\n\n\n\n\nexample_string[2:]\n\n\n\"p\"\n\"l\"\n\"e\"\n\n\nexample_string[:-1]\n\"a\"\n\"p\"\n\"p\"\n\"l\"\n\n\n\n\n\n\n\n\n#### Part 2 ####\nfiles_in_texts = os.listdir(\"texts\")\n\nfor text_path in files_in_texts:\n    title = text_path[:-4].replace(\"_\", \" \") # &lt;-- extract the title\n\n    with open(f\"texts/{text_path}\", encoding = \"utf-8\") as file:\n        contents = file.read()\n\n    words = contents.split()\n    unique_words = set(words)\n\n    word_count = len(words)\n    unique_word_count = len(unique_words)\n    ratio = unique_word_count / word_count\n\n    print()\n    print(f\"There are {word_count} words in {title}.\") # &lt;-- include in message\n    print(f\"There are {unique_word_count} different words in {title}.\") # &lt;-- include in message\n    print(f\"The unique word ratio is {unique_word_count / word_count}\")\n\n\nThere are 51257 words in The Great Gatsby.\nThere are 10206 different words in The Great Gatsby.\nThe unique word ratio is 0.19911426731958562\n\nThere are 130410 words in Pride and Prejudice.\nThere are 14702 different words in Pride and Prejudice.\nThe unique word ratio is 0.11273675331646346\n\nThere are 114125 words in The Adventures of Huckleberry Finn.\nThere are 14307 different words in The Adventures of Huckleberry Finn.\nThe unique word ratio is 0.12536254107338446\n\nThere are 464023 words in The Count of Monte Cristo.\nThere are 40030 different words in The Count of Monte Cristo.\nThe unique word ratio is 0.0862672755445312\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\n\n\n\nActivity 2\nIf you open one of the files, you’ll notice that there is front and end matter which isn’t from the original texts. Let’s remove them and save the cleaned texts. To do so, we’ll need to use two skills:\n\nSlicing\nWriting to files\n\n\nPart 1: Clean the texts\nTo remove the front/end matter, notice that the original texts all begin after the string\n*** START OF THE PROJECT GUTENBERG EBOOK\nand end before\n*** END OF THE PROJECT GUTENBERG EBOOK.\nTo clean the text,\n\nUse the function contents.find(...) to find the index corresponding to the keys,\n\nstart_index = contents.find(...)\nend_index = contents.find(...)\n\nSlice the text between those two indices and save it in a variable.\n\n\n\nPart 2: Write the strings to files\nWriting strings to a file is similar to reading. We’ll start by creating a with ... as ... block pointing to the new file path:\nwith open(f\"{title}_clean.txt\", \"w\", encoding = \"utf-8\") as file:\n    ...\nThen use file.write(...) to write the cleaned string to the new file.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe following code is a possible solution to the problem.\n\n#### Part 2 ####\nfiles_in_texts = os.listdir(\"texts\")\n\nfor text_path in files_in_texts:\n    \n    # ...\n    # ...\n    # ...\n\n    # Remove front/end matter and save clean files\n    start_message = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n    end_message = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n\n    start = contents.find(start_message)\n    end = contents.find(end_message)\n\n    clean_text = contents[start:end]\n\n    with open(f\"{title}_clean.txt\", \"w\", encoding = \"utf-8\") as file:\n        file.write(clean_text)",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "5. Building your Python toolkit"
    ]
  },
  {
    "objectID": "Python/5-python_toolkit/python_toolkit.html#part-3-extension-making-it-modular",
    "href": "Python/5-python_toolkit/python_toolkit.html#part-3-extension-making-it-modular",
    "title": "Building your Python Toolkit",
    "section": "Part 3 (extension): Making it modular",
    "text": "Part 3 (extension): Making it modular\nIn this final (optional) part we take a look making our code modular. In Python, you can do this in two ways:\n\nWithin the script, with functions\nOutside the script, with modules.\n\n\n\n\n\n\n\nNoteCode from Parts 1 and 2\n\n\n\n\n\nBefore beginning, just check that your code is up to date:\n\nimport os\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nif os.path.exists(\"texts\"):\n    print(\"The folder /texts/ exists.\")\nelse:\n    raise FileNotFoundError(\"Cannot find the folder /texts/.\")\n\nfiles_in_texts = os.listdir(\"texts\")\n\n# Check that there are five files within texts\nif len(files_in_texts) != 5:\n    raise FileNotFoundError(\"Incorrect number of files in /texts/.\")\n\n#### Part 2 ####\nfiles_in_texts = os.listdir(\"texts\")\n\nfor text_path in files_in_texts:\n    \n    title = text_path[:-4]\n\n    with open(f\"texts/{text_path}\", encoding = \"utf-8\") as file:\n        contents = file.read()\n\n    words = contents.split()\n    unique_words = set(words)\n\n    word_count = len(words)\n    unique_word_count = len(unique_words)\n    ratio = unique_word_count / word_count\n\n    print()\n    print(f\"There are {word_count} words in {title}.\")\n    print(f\"There are {unique_word_count} different words in {title}.\")\n    print(f\"The unique word ratio is {unique_word_count / word_count}\")\n\n    # Remove front/end matter and save clean files\n    start_message = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n    end_message = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n\n    start = contents.find(start_message) + len(start_message)\n    end = contents.find(end_message)\n\n    clean_text = contents[start:end]\n\n    with open(f\"{title}_clean.txt\", \"w\", encoding = \"utf-8\") as file:\n        file.write(clean_text)\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nThe folder /texts/ exists.\n\nThere are 51257 words in The_Great_Gatsby.\nThere are 10206 different words in The_Great_Gatsby.\nThe unique word ratio is 0.19911426731958562\n\nThere are 130410 words in Pride_and_Prejudice.\nThere are 14702 different words in Pride_and_Prejudice.\nThe unique word ratio is 0.11273675331646346\n\nThere are 114125 words in The_Adventures_of_Huckleberry_Finn.\nThere are 14307 different words in The_Adventures_of_Huckleberry_Finn.\nThe unique word ratio is 0.12536254107338446\n\nThere are 464023 words in The_Count_of_Monte_Cristo.\nThere are 40030 different words in The_Count_of_Monte_Cristo.\nThe unique word ratio is 0.0862672755445312\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\n\n\n\n\n\nModularity within the script: functions\nWe’ll start with functions. Functions are like a script within a script - a section of code that runs when you call its name. They come in two parts:\n\nThe function call, which runs the code (e.g. print(), len(), etc.)\nThe function definition, which defines that code\n\nEvery time we’ve used a function, like print(...), len(...), etc., we have performed function calls. However, we need to write new definitions to make our own functions.\nLet’s make a new function now, read_book(...), which reads a text file and returns the contents as a string, like we do in the loop\nWe’ll start by defining our function. Do this at the top of your script, just after the import statements. Functions definitions have the following syntax:\n\ndef &lt;function_name&gt;(&lt;input1&gt;, &lt;input2&gt;, ...):\n    code\n    code\n    code\n    return &lt;output&gt;\n\nLet’s set it up, without including any code yet, with a single input variable path:\n\nimport os\n\ndef read_book(path):\n    return \n\n\n\n\n\n\n\nTipFunction signature and inputs\n\n\n\n\n\nThe function signature, def &lt;signature&gt;: forms a key part of the definition. Inside the brackets there are different ways to specify the inputs.\nNo inputs\nYour function doesn’t have to take any inputs. For example,\n\n# Definition:\ndef no_inputs():\n   ...\n   return &lt;output&gt;\n\n# Call:\nno_inputs()\n\nCompulsory inputs\nIf you just give the inputs names they are compulsory: all calls must include them\n\n# Definition:\ndef compulsory_inputs(input1, input2):\n   ...\n   return &lt;output&gt;\n\n# Call:\ncompulsory_inputs(a, b)\n\nDefault / optional inputs\nYou can specify default values for function inputs, which makes them optional\n\n# Definition:\ndef optional_inputs(input1 = \"apple\", input2 = \"banana\"):\n   ...\n   return &lt;output&gt;\n\n# Call:\noptional_inputs(\"cherry\") # Will interpret as input1 = \"cherry\", input2 = \"banana\"\n\nPositional vs Keyword arguments\nFinally, when you call a function, you can either specify the inputs directly or let it assume by position.\ndef example(input1, input2, input3):\n    ...\n    return &lt;output&gt;\n\nexample(\"apple\", \"banana\", \"cherry\") \nexample(\"apple\", \"banana\", input3 = \"cherry\")\nexample(input1 = \"apple\", input2 = \"banana\", input3 = \"cherry\")\nexample(input3 = \"cherry\", input2 = \"banana\", input1 = \"apple\")\nThese are all valid calls, with various differences:\n\nAll positional\ninput1 and input2 are positional, while input3 is keyword\nAll keyword\nAll keyword - the order doesn’t matter for keyword arguments!\n\n\n\n\n\n\n\nWarningPositional arguments before keyword arguments\n\n\n\nBecause keyword arguments are unordered, positional arguments must precede them:\n\n# Valid\nexample(\"apple\", input2 = \"banana\", input3 = \"cherry\")\n\n# Invalid - positional argument after keyword argument!\nexample(input1 = \"apple\", \"banana\", \"cherry\")\n\n\n\n\n\n\nNow, let’s include the code that we previously used to read the file and split the words. Note that the variable containing the full file path is path, so we should change that accordingly.\n\nimport os\n\ndef read_book(path):  \n    with open(path, encoding = \"utf-8\") as file:\n        contents = file.read()\n    \n    return contents\n\n\n\n\n\n\n\nTipFunction scope\n\n\n\n\n\nVariables created within functions are deleted once the function runs, so they can’t be accessed by your main code! This is called scope.\n\n\n\nFinally, let’s replace code within the loop with a simple function call to our new function. All together,\n\nimport os\n\ndef read_book(path):  \n    with open(path, encoding = \"utf-8\") as file:\n        contents = file.read()\n    \n    return contents\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nif os.path.exists(\"texts\"):\n    print(\"The folder /texts/ exists.\")\nelse:\n    raise FileNotFoundError(\"Cannot find the folder /texts/.\")\n\nfiles_in_texts = os.listdir(\"texts\")\n\n# Check that there are five files within texts\nif len(files_in_texts) != 5:\n    raise FileNotFoundError(\"Incorrect number of files in /texts/.\")\n\n#### Part 2 ####\nfor text_path in files_in_texts:\n    title = text_path[:-4]\n\n    contents = read_book(f\"texts/{text_path}\")   # &lt;-- Custom function call\n    words = contents.split()\n\n    unique_words = set(words)\n\n    word_count = len(words)\n    unique_word_count = len(unique_words)\n    ratio = unique_word_count / word_count\n\n    print()\n    print(f\"There are {word_count} words in {title}.\")\n    print(f\"There are {unique_word_count} different words in {title}.\")\n    print(f\"The unique word ratio is {unique_word_count / word_count}\")\n\n    # Remove front/end matter and save clean files\n    start_message = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n    end_message = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n\n    start = contents.find(start_message) + len(start_message)\n    end = contents.find(end_message)\n\n    clean_text = contents[start:end]\n\n    with open(f\"{title}_clean.txt\", \"w\", encoding = \"utf-8\") as file:\n        file.write(clean_text)\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nThe folder /texts/ exists.\n\nThere are 51257 words in The_Great_Gatsby.\nThere are 10206 different words in The_Great_Gatsby.\nThe unique word ratio is 0.19911426731958562\n\nThere are 130410 words in Pride_and_Prejudice.\nThere are 14702 different words in Pride_and_Prejudice.\nThe unique word ratio is 0.11273675331646346\n\nThere are 114125 words in The_Adventures_of_Huckleberry_Finn.\nThere are 14307 different words in The_Adventures_of_Huckleberry_Finn.\nThe unique word ratio is 0.12536254107338446\n\nThere are 464023 words in The_Count_of_Monte_Cristo.\nThere are 40030 different words in The_Count_of_Monte_Cristo.\nThe unique word ratio is 0.0862672755445312\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526\n\n\n\n\nModularity beyond the script: modules\nWhat actually happens when you run import ...? Python adds the contents of another Python file to the existing ‘namespace’. Basically, you import a bunch of functions (and classes, and other objects…)!\nWe can make our own modules that Python recognises with the import command. The simplest way is just another Python script. Let’s make one to store our new function, so it’s out of the way.\n\nCreate a new script in this folder called reader.py\nMove the function into that file.\n\nThe script should look like this:\n\ndef read_book_words(path):  \n    with open(path, encoding = \"utf-8\") as file:\n        contents = file.read()\n    \n    return contents\n\nFinally, we should reflect the changes in our original script.\n\nReplace the old function definition with the command import reader.\nReplace the old function read_book_words(...) with the command `reader.read_book_words(…)\n\nThe main script should look like this\n\nimport os\nimport reader\n\nprint(\"Running the Python Toolkit Program\")\nprint(f\"The current working directory is {os.getcwd()}\")\n\n# Check that the folder exists in our working directory\nif os.path.exists(\"texts\"):\n    print(\"The folder /texts/ exists.\")\nelse:\n    raise FileNotFoundError(\"Cannot find the folder /texts/.\")\n\nfiles_in_texts = os.listdir(\"texts\")\n\n# Check that there are five files within texts\nif len(files_in_texts) != 5:\n    raise FileNotFoundError(\"Incorrect number of files in /texts/.\")\n\n#### Part 2 ####\nfiles_in_texts = os.listdir(\"texts\")\n\nfor text_path in files_in_texts:\n    title = text_path[:-4]\n\n    contents = reader.read_book(f\"texts/{text_path}\")\n    words = contents.split()\n\n    unique_words = set(words)\n\n    word_count = len(words)\n    unique_word_count = len(unique_words)\n    ratio = unique_word_count / word_count\n\n    print()\n    print(f\"There are {word_count} words in {title}.\")\n    print(f\"There are {unique_word_count} different words in {title}.\")\n    print(f\"The unique word ratio is {unique_word_count / word_count}\")\n\n    # Remove front/end matter and save clean files\n    start_message = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n    end_message = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n\n    start = contents.find(start_message)\n    end = contents.find(end_message)\n\n    clean_text = contents[start:end]\n\n    with open(f\"{title}_clean.txt\", \"w\", encoding = \"utf-8\") as file:\n        file.write(clean_text)\n\nRunning the Python Toolkit Program\nThe current working directory is /home/runner/work/technology-training/technology-training/Python/5-python_toolkit\nThe folder /texts/ exists.\n\nThere are 51257 words in The_Great_Gatsby.\nThere are 10206 different words in The_Great_Gatsby.\nThe unique word ratio is 0.19911426731958562\n\nThere are 130410 words in Pride_and_Prejudice.\nThere are 14702 different words in Pride_and_Prejudice.\nThe unique word ratio is 0.11273675331646346\n\nThere are 114125 words in The_Adventures_of_Huckleberry_Finn.\nThere are 14307 different words in The_Adventures_of_Huckleberry_Finn.\nThe unique word ratio is 0.12536254107338446\n\nThere are 464023 words in The_Count_of_Monte_Cristo.\nThere are 40030 different words in The_Count_of_Monte_Cristo.\nThe unique word ratio is 0.0862672755445312\n\nThere are 21428 words in Macbeth.\nThere are 6207 different words in Macbeth.\nThe unique word ratio is 0.2896677244726526",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "5. Building your Python toolkit"
    ]
  },
  {
    "objectID": "Python/5-python_toolkit/python_toolkit.html#conclusion-and-summary",
    "href": "Python/5-python_toolkit/python_toolkit.html#conclusion-and-summary",
    "title": "Building your Python Toolkit",
    "section": "Conclusion and Summary",
    "text": "Conclusion and Summary\nThis is a big workshop, and we’ve covered a lot of content! See the summary table below for details on the topics covered. Each is linked to the notes in the workshop.\n\nIf you have any further questions, don’t hesistate to contact us at training@library.uq.edu.au.\n\n\n\n\nTopic\nCode\nDescription\n\n\n\n\nThe os module\nimport os\nos.getwd()\nos.chdir()\nos.listdir()\nA built-in module which enables interacting with your operating system.\n\n\nf-strings\nf\"1+1 = {1+1}\"\nFormatted strings, which behave like normal strings except that code within curly brackets {...} is executed.\n\n\nConditionals\n\nif &lt;condition1&gt;:\n    ...\nelif &lt;condition2&gt;:\n    ....\nelse:\n    ....\n\nSections of code which only run if a condition is true.\nAlways start with if.\nUse elif to check additional conditions (only if the first fail).\nUse else to catch everything that fails all conditions.\n\n\nRaising exceptions\nraise ...Error(\"error_message\")\nA way to manually trigger error messages and stop the program. Replace ... with an errortype, e.g. KeyError, ValueError.\n\n\nFile input/output\n\nwith open(...) as &lt;placeholder&gt;:\n    ...\n\nopen(\"path\", encoding = \"utf-8\")\nopen(\"path\", \"w\",\n     encoding = \"utf-8\")\nRead and write to files with the open() function and with ... as ... blocks.\nMost files use the utf-8 encoding, which isn’t set by default.\nSend the \"w\" parameter to write to a file, and leave it out to read.\n\n\nLoops\n\nfor &lt;placeholder&gt; in &lt;iterable&gt;:\n   ...\nwhile &lt;condition&gt;:\n   ...\nRun sections of code multiple times with a loop.\nfor loops run once for each element in an iterable object (e.g. a list). Each iteration stores the current element in what you specify for &lt;placeholder&gt;.\nwhile loops run until &lt;condition&gt; is False. If it never becomes False, the loop runs indefinitely, and will eventually crash your program.\n\n\nIndexing and slicing\nexample_string[1:4]\nAccess individual elements of a string or list by indexing and slicing with square brackets.\n\n\nCustom functions\nCustom modules\n\ndef function_name(input1, input2):\n   ...\n   return ...\n\nfunction_name(a, b)\n\nimport module\nStore sections of code away in functions to run them at a later point.\n\nWrite a function definition with def ... which contains the code\nCall the function to use it with specific inputs\n\nYou can store the functions in a separate script and import that script as a module.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "5. Building your Python toolkit"
    ]
  },
  {
    "objectID": "Python/3-intro_visualisation/visualisation.html",
    "href": "Python/3-intro_visualisation/visualisation.html",
    "title": "Python Training (3 of 4): Introductory Data Visualisation",
    "section": "",
    "text": "TipUpcoming workshop(s) available!\n\n\n\nThe next workshop is on Fri Mar 13 at 09:30 AM.\nBook in to the next offering now.\nAlternatively, check our calendar for future events.\nIn this third workshop we will cover",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "3. Introductory Data Visualisation"
    ]
  },
  {
    "objectID": "Python/3-intro_visualisation/visualisation.html#setting-up",
    "href": "Python/3-intro_visualisation/visualisation.html#setting-up",
    "title": "Python Training (3 of 4): Introductory Data Visualisation",
    "section": "Setting up",
    "text": "Setting up\n\nSpyder version\nBefore we begin, please check which version of Spyder you’re using (you can see this in the Anaconda Navigator, or in Help &gt; About Spyder). If it’s less than 6, you should update Spyder before continuing. However, if you’re in a workshop, this will take too long - instead, use the following workaround if your plots don’t work:\nplt.show()\nMore on this later.\n\n\nModules and data\nWe’ll need three modules today:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nFor this workshop we’ll be working from the “Players2024.csv” dataset. If you don’t have it yet,\n\nDownload the dataset.\nCreate a folder in in the same location as your script called “data”.\nSave the dataset there.\n\nWe should then bring it in with pandas:\n\ndf = pd.read_csv(\"data/Players2024.csv\")\n\nTake a quick peak at the dataset to remind yourself\n\nprint(df)\n\n                        name  birth_date  height_cm   positions nationality  \\\n0               James Milner  1986-01-04      175.0    Midfield     England   \n1        Anastasios Tsokanis  1991-05-02      176.0    Midfield      Greece   \n2              Jonas Hofmann  1992-07-14      176.0    Midfield     Germany   \n3                 Pepe Reina  1982-08-31      188.0  Goalkeeper       Spain   \n4              Lionel Carole  1991-04-12      180.0    Defender      France   \n...                      ...         ...        ...         ...         ...   \n5930  Oleksandr Pshenychnyuk  2006-05-01      180.0    Midfield     Ukraine   \n5931            Alex Marques  2005-10-23      186.0    Defender    Portugal   \n5932             Tomás Silva  2006-05-25      175.0    Defender    Portugal   \n5933             Fábio Sambú  2007-09-06      180.0      Attack    Portugal   \n5934          Hakim Sulemana  2005-02-19      164.0      Attack       Ghana   \n\n      age                                    club  \n0      38  Brighton and Hove Albion Football Club  \n1      33        Volou Neos Podosferikos Syllogos  \n2      32             Bayer 04 Leverkusen Fußball  \n3      42                             Calcio Como  \n4      33                      Kayserispor Kulübü  \n...   ...                                     ...  \n5930   18              ZAO FK Chornomorets Odessa  \n5931   18                  Boavista Futebol Clube  \n5932   18                  Boavista Futebol Clube  \n5933   17                  Boavista Futebol Clube  \n5934   19                    Randers Fodbold Club  \n\n[5935 rows x 7 columns]",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "3. Introductory Data Visualisation"
    ]
  },
  {
    "objectID": "Python/3-intro_visualisation/visualisation.html#seaborn-for-simple-visualisations",
    "href": "Python/3-intro_visualisation/visualisation.html#seaborn-for-simple-visualisations",
    "title": "Python Training (3 of 4): Introductory Data Visualisation",
    "section": "Seaborn for simple visualisations",
    "text": "Seaborn for simple visualisations\nTo begin our visualisations, we’ll use the package seaborn, which allows you to quickly whip up decent graphs.\nSeaborn has three plotting functions\nsns.catplot(...) # for categorical plotting, e.g. bar plots, box plots etc.\nsns.relplot(...) # for relational plotting, e.g. line plots, scatter plots\nsns.displot(...) # for distributions, e.g. histograms\nWe’ll begin with the first.\n\nIt’s called “seaborn” as a reference to fictional character Sam Seaborn, whose initials are “sns”.\n\n\nCategorical plots\nCategorical plots are produced with seaborn’s sns.catplot() function. There are two key pieces of information to pass:\n\nThe data\nThe variables\n\nLet’s see if there’s a relationship between the players’ heights and positions, by placing their positions on the \\(x\\) axis and heights on the \\(y\\).\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\")\n\n\n\n\n\n\n\n\nOur first graph! This is called a swarm plot; it’s like a scatter plot for categorical variables.\n\n\n\n\n\n\nWarningIf your plots don’t appear…\n\n\n\n\n\n…AND you don’t have an error, then you might have a Spyder version with a bug.\nThe simplest workaround is to run plt.show() every time you make a plot. If you have time, you should update Spyder.\n\n\n\nIt’s already revealed two things to us about the data:\n\nThere are some incorrect heights - nobody is shorter than 25cm!\nSomeone’s position is “missing”\n\nLet’s get rid of these with the data analysis techniques from last session\n\n# Remove missing position\ndf = df[df[\"positions\"] != \"Missing\"]\n\n# Ensure reasonable heights\ndf = df[df[\"height_cm\"] &gt; 100]\n\nRun the plot again, it’s more reasonable now\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\")\n\n\n\n\n\n\n\n\n\nBar plots\nSwarm plots are interesting but not standard. You can change the plot type with the kind parameter\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\", kind = \"bar\")\n\n\n\n\n\n\n\n\n\nMany aspects of your plot can be adjusted by sending in additional parameters and is where seaborn excels.\n\nIt seems like goalkeepers are taller, but not by much. Let’s look at the standard deviation for each position by changing the estimator = parameter (default is mean)\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\", kind = \"bar\", estimator = \"std\")\n\n\n\n\n\n\n\n\nClearly there’s a lot less variation in goalkeepers - they’re all tall.\n\n\nDetour - line length\nNotice that our last line was longer than 79 characters? That’s bad Python, and hard to read. We can fix this by making it a multi-line function, placing arguments on new lines, according to PEP 8\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\", kind = \"bar\", \n            estimator = \"std\")\n\n\n\n\n\n\n\n\n\n\nBox plots\nLet’s make box plots instead. It’s the same procedure, just change to kind = \"box\" and remove estimator =\n\nsns.catplot(data = df, x = \"positions\", y = \"height_cm\", kind = \"box\")\n\n\n\n\n\n\n\n\nJust as we predicted.\n\n\n\nDistributions\n\nHistograms\nLet’s move to the “Age” parameter now. We can look at the distribution of ages with\n\nsns.displot(data = df, x = \"age\")\n\n\n\n\n\n\n\n\nLooks a bit funny with those gaps - let’s change the number of bins with bins = 28\n\nsns.displot(data = df, x = \"age\", bins = 28)\n\n\n\n\n\n\n\n\nNow, what if you wanted to look at the distribution for different variables? We can make a separate distribution for each position with the col = \"positions\" argument, specifying a new row for each position\n\nsns.displot(data = df, x = \"age\", bins = 28, col = \"positions\")\n\n\n\n\n\n\n\n\n\n\nKernel density estimates\nFinally, you don’t have to do histograms. You could also do a Kernel Density Estimate, with kind = \"kde\" (let’s remove bins = and row =)\n\nsns.displot(data = df, x = \"age\", kind = \"kde\")\n\n\n\n\n\n\n\n\nIf you want a separate line for each position, we should indicate that each position needs a different colour/hue with hue = \"positions\"\n\nsns.displot(data = df, x = \"age\", hue = \"positions\", kind = \"kde\")\n\n\n\n\n\n\n\n\n\n\n\nActivity 1\nCreate a histogram which looks at the distribution of heights, with a separate distribution for each position, distinguished by colour. Then, use the multiple = \"stack\" parameter to make it a bit neater.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.displot(data = df, x = \"height_cm\", hue = \"positions\", multiple = \"stack\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nRelational plots\nIt seems like players peak in their mid-twenties, but goalkeepers stay for longer. Let’s see if there’s a relationship between players’ age and height\n\nScatter plots\nWe’ll start with a scatter plot\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\n\n\n\n\n\n\n\n\nNot much of a trend there, although the bottom-right looks a bit emptier than the rest (could it be that short old players are the first to retire?).\nWe can use hue = to have a look at positions again\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n\n\n\n\n\n\n\nYup, goalkeepers are tall, and everyone else is a jumble.\n\n\nLine plots\nLet’s do a line plot of the average height per age.\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", kind = \"line\")\n\n\n\n\n\n\n\n\nSeems pretty flat, except the ends are a bit weird because there’s not much data. Let’s eliminate everything before 17 and after 38 and plot it\n\n# Create smaller dataframe\ncondition = (df[\"age\"] &gt; 17) & (df[\"age\"] &lt; 38)\ninner_ages = df[condition]\n\n# Line plot\nsns.relplot(data = inner_ages, x = \"age\", y = \"height_cm\", kind = \"line\")\n\n\n\n\n\n\n\n\nLooks a bit shaky but that’s just because it’s zoomed in - notice that we go from 182cm to 184cm. We’ll fix this when we look at matplotlib in the next section.\n\n\nCombining the two\nWe can combine our scatter and line plots together.\n\nMake the first plot as normal\nFor all additional (overlaying) plots, use an axes-level plot instead of sns.relplot() etc. These just draw the points/bars/lines, and are normally behind-the-scenes. There’s one for every plot type, and look like\n\nsns.lineplot()\nsns.scatterplot()\nsns.boxplot()\nsns.histplot()\netc.\n\n\nFor example,\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\")\n\n\n\n\n\n\n\n\n\nYou can’t include kind = inside an axes level plot\n\nLet’s swap the colour variable from the scatter plot to the line plot\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n\n\n\n\n\n\n\nFinally, let’s make the scatter dots smaller with s = 10 and grey with color = \"grey\".\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n\n\n\n\n\n\n\n\n\n\nActivity 2\nIt’s really important to become familiar with the documentation. Go to the sns.relplot documentation, and look up the following parameters:\n\ncol\ncol_wrap\ncol_order\nhue_order\nlegend\n\nUsing those parameters, create a scatter plot for age vs height (like above), which meets the following conditions:\n\nSeparate the plots for each position.\nApply a different colour to each position.\nArrange the plots in two columns\nRemove the legend\nManually order the colours:\n\nAttack\nMidfield\nDefender\nGoalkeeper\n\n\n\n\n\n\n\n\nTipHint\n\n\n\nFor the ordering, you might want to make a list with the desired order, e.g. order = [\"Attack\", \"Midfield\", ... ]\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\norder = [\"Attack\", \"Midfield\", \"Defender\", \"Goalkeeper\"]\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", hue = \"positions\", \n            col = \"positions\", col_wrap = 2, hue_order = order, \n            col_order = order)",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "3. Introductory Data Visualisation"
    ]
  },
  {
    "objectID": "Python/3-intro_visualisation/visualisation.html#going-deeper-with-matplotlib",
    "href": "Python/3-intro_visualisation/visualisation.html#going-deeper-with-matplotlib",
    "title": "Python Training (3 of 4): Introductory Data Visualisation",
    "section": "Going deeper with matplotlib",
    "text": "Going deeper with matplotlib\nSeaborn is great for simple and initial visualisations, but when you need to make adjustments it gets tricky. At its core, seaborn is just a simple way of using matplotlib, an extensive and popular plotting package. It was created as a way of doing MATLAB visualisations with Python, so if you’re coming from there, things will feel familiar.\nPros\n\nCustomisable. You can tweak almost every parameter of the visualisations\nFast. It can handle large data\nPopular. Lots of people use it, and knowing it will help you collaborate\n\nCons - a bit programmy\n\nSteep-ish learning curve. Creating basic plots can be easy, but its set up with enough complexity that it takes a bit of work to figure out what’s going on.\nCumbersome. You can tweak almost everything, but this means that it can take some effort to tweak anything.\n\nWe’re barely going to touch the matplotlib surface, but we’ll look at some essentials.\n\nSaving plots\nBefore we move to adjusting the plot, let’s just look at how you save it. While you can do this with seaborn, the matplotlib way is also very simple.\nAs a first step, you should make a new folder. Navigate using your file explorer to the project and create a new folder called “plots”.\nNext, save the current plot with plt.savefig(\"place_location_here\"), and we have to do this at the same time that we make the plot. Let’s save our previous overlaying plot:So run all this code at once:\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\nplt.savefig(\"plots/first_saved_plot.png\")\n\n\n\n\n\n\n\n\n\n\nMaking modifications\n\nTitles\nNotice that the \\(y\\) axis has an ugly label? That’s because seaborn is just drawing from your dataframe.\nWe can change axis labels with plt.ylabel()\n\n# Plotting functions\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Customisation\nplt.ylabel(\"Height (cm)\")\n\nText(4.8166666666666655, 0.5, 'Height (cm)')\n\n\n\n\n\n\n\n\n\nand similarly you could change plt.xlabel(...).\n\nMake sure you run the above line at the same time as your plotting function. You can either * Highlight all the code and press F9 * Make a cell with #%% and press ctrl + enter\n\nWe can also change the legend title to “positions” with plt.legend()\n\n# Plotting functions\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Customisation\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"Positions\")\n\n\n\n\n\n\n\n\nAnd its location with loc = \"lower left\"\n\n# Plotting functions\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Customisation\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"Positions\")\n\n\n\n\n\n\n\n\nAnd give the whole plot a title with plt.title()\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Titles\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"Positions\")\nplt.title(\"Are players' heights and ages related?\")\n\nText(0.5, 1.0, \"Are players' heights and ages related?\")\n\n\n\n\n\n\n\n\n\n\n\n\nAnnotations\nYou might want to annotate your plot with text and arrows. Text is simple with the plt.text() function; we just need to specify its coordinates and the contents.\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Titles\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"Positions\")\nplt.title(\"Are players' heights and ages related?\")\n\n# Annotations\nplt.text(38.5, 181, \"Not enough\\ndata for mean\")\n\nText(38.5, 181, 'Not enough\\ndata for mean')\n\n\n\n\n\n\n\n\n\n\nThe characters \\n mean ‘new line’\n\nWe could annotate with arrows too. This is more complex, using the plt.annotate() function:\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Titles\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"Positions\")\nplt.title(\"Are players' heights and ages related?\")\n\n# Annotations\nplt.text(38.5, 181, \"Not enough\\ndata for mean\")\nplt.annotate(text = \"No short\\nolder players\", xy = [37,165], xytext = [40,172],\n             arrowprops = dict(width = 1, headwidth = 10, headlength = 10, \n                          facecolor = \"black\"))\n\nText(40, 172, 'No short\\nolder players')\n\n\n\n\n\n\n\n\n\n\nI’ve split this over multiple lines, but its still one function - check the brackets\n\nAll together, our plot has become\n\n\nAxis limits\nThe last feature we’ll look at is editing axis limits. Let’s try to make more room in the bottom left for the legend with the functions plt.xlim() and plt.ylim()\n\n# Figure level plot\nsns.relplot(data = df, x = \"age\", y = \"height_cm\", s = 10, color = \"grey\")\n\n# Axes level plot (drop the kind = )\nsns.lineplot(data = inner_ages, x = \"age\", y = \"height_cm\", hue = \"positions\")\n\n# Titles\nplt.ylabel(\"Height (cm)\")\nplt.legend(title = \"Positions\", loc = \"lower left\")\nplt.title(\"Are players' heights and ages related?\")\n\n# Annotations\nplt.text(38.5, 181, \"Not enough\\ndata for mean\")\nplt.annotate(\"No short\\nolder players\", [37,165], [40,172], \n             arrowprops = dict(width = 1,headwidth = 10,headlength = 10, \n                               facecolor = \"black\"))\n\n# Axis limits\nplt.xlim([10,45])\nplt.ylim([150,210])\n\n\n\n\n\n\n\n\nI’m not sure that looks any better, but you get the idea!",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "3. Introductory Data Visualisation"
    ]
  },
  {
    "objectID": "Python/3-intro_visualisation/visualisation.html#conclusion",
    "href": "Python/3-intro_visualisation/visualisation.html#conclusion",
    "title": "Python Training (3 of 4): Introductory Data Visualisation",
    "section": "Conclusion",
    "text": "Conclusion\nAs we have seen, seaborn and matplotlib are a powerful tools for visualising data efficiently and aesthetically. A range of other plot types and customisation is available, for inspiration have a look at the seaborn gallery and matplotlib gallery. If any of the content here was too challenging, you have other related issues you’d like to discuss or would simply like to learn more, we the technology training team would love to hear from you. You can contact us at training@library.uq.edu.au.\nHere’s a summary of what we’ve covered\n\n\n\n\n\n\n\nTopic\nDescription\n\n\n\n\nPlotting\nUsing seaborn’s sns.catplot() (categorical), sns.relplot() (relational, i.e. scatter & line) and sns.displot() (distributive) functions, we can make plots by specifying various parameters, e.g. x = ..., y = ..., hue = ..., etc.\n\n\nIntroducing variables into visualisations\nWe don’t just have to use \\(x\\)- and \\(y\\)-axes: we can use colour (hue = ...), shape (style = ...), size (size = ...) and facets (col = ..., row = ...) to introduce more variables to our visualisations.\n\n\nOverlaying plots\nBy combining a figure-level plot (e.g. sns.catplot()) with multiple axes-level plots (e.g. sns.boxplot()), we can overlay multiple graphs onto the same visualisation\n\n\nSaving figures\nWe can use matplotlib’s function plt.savefig(...) to export our plots\n\n\nCustomisations\nThe functions plt.xlabel(), plt.ylabel() and plt.title() allow you to customise your plot’s axes. The plt.legend() function modifies the legend, and plt.xlim() and plt.ylim() adjust the axis limits.\n\n\nAnnotations\nUse the functions plt.text() and plt.annotate() to draw lines and text on your visualisation.\n\n\n\nBelow is a summary of all available* plots in seaborn. Most of these have been examined in either the introductory session or this one, however, there are some which we have not yet looked at. The seaborn documentation and tutorials provide desciptions and advice for all available plots.\n\n*As of v0.12.2\n\n\nFigure- to Axes-level plot\nAll the plots below are figure-level. To produce the axes-level plot of the same type, simply use\nsns.****plot()\nwhere **** is given in kind = \"****\" for the corresponding figure-level plot. For example,\nsns.relplot( ..., kind = \"scatter\", ... ) # Figure-level scatter plot\nsns.scatterplot( ... ) # Axes-level scatter plot\n\n\nRelational Plots\n\n\n\n\n\n\n\n\nPlot Name\nCode\nNotes\n\n\n\n\nScatter Plot\nsns.relplot( ... , kind = \"scatter\", ... )\nRequires numerical data\n\n\nLine Plot\nsns.relplot( ... , kind = \"line\", ... )\nRequires numerical data\n\n\n\n\n\nDistributions\n\n\n\n\n\n\n\n\nPlot Name\nCode\nNotes\n\n\n\n\nHistogram\nsns.displot( ... , kind = \"hist\", ... )\nCan be univariate (x only) or bivariate (x and y)\n\n\nKernel Density Estimate\nsns.displot( ... , kind = \"kde\" , ... )\nCan be univariate (x only) or bivariate (x and y)\n\n\nECDF*\nsns.displot( ... , kind = \"ecdf\", ... )\n.\n\n\nRug Plot\nsns.displot( ... , rug = True , ... )\nCombine with another sns.displot, plots marginal distributions\n\n\n\n\n*Empirical Cumulative Distribution Functions\n\n\n\nCategorical Plots\n\n\n\n\n\n\n\n\nPlot Name\nCode\nNotes\n\n\n\n\nStrip Plot\nsns.catplot( ... , kind = \"strip\" , ... )\nLike a scatterplot for categorical data\n\n\nSwarm Plot\nsns.catplot( ... , kind = \"swarm\" , ... )\n.\n\n\nBox Plot\nsns.catplot( ... , kind = \"box\" , ... )\nOne variable is always interpreted categorically\n\n\nViolin Plot\nsns.catplot( ... , kind = \"violin\" , ... )\nOne variable is always interpreted categorically\n\n\nEnhanced Box Plot\nsns.catplot( ... , kind = \"boxen\", ... )\nA box plot with additional quantiles\n\n\nPoint Plot\nsns.catplot( ... , kind = \"point\" , ... )\nLike a line plot for categorical data\n\n\nBar Plot\nsns.catplot( ... , kind = \"bar\" , ... )\nAggregates data\n\n\nCount Plot\nsns.catplot( ... , kind = \"count\" , ... )\nA bar plot with the total number of observations\n\n\n\n\n\nOther Plots\n\n\n\n\n\n\n\n\nPlot Name\nCode\nNotes\n\n\n\n\nPair Plot\nsns.pairplot( ... )\nA form of facetting\n\n\nJoint Plot\nsns.jointplot( ... )\n.\n\n\nRegressions\nsns.lmplot( ... )\n.\n\n\nResidual Plot\nsns.residplot( ... )\nThe residuals of a linear regression\n\n\nHeatmap\nsns.heatmap( ... )\n.\n\n\nClustermap\nsns.clustermap( ... )\n.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "3. Introductory Data Visualisation"
    ]
  },
  {
    "objectID": "Python/3-intro_visualisation/visualisation.html#resources",
    "href": "Python/3-intro_visualisation/visualisation.html#resources",
    "title": "Python Training (3 of 4): Introductory Data Visualisation",
    "section": "Resources",
    "text": "Resources\n\nOfficial seaborn documentation\nOfficial matplotlib documentation\nOur compilation of useful Python links",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "3. Introductory Data Visualisation"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html",
    "href": "Python/1-fundamentals/fundamentals.html",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "",
    "text": "TipUpcoming workshop(s) available!\n\n\n\nThe next workshop is on Fri Feb 27 at 09:30 AM.\nBook in to the next offering now.\nAlternatively, check our calendar for future events.\nThis hands-on programming course – directed at beginners – will get you started on using Python 3 and the program Spyder to import, explore, analyse and visualise data.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#setup",
    "href": "Python/1-fundamentals/fundamentals.html#setup",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Setup",
    "text": "Setup\nThe easiest way to use Python 3 and Spyder is to install the Anaconda Distribution, a data science platform for Windows, Linux and macOS.\nOpen the Anaconda Navigator (you might have to run anaconda-navigator from a terminal on Linux), and launch Spyder. On some operating systems, you might be able to find Spyder directly in your applications.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#introducing-python-and-spyder",
    "href": "Python/1-fundamentals/fundamentals.html#introducing-python-and-spyder",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Introducing Python and Spyder",
    "text": "Introducing Python and Spyder\nPython is a programming language that can be used to build programs (i.e. a “general programming language”), but it can also be used to analyse data by importing a number of useful modules.\nWe are using Spyder to interact with Python more comfortably. If you have used RStudio to interact with R before, you should feel right at home: Spyder is a program designed for doing data science with Python.\nWe will start by using the console to work interactively. This is our direct line to the computer, and is the simplest way to run code. Don’t worry about any unfamiliar language, fonts or colours - we can ignore most of it for now - all you need to know is that\n\nIn [1]: ... is code that we’ve sent to the computer, and\nOut[1]: ... is its response.\n\n\nMaths\nTo start with, we can use Python like a calculator. Type the following commands in the console, and press Enter to execute them:\n\n1 + 1\n2 * 3\n4 / 10\n5 ** 2\n\n25\n\n\nAfter running each command, you should see the result as an output.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#variables",
    "href": "Python/1-fundamentals/fundamentals.html#variables",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Variables",
    "text": "Variables\n\nNumbers\nMost programming languages are like spoken language in that they have nouns and verbs - you have “things” and they “do things”. In Python, we have variables and functions. We’ll look first at variables, the nouns of Python, which store data.\nTo create a variable, we choose its name (e.g. favNumber) and assign (=) it a value (e.g. 42):\n\nexample_int = 42\n\nYou can then retrieve the value by running the variable name on its own:\n\nexample_int\n\n42\n\n\nLet’s create more variables. We can use the variable names in place of their values, so we can perform maths:\n\nexample_float = 5.678\nexample_int * example_float\n\n238.476\n\n\nSo far, we’ve only looked at numbers. If you click on the “variable explorer” tab, you should see two variables.\nNotice that the “Type” of example_int is int, while the other is float. These are different variable types and can operate differently. int means integer, and corresponds to whole numbers, while float stands for floating point number, meaning decimals. You may occasionally encounter errors where you can only use one type.\n\n\nBooleans\nEven simpler than integers is the boolean type. These are either 1 or 0 (True or False), representing a single binary unit (bit). Don’t be fooled by the words, these work like numbers: True + True gives 2.\n\nexample_bool = True\n\n\nIn Python, the boolean values True and False must begin with a capital letter.\n\n\n\nSequences\nLet’s look at variable types which aren’t (necessarily) numbers. Sequences are variables which store more than one data point. For example, strings store a sequence of characters and are created with quotation marks '&lt;insert string&gt;' or \"&lt;insert string&gt;\":\n\nexample_string = 'Hello world!'\n\nWe can also create lists, which will store several variables (not necessarily of the same type). We need to use square brackets for that:\n\nexample_list = [38, 3, 54, 17, 7]\ndiverse_list = [3, 'Hi!', 9.0]\n\nLists are very flexible as they can contain any number of items, and any type of data. You can even nest lists inside a list, which makes for a very flexible data type.\nOperations on sequences are a bit different to numbers. We can still use + and *, but they will concatenate (append) and duplicate, rather than perform arithmetic.\n\nexample_string + ' How are you?'\nexample_list + diverse_list\n3 * example_list\n\n[38, 3, 54, 17, 7, 38, 3, 54, 17, 7, 38, 3, 54, 17, 7]\n\n\nHowever, depending on the variable, some operations won’t work:\n\nexample_string + example_int\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[9], line 1\n----&gt; 1 example_string + example_int\n\nTypeError: can only concatenate str (not \"int\") to str\n\n\n\nThere are other data types like tuples, dictionaries and sets, but we won’t look at those in this session. Here’s a summary of the ones we’ve covered:\n\n\n\n\n\n\n\n\n\n\nCategory\nType\nShort name\nExample\nGenerator\n\n\n\n\nNumeric\nInteger\nint\n3\nint()\n\n\nNumeric\nFloating Point Number\nfloat\n4.2\nfloat()\n\n\nNumeric\nBoolean\nbool\nTrue\nbool()\n\n\nSequence\nString\nstr\n'A sentence '\n\" \" or ' ' or str()\n\n\nSequence\nList\nlist\n['apple', 'banana', 'cherry']\n[ ] or list()\n\n\n\nThe generator commands are new. We use these to manually change the variable type. For example,\n\nint(True)\n\n1\n\n\nyields 1, converting a boolean into an integer. These commands are functions, as opposed to variables - we’ll look at functions a bit later.\n\nIndexing\nWe can access part of a sequence by indexing. Sequences are ordered, starting at 0, so the first element has index 0, the second index 1, the third 2 and so on. For example, see what these commands return:\n\nexample_string[0]\nexample_string[6]\nexample_list[4]\n\n7\n\n\nIf you want more than one element in a sequence, you can slice. Simple slices specify a range to slice, from the first index to the last, but not including the last. For example:\n\nexample_list[0:4]\n\n[38, 3, 54, 17]\n\n\nThat command returns elements from position 0 up to - but not including! - position 4.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#scripts-and-projects",
    "href": "Python/1-fundamentals/fundamentals.html#scripts-and-projects",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Scripts and Projects",
    "text": "Scripts and Projects\nSo far, we’ve been working in the console, our direct line to the computer. However, it is often more convenient to use a script. These are simple text files which store code and run when we choose. They are useful to\n\nwrite code more comfortably,\nstore clearly defined steps in chronological order,\nshare a process with peers easily, and\nmake your work reproducible\n\nLet’s create a folder system to store our script in by creating a project.\n\nPress Projects &gt; New project... and name your project, perhaps “python_fundamentals”.\nCreate a new script with ctrl+N, File &gt; New file... or the new file button  .\n\nYou should now see a script on the left panel in Spyder, looking something like this:\n\n\n\nExample of a new script\n\n\nTry typing a line of code in your new script, such as\n\n1 + 1\n\n2\n\n\nPress F9 to run each line, or ctrl+enter for the whole script. You should see something like the following appear in the console (depending on how you ran it):\n\n\n\nExample of a console input\n\n\nWe’ll work out of a script for the rest of the session. Don’t forget to save your script by pressing ctrl+S or the save button .",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#functions",
    "href": "Python/1-fundamentals/fundamentals.html#functions",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Functions",
    "text": "Functions\nFunctions are little programs that do specific jobs. These are the verbs of Python, because they do things to and with our variables. Here are a few examples of built-in functions:\n\nexample_list = [1,2,3,4]\nlen(example_list)\nmin(example_list)\nmax(example_list)\nsum(example_list)\nround(5.123)\n\n5\n\n\nFunctions always have parentheses after their name, and they can take one or several arguments, or none at all, depending on what they can do, and how the user wants to use them.\nHere, we use two arguments to modify the default behaviour of the round() function:\n\nround(5.123, 2)\n\n5.12\n\n\n\nNotice how Spyder gives you hints about the available arguments after typing the function name?\n\n\nOperators\nOperators are a special type of function in Python with which you’re already familiar. The most important is =, which assigns values to variables. Here is a summary of some important operators, although there are many others:\n\nGeneral\n\n\n\n\n\n\n\n\n\nOperator\nFunction\nDescription\nExample command\n\n\n\n\n=\nAssignment\nAssigns values to variables\na = 7\n\n\n#\nComment\nExcludes any following text from being run\n# This text will be ignored by Python\n\n\n\n\n\nMathematical\n\n\n\n\n\n\n\n\n\n\nOperator\nFunction\nDescription\nExample command\nExample output\n\n\n\n\n+\nAddition\nAdds or concatenates values, depending on variable types\n7 + 3 or \"a\" + \"b\"\n10 or 'ab'\n\n\n-\nSubtraction\nSubtracts numerical values\n8 - 3\n5\n\n\n*\nMultiplication\nMultiplies values, depending on variable types\n7 * 2 or \"a\" * 3\n14 or 'aaa'\n\n\n/\nDivision\nDivides numerical vlues\n3 / 4\n0.75\n\n\n**\nExponentiation\nRaises a numerical value to a power\n7 ** 2\n49\n\n\n%\nRemainder\nTakes the remainder of numerical values\n13 % 7\n6\n\n\n\n\n\nComparison\n\n\n\n\n\n\n\n\n\n\nOperator\nFunction\nDescription\nExample command\nExample output\n\n\n\n\n==\nEqual to\nChecks whether two variables are the same and outputs a boolean\n1 == 1\nTrue\n\n\n!=\nNot equal to\nChecks whether two variables are different\n'1' != 1\nTrue\n\n\n&gt;\nGreater than\nChecks whether one variable is greater than the other\n1 &gt; 1\nFalse\n\n\n&gt;=\nGreater than or equal to\nChecks whether greater than (&gt;) or equal to (==) are true\n1 &gt;= 1\nTrue\n\n\n&lt;\nLess than\nChecks whether one variable is less than the other\n0 &lt; 1\nTrue\n\n\n&lt;=\nLess than or equal to\nChecks whether less than (&lt;) or equal to (==) are true\n0 &lt;= 1\nTrue",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#finding-help",
    "href": "Python/1-fundamentals/fundamentals.html#finding-help",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Finding help",
    "text": "Finding help\nTo find help about a function, you can use the help() function, or a ? after a function name:\n\nhelp(max)\nprint?\n\nHelp on built-in function max in module builtins:\n\nmax(...)\n    max(iterable, *[, default=obj, key=func]) -&gt; value\n    max(arg1, arg2, *args, *[, key=func]) -&gt; value\n\n    With a single iterable argument, return its biggest item. The\n    default keyword-only argument specifies an object to return if\n    the provided iterable is empty.\n    With two or more arguments, return the largest argument.\n\n\n\nIn Spyder, you can use the Ctrl + I keyboard shortcut to open the help in a separate pane.\n\nThe help information can often be dense and difficult to read at first, taking some practice. In the next session we look closer at interpreting this documentation, one of the most important Python skills.\n\nFor a comprehensive manual, go to the official online documentation. For questions and answers, typing the right question in a search engine will usually lead you to something helpful. If you can’t find an answer, StackOverflow is a great Q&A community.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#activity-1",
    "href": "Python/1-fundamentals/fundamentals.html#activity-1",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Activity 1",
    "text": "Activity 1\nIn this first activity, write a program which takes an age in years and outputs how many minutes they’ve lived for. Note that\n\\[\\text{Age (minutes)} = \\text{Age (years)} \\times 365 \\times 24 \\times 60\\]\nSteps\n\nStore the age in years in a variable\nCalculate the age in minutes\nPrint a message with the output\n\n\nNote: if you want to print a number (e.g. the age), the easiest way is to send multiple arguments to the print function. For example, print(\"The first number is\", 1)\n\n\n\n\n\n\n\nNoteAdvanced\n\n\n\nIf this is too easy, try to get the user to provide their age themselves with the command int(input(...)). Make sure to use int(), otherwise the string multiplication will crash Spyder. You’ll want the documentation for input().\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nWe have three lines of code corresponding to the steps above:\n\n### Age in minutes calculator\n  \n# Input age\nage_years = 56\n\n# Calculate age in mins\nage_mins = age_years * 365 * 24 * 60\n\n# Print result\nprint(\"You have lived for\", age_mins, \"minutes!\")\n\nYou have lived for 29433600 minutes!\n\n\nTo include the input() command,\n\n### Age in minutes calculator\n  \n# Input age\nage_years = int(input(\"What is your age? \"))\n\n# Calculate age in mins\nage_mins = age_years * 365 * 24 * 60\n\n# Print result\nprint(\"You have lived for\", age_mins, \"minutes!\")",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#packages",
    "href": "Python/1-fundamentals/fundamentals.html#packages",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Packages",
    "text": "Packages\nPython is set apart from other languages by the scale of its community packages and the ease with which you import them. While you could code everything you need from scratch, it’s often more effective to import someone else’s predefined functions.\n\nBuilt-in packages\nPython comes with a number of pre-installed packages, so they’re already on your computer. However, your specific Python application doesn’t have access to them until they’re imported:\n\nimport math\n\nThe module math brings in some mathematics constants and functions. For example, you will get an error if you run pi on its own, but we can access the constant using the module:\n\nmath.pi\n2*math.pi\nmath.cos(math.pi)\n\n-1.0\n\n\nNote that we use a period . in order to access objects inside the module. In general, we use periods in Python to access objects stored inside other objects.\n\n\nNaming\nSome modules have long names and use abbreviated nicknames when imported.\n\nimport math as m\nm.pi\n\n3.141592653589793\n\n\nHere the module math is stored as m in Python.\nWhere this naming is used, it is usually the standard, and sharing code with different (including original/full) module names will not be compatible with other programmers.\n\n\nCommon external packages\nThere are hundreds of thousands of external packages available, and Anaconda comes with the most common. For example, the numpy module, which enables numerical methods in Python.\nYou might recall that multiplication for sequences replicates them. This makes multiplying numeric lists unexpected:\n\n3 * [1,2,3]\n\n[1, 2, 3, 1, 2, 3, 1, 2, 3]\n\n\nWhat if we wanted to multiply each element by 3? Well, numpy has a variable where this does happen: the array. To turn a list into an array, use the np.array() function\n\nimport numpy as np\nexample_array = np.array([1,2,3])\n\n3 * example_array\n\narray([3, 6, 9])\n\n\nThat works well!\nSome popular packages include\n\n\n\n\n\n\n\n\n\nPackage\nInstall command\nImport command\nDescription\n\n\n\n\nNumPy\npip/conda install numpy\nimport numpy as np\nA numerical Python package, providing mathematical functions and constants, vector analysis, linear algebra etc.\n\n\nPandas\npip/conda install pandas\nimport pandas as pd\nPanel Data - data transformation, manipulation and analysis\n\n\nMatplotlib\npip/conda install matplotlib\nimport matplotlib.pyplot as plt\nMathematical ploting library, a popular visualisation tool. Note that there are other ways to import it, but the .pyplot submodule as plt is most common.\n\n\nSeaborn\npip/conda install seaborn\nimport seaborn as sns\nAnother visualisation tool, closer to ggplot2 in R, built upon a matplotlib framework.\n\n\nSciPy\npip/conda install scipy\nimport scipy or import scipy as sp\nA scientific Python package with algorithms and models for analysis and statistics.\n\n\nStatsmodels\npip/conda install statsmodels\nimport statsmodels.api as sm and/or import statsmodels.tsa.api as tsa\nStatistical modelling. The first import sm is for cross-sectional models, while tsa is for time-series models.\n\n\nRequests\npip/conda install requests\nimport requests\nMake HTTP (internet) requests.\n\n\nBeautiful Soup\npip/conda install beautifulsoup4\nfrom bs4 import BeautifulSoup\nCollect HTML data from websites.\n\n\n\n\n\nInstalling external packages\nMost external packages do not get shipped with Anaconda, and you’ll need to install these yourself.\n\nUsing conda\nIf you are using Anaconda, then the recommended installation method is using the conda command, which installs from Anaconda’s package database:\n\nconda install numpy\n\nIf you’ve got Spyder running, you’ll need to restart the kernel. In the console, press the options button (three horizontal lines) &gt; restart kernel.\n\n\n\n\n\n\nNoteSyntaxError: invalid syntax\n\n\n\nIf this doesn’t work for you, try using !conda install numpy - using an exclamation mark ! sends your command straight to your operating system shell.\nIf that doesn’t work, try using conda install numpy from an Anaconda prompt.\n\n\n\n\nUsing pip\nIf you are not using Anaconda, then the most common way to install a package is using the command pip, which installs packages from the Python Package Index (PyPI)\n\npip install numpy\n\n\n\n\n\n\n\nNoteSyntaxError: invalid syntax\n\n\n\nIf this doesn’t work for you, try using !pip install numpy - using an exclamation mark ! sends your command straight to your operating system shell.\nIf that doesn’t work, try using pip install numpy from an command prompt.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#activity-2",
    "href": "Python/1-fundamentals/fundamentals.html#activity-2",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Activity 2",
    "text": "Activity 2\nIn this final activity, we’re going to create some sample data and visualise it.\nOur goal is to import and visualise random BMI data\nWe’ll complete this in two parts. Before we begin, we need to set things up by importing the modules we need\n\nimport pandas as pd\nimport seaborn as sns\n\n\nIf importing any of these causes an error, it could be because you haven’t installed it yet. See above for information on how to do so.\n\nBefore we begin this activity we should bring in the data. To do this, we use the pd.read_csv() function, specifying the file path as the first argument (this can be a URL), and store it in a variable (typically df). For example,\n\ndf = pd.read_csv(\"insert_filepath_here\")\n\nToday’s data is five (random) people’s height and weight. You can download it here.\n\nDownload the data\nMove the data into your project folder\nRead it in with df = pd.read_csv(\"BMI_data.csv\")\n\n\nPart 1: Modifying the data\nFor the first part of the challenge, you’ll need to compute each person’s BMI, and store it in a new column. For reference, we access columns by indexing based on their name, e.g. df[\"Weight\"] is the Weight column. To make a new column, we pretend that it already exists and assign into it. For example, to convert from kilograms to pounds,\n\n# Create a new column called Weight (lb) and store the weight in pounds\ndf[\"Weight (lb)\"] = df[\"Weight\"]*2.205\n\nTo compute the BMIs, make another new column and use the following formula to calculate the BMI.\n\\[ \\text{BMI} = \\frac{\\text{Weight (kg)}}{(\\text{Height (m)})^2} \\]\nIt should look something like\n\ndf[\"BMI\"] = ...\n\n\nHint: \\(x^2\\) is x**2\n\nOnce you’ve done these steps, you should see the following:\n\n\n\n\n\n\n\n\n\nNames\nHeight\nWeight\nWeight (lb)\nBMI\n\n\n\n\n0\nAlice\n1.90\n94\n207.27\n26.038781\n\n\n1\nBob\n1.81\n102\n224.91\n31.134581\n\n\n2\nCharlie\n1.87\n108\n238.14\n30.884498\n\n\n3\nDilsah\n1.88\n84\n185.22\n23.766410\n\n\n4\nEliza\n1.68\n108\n238.14\n38.265306\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nOne solution could be the following:\n\n# Import packages\nimport pandas as pd\nimport seaborn as sns\n\n# Import data - don't forget to change the file path as you need\ndf = pd.read_csv(\"BMI_data.csv\")\n\n# Example - create a new column called Weight (lb) and store the weight in pounds\ndf[\"Weight (lb)\"] = df[\"Weight\"]*2.205\n\n# Create BMI column\ndf[\"BMI\"] = df[\"Weight\"] / (df[\"Height\"]**2)\n\n# Look at the data\ndf\n\n\n\n\n\n\nPart 2: Visualisation\nTo visualise the data, we can use the seaborn module, with the function sns.catplot( ... ). Inside the function, we’ll need to specify the x and y values, and if we specifically want a bar plot, kind as well. Use the help() documentation to see if you can visualise the data we just created. See if you can produce something like the following plot:\n\n\n\n\n\n\n\n\n\nYou’ll need to start with\n\nsns.catplot(data = df, x = ...)\n\n\nHint: You only need to use the data =, x =, y = and kind = parameters, so try figure out what they require!\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe plot above is produced with the code\n\n# Visualise\nsns.catplot(data = df, x = \"Names\", y = \"BMI\", kind = \"bar\")",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "Python/1-fundamentals/fundamentals.html#conclusion-and-saving-your-work",
    "href": "Python/1-fundamentals/fundamentals.html#conclusion-and-saving-your-work",
    "title": "Python training (1 of 4): The Fundamentals",
    "section": "Conclusion and saving your work",
    "text": "Conclusion and saving your work\nYour project can be reopened from the “Projects” menu in Spyder.\nBy default, your variables are not saved, which is another reason why working with a script is important: you can execute the whole script in one go to get everything back. You can however save your variables as a .spydata file if you want to (for example, if it takes a lot of time to process your data).\n\nSummary\nToday we looked at a lot of Python features, so don’t worry if they haven’t all sunk in. Programming is best learned through practice, so keep at it! Here’s a rundown of the concepts we covered\n\n\n\n\n\n\n\nConcept\nDesctiption\n\n\n\n\nThe console vs scripts\nThe console is our window into the computer, this is where we send code directly to the computer. Scripts are files which we can write, edit, store and run code, that’s where you’ll write most of your Python.\n\n\nVariables\nVariables are the nouns of programming, this is where we store information, the objects and things of our coding. They come in different types like integers, strings and lists.\n\n\nIndexing\nIn order to access elements of a sequence variable, like a list, we need to index, e.g. myList[2]. Python counts from 0.\n\n\nFunctions\nFunctions are the verbs of programming, they perform actions on our variables. Call the function by name and put inputs inside parentheses, e.g. round(2.5)\n\n\nHelp\nRunning help( ... ) will reveal the help documentation about a function or type.\n\n\nConditionals\nif, elif and else statements allow us to run code if certain conditions are true, and skip it otherwise.\n\n\nLoops\nwhile loops will repeatedly run code until a condition is no longer true, and for loops will iterate through a variable\n\n\nPackages\nWe can bring external code into our environment with import .... This is how we use packages, an essential for Python. Don’t forget to install the package first!\n\n\n\n\n\nNext session\nThanks for completing this introductory session to Python! You’re now ready for our next session, Data Transformation, which looks at using the pandas package in greater depth.\nBefore you go, don’t forget to check out the Python User Group, a gathering of Python users at UQ.\nFinally, if you need any support or have any other questions, shoot us an email at training@library.uq.edu.au.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "1. The Fundamentals"
    ]
  },
  {
    "objectID": "PowerPoint/powerpoint.html",
    "href": "PowerPoint/powerpoint.html",
    "title": "Microsoft PowerPoint",
    "section": "",
    "text": "Find below the resources for our Powerpoint sessions.\n\nManual\nFiles",
    "crumbs": [
      "Home",
      "![](/images/powerpoint.svg){width=20} PowerPoint"
    ]
  },
  {
    "objectID": "Orientation/orientation.html",
    "href": "Orientation/orientation.html",
    "title": "O Week resources",
    "section": "",
    "text": "Orientation runs for 2 weeks at the beginning of each semester.\nBelow are the slides used during our Library sessions."
  },
  {
    "objectID": "Orientation/orientation.html#more-than-books-make-the-most-of-your-library",
    "href": "Orientation/orientation.html#more-than-books-make-the-most-of-your-library",
    "title": "O Week resources",
    "section": "More than books: Make the most of your library",
    "text": "More than books: Make the most of your library\n\nSlides\nVideo"
  },
  {
    "objectID": "Orientation/orientation.html#get-connected-wifi-and-logging-in",
    "href": "Orientation/orientation.html#get-connected-wifi-and-logging-in",
    "title": "O Week resources",
    "section": "Get connected (WiFi and logging in)",
    "text": "Get connected (WiFi and logging in)\n\nSlides\nVideo"
  },
  {
    "objectID": "Orientation/orientation.html#systems-and-software-at-uq",
    "href": "Orientation/orientation.html#systems-and-software-at-uq",
    "title": "O Week resources",
    "section": "Systems and Software at UQ",
    "text": "Systems and Software at UQ\n\nSlides\nVideo"
  },
  {
    "objectID": "Orientation/orientation.html#navigating-learn.uq-blackboard",
    "href": "Orientation/orientation.html#navigating-learn.uq-blackboard",
    "title": "O Week resources",
    "section": "Navigating Learn.UQ (Blackboard)",
    "text": "Navigating Learn.UQ (Blackboard)\n\nSlides\nVideo"
  },
  {
    "objectID": "OSM/osm_slides.html#what-is-osm",
    "href": "OSM/osm_slides.html#what-is-osm",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": " What is OSM?",
    "text": "What is OSM?\nSee the front page: https://www.openstreetmap.org\n\nCommunity-driven project, mainly crowdsourced from volunteers 💚\nValues local knowledge 📌\nReleased as open data: Open Database Licence 👐🏽\nSupported by non-for-profit: OSM Foundation 💸"
  },
  {
    "objectID": "OSM/osm_slides.html#statistics",
    "href": "OSM/osm_slides.html#statistics",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": "Statistics",
    "text": "Statistics\n\n\n2004 -&gt; 2025 📈 :\n\n10+ million contributors\n165+ million changesets\n9+ billion nodes"
  },
  {
    "objectID": "OSM/osm_slides.html#uses",
    "href": "OSM/osm_slides.html#uses",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": "Uses",
    "text": "Uses"
  },
  {
    "objectID": "OSM/osm_slides.html#uses-2",
    "href": "OSM/osm_slides.html#uses-2",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": "Uses (2)",
    "text": "Uses (2)\n\n\n\n\n\nWheelmap\n\n\n\n\n\n\nBriscycle"
  },
  {
    "objectID": "OSM/osm_slides.html#uses-3",
    "href": "OSM/osm_slides.html#uses-3",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": "Uses (3)",
    "text": "Uses (3)\n\n\n\n\n\nOrganic Maps\n\n\n\n\n\n\nOpenBeerMap"
  },
  {
    "objectID": "OSM/osm_slides.html#considerations",
    "href": "OSM/osm_slides.html#considerations",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": "Considerations ⚠️",
    "text": "Considerations ⚠️\n\nKeep in mind:\n\nCoverage very variable: https://tyrasd.github.io/osm-node-density\nOnly as good as the contributed data\n\nHowever:\n\nPossibly the only source of data\nPossibly more up to date than others\n\nUse your judgment!"
  },
  {
    "objectID": "OSM/osm_slides.html#missing-maps",
    "href": "OSM/osm_slides.html#missing-maps",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": "Missing Maps",
    "text": "Missing Maps\nHumanitarian project that maps parts of the world that are vulnerable to natural disasters, conflicts, and epidemics.\nFounders:\n\nEmphasizes engagement of, and respect towards the local community. 🤝"
  },
  {
    "objectID": "OSM/osm_slides.html#missing-maps-1",
    "href": "OSM/osm_slides.html#missing-maps-1",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": "Missing Maps",
    "text": "Missing Maps\nImpact:\n \n2010 Haïti earthquake (left); 2017 South Asian floods (right)"
  },
  {
    "objectID": "OSM/osm_slides.html#lets-map",
    "href": "OSM/osm_slides.html#lets-map",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": "Let’s map!",
    "text": "Let’s map!\nhttps://osmlab.github.io/show-me-the-way/"
  },
  {
    "objectID": "OSM/osm_slides.html#what-else-can-be-mapped",
    "href": "OSM/osm_slides.html#what-else-can-be-mapped",
    "title": "OpenStreetMap: create and extract geospatial data",
    "section": "What else can be mapped?",
    "text": "What else can be mapped?\nA lot: https://wiki.openstreetmap.org/wiki/Map_Features"
  },
  {
    "objectID": "NVivo/nvivo.html",
    "href": "NVivo/nvivo.html",
    "title": "NVivo",
    "section": "",
    "text": "NVivo is a Qualitative Data Analysis (QDA) tool.\nFind below the resources for our NVivo sessions.",
    "crumbs": [
      "Home",
      "![](/images/nvivo.png){width=20} NVivo"
    ]
  },
  {
    "objectID": "NVivo/nvivo.html#getting-started",
    "href": "NVivo/nvivo.html#getting-started",
    "title": "NVivo",
    "section": "Getting started",
    "text": "Getting started\n\nManual\nFiles\nVideo",
    "crumbs": [
      "Home",
      "![](/images/nvivo.png){width=20} NVivo"
    ]
  },
  {
    "objectID": "NVivo/nvivo.html#next-steps",
    "href": "NVivo/nvivo.html#next-steps",
    "title": "NVivo",
    "section": "Next steps",
    "text": "Next steps\n\nManual\nFiles - No files for Next Steps. Use a sample project from the opening screen",
    "crumbs": [
      "Home",
      "![](/images/nvivo.png){width=20} NVivo"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html",
    "href": "LaTeX/overleaf_latex.html",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "",
    "text": "\\(\\LaTeX\\) is a markup language and software system that allows writing beautiful documents. It is widely used in professional mathematics writing in particular because of its powerful mathematical formula typesetting, but it is also popular in other fields, including philosophy, physics, economics, engineering and linguistics.\nThe strong point of LaTeX is that it allows you to create very consistent documents, while concentrating on the contents and the structure, rather than worry about the appearance. The style of the document can then be changed independently of the contents.\nTo author today’s document, we use Overleaf: an online platform that facilitates editing and collaborating on LaTeX documents.\n\n\nThe UQ community has access to Overleaf Professional accounts. The extra features offered (when compared to free accounts) are:\n\nUnlimited collaborators\nReal-time track changes\nFull document history\nAdvanced reference search\nReference manager sync\nDropbox integration\nGit and GitHub integration\nPriority support\n\nTo access your professional account, sign into Overleaf with your UQ email address.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#introduction",
    "href": "LaTeX/overleaf_latex.html#introduction",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "",
    "text": "\\(\\LaTeX\\) is a markup language and software system that allows writing beautiful documents. It is widely used in professional mathematics writing in particular because of its powerful mathematical formula typesetting, but it is also popular in other fields, including philosophy, physics, economics, engineering and linguistics.\nThe strong point of LaTeX is that it allows you to create very consistent documents, while concentrating on the contents and the structure, rather than worry about the appearance. The style of the document can then be changed independently of the contents.\nTo author today’s document, we use Overleaf: an online platform that facilitates editing and collaborating on LaTeX documents.\n\n\nThe UQ community has access to Overleaf Professional accounts. The extra features offered (when compared to free accounts) are:\n\nUnlimited collaborators\nReal-time track changes\nFull document history\nAdvanced reference search\nReference manager sync\nDropbox integration\nGit and GitHub integration\nPriority support\n\nTo access your professional account, sign into Overleaf with your UQ email address.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#new-project",
    "href": "LaTeX/overleaf_latex.html#new-project",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "New project",
    "text": "New project\nIn Overleaf, creating a project will create a new space to store, upload and edit files.\nOn the main page, once logged in:\n\nClick on “New Project”\nSelect “Example Project”\nGive it a name and click “Create”\n\nOverleaf will then take you to a new document called “main.tex”: the source code is in the panel on the left, whereas the “compiled” document is shown on the right.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#overleaf-interface",
    "href": "LaTeX/overleaf_latex.html#overleaf-interface",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "Overleaf interface",
    "text": "Overleaf interface\nApart from the source and PDF panels, Overleaf includes a left-hand sidebar that shows the project files and the file outline (very useful to jump from section to section).\nThe top-left “Menu” button reveals another sidebar with project options, editor customisations and help links.\nYou can resize and hide the panels at you will to make editing the document more comfortable.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#editing",
    "href": "LaTeX/overleaf_latex.html#editing",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "Editing",
    "text": "Editing\n\nPreamble\nThe top of the source code, before the \\begin{document} tag, is called the Preamble. This is where the settings and main metadata of the document are stored. Overleaf automatically populated with some defaults, but as we need more features to write our document, we regularly go back to the preamble to add extra packages and modify their settings.\nThe following table describes what the example document contains in its preamble:\n\n\n\nTag\nWhat it does\n\n\n\n\n\\documentclass{article}\nSet the document type\n\n\n\\usepackage[english]{babel}\nSet the document language\n\n\n\\usepackage[...]{geometry}\nSet the page and margin sizes\n\n\n\\usepackage{amsmath}\nDisplay options for math equations\n\n\n\\usepackage{graphicx}\nInclude graphics\n\n\n\\usepackage[...]{hyperref}\nInclude hyperlinks\n\n\n\\title{Your Paper}\nDocument title\n\n\n\\author{You}\nDocument author\n\n\n\nMake sure you update the title and author of the document. We might also want to change the paper format to “a4paper” instead of “letterpaper”, and add an extra \\date{...} tag to fix the date.\n\n\nThe tab key\nSome of the most useful shortcuts in Overleaf are provided by the tab key. It helps autocomplete your code: using the tab key, you can autocomplete the name of a command from the dropdown list of suggestions, and some values can also be autocompleted inside the curly braces (for example, when using references to figures).\nThe same key also helps move outside of the curly braces.\n\n\nCompiling\nAfter editing the document, you will have to compile the document to see the results. This can be done with the green “Recompile” button, or using the ctrl + return shortcut.\n\n\nEditing the contents\nThe contents of the document are in between the \\begin{document} and \\end{document} tags.\nYou can switch between the “Source” editor, which shows you the LaTeX code, and the “Rich Text” editor, which is more similar to visual text editors we are used to, like Microsoft Word or LibreOffice Writer – although you will quickly find that the features of the Rich Text editor are very limiting.\nTry adding a new section to your document (with the “Insert Section Heading” button), and styling it with the “Rich Text” mode. See what the corresponding code looks like in the “Source” mode.\n\n\nCommon tags\nHere are a few common tags used in formatting the text:\n\n\n\nTag\nWhat it defines\n\n\n\n\n\\textbf{bold text}\nbold text\n\n\n\\textit{italic text}\nitalic text\n\n\n\\section{main sections}\nmain section titles\n\n\n\\subsection{second level sections}\nsecond level section titles\n\n\n\\subsubsection{second level sections}\nthird level section titles\n\n\n\n\n\nLists\nNow, add a list to your article. The tag itemize is for unordered lists, whereas enumerate is for numbered lists:\nUnordered list:\n\\begin{itemize}\n    \\item teatree\n    \\item bottlebrush\n    \\item melaleuca\n\\end{itemize}\nAnd numbered:\n\\begin{enumerate}\n    \\item apple\n    \\item rose\n    \\item peach\n\\end{enumerate}\nWhich results in the following:\n\n\n\nUnordered (bulleted) and ordered (numbered) lists, three items each.\n\n\n\n\nErrors\nIn the “Source” editor, you will notice that you get some visual feedback if your code is not valid. Try for example to misspell one of the tags opening or closing your bullet list: the whole block is highlighted in red, and you can get a hint of what the issue is by hovering over the red mark in the margin. If you try to compile the document, you will also see an error message in the Logs (next to the “Recompile” button).\n\n\n\nWhen hovering over the margin’s red mark open a tooltip with an error message: “unclosed \\begin{itemize} found at \\end{list}”\n\n\n\n\nHyperlinks\nTo be able to add hyperlinks, you will need to add the hyperref package at the top of your document, with this line:\n\\usepackage{hyperref}\nYou’ll then be able to add external URLS like so:\nA URL shown as is:\n    \\url{https://www.overleaf.com/}; \nOr some hyperlinked text:\n    \\href{https://www.overleaf.com/}{The Overleaf website}\nWhich would result in this block of text:\n\n\n\nSame text as before, with first URL shown, and second URL as linked text, both clickable.\n\n\n\n\nCommenting the source\nAs you are editing the source that defines what the documents will look like when compiled, it might be useful to include comments that are only visible when looking at the source. It is also useful when collaborating on a document, to make sure others can make sense of a syntax not commonly used.\nYou can add a comment to your source by starting a line with %.\nFor example, to let others know what the hyperref package is for, you could add at the top of the page:\n\\usepackage{hyperref} % for including hyperlinks\n\n\nGraphics\nTo add graphics to a document, the graphicx is needed, which you can add at the top of the document with:\n\\usepackage{graphicx}\nYou will then be able to add graphics with:\n\\includegraphics{example_figure.png}\nIn Overleaf, if you want to keep things tidy, you can upload images to a dedicated directory. However, it will be easier to then declare that directory as the default one for images, at the top of the document. For example, if all the images are stored in an “images” directory, we should add this to the top of the document:\n\\graphicspath{ {images/} }\nTry it now:\n\nFind and download a Public Domain featured picture from Wikimedia Commons\nUpload the picture to the “images” directory\nAdd the picture to the document\n\nThis is the simplest form of adding graphics to the document, but we often want to add other associated information (like a caption) and modify placement options.\nOverleaf makes it easy to add a code template for figures by adding the opening tag \\begin{figure} to the source code. You should automatically end up with the following:\n\\begin{figure}\n    \\centering\n    \\includegraphics[width=0.5\\linewidth]{}\n    \\caption{Caption}\n    \\label{fig:enter-label}\n\\end{figure}\nThis allows us to specify:\n\nThe horizontal position of the figure\nThe width of the figure\nThe name of the file\nThe figure caption\nThe label that will allow cross-referencing of the figure\n\nTry to move your previous picture to this new block of code, adding a caption and a label.\nOne important option that is missing from this template is the placement specifier. It comes straight after the \\begin{figure} tag, in between square brackets. If it is not specified, the figure will default to being placed at the top of the page. If you want to place the figure where the code is located, you can use the h position, which stands for “here”. Other placement options can be found in the .\nAbout the placement of the figure: an important detail is that the (lowercase) “h” option place the picture where it the code is (for example when page breaks move a picture to the next page). To really force the location of a figure relatively to the text, you can use the (uppercase) “H” option, which requires the extra float package (i.e. you need to add \\usepackage{float} to your preamble).\nAnother important option is the **width of the picture}. If a picture is too large, it might not be displayed at all, simply giving you a warning along the lines of “float too large”. The width of a figure can be specified with the `\\includegraphics+ tag.\nAs an example, the following code:\n\\begin{figure}[H]\n    \\centering\n    \\includegraphics[width=0.8\\linewidth]{J_D_Story_1966.jpg}\n    \\caption{J D Story Administration building, UQ St Lucia (1966, Public Domain)}\n    \\label{fig:JD}\n\\end{figure}\nResults in the following figure:\n\n\n\nFigure with caption below\n\n\nAlternatively, you can insert a figure by using the “Insert Figure” button in the toolbar, or by copy-pasting it into the document. This opens a dialog that allows to choose the file’s name and location, specify the width and turn caption and label on or off.\nBecause a label was used, we can now reference the figure itself and the page it is on:\n    \\ref{fig:JD}     % reference the figure\n    \\pageref{fig:JD} % reference the page the figure is on\nOverleaf will suggest figure labels when using these two tags, so you can autocomplete your code.\nThe following code:\nSee \\ref{fig:JD} on page \\pageref{fig:JD}.\nResults in the following text:\n\n\n\nIn-text referencing: figure and page numbers are hyperlinks.\n\n\n\n\nWarnings\nAs you write the source code of your document, and compile it regularly, you might see warnings pop up in the margin of the source panel. By hovering the pointer over the icon, you should see extra information.\n\n\n\nA warning message about an “Overfull hbox”, revealed by hovering over the warning icon in the source’s margin.\n\n\nIt is very common to see warnings about “overfull and underfull boxes” (“hbox” or “vbox”). These warnings are issued by the TeX engine the compile the document, and can usually be safely ignored. However, they might sometime help you spot undesirable typesetting, (e.g. some text overflowing to one side). Take note of them, but often won’t require you to take any action. If you want to learn more about these warnings, see the Overleaf documentation about overfull and underfull warnings.\nAll errors and warning are listed in the log view, which you can access with the “Logs and output files” button next to the “Recompile” button.\n\n\nEquations\nA lot of LaTeX’s power lies in its mathematical equation typesetting. For example, the following code:\n\\begin{equation} \\label{eq:euler}\ne^{\\pi i} + 1 = 0\n\\end{equation}\n\nThe beautiful equation \\ref{eq:euler} is known as Euler's\nidentity.\nWill produce this output:\n\n\n\nEquation rendered centred on the page, tagged as “(1)” and referenced in the paragraph below: “The beautiful equation 1 is known as Euler’s identity.\n\n\nNotice the use of labels for cross-reference, just like we used for figures.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#collaborating",
    "href": "LaTeX/overleaf_latex.html#collaborating",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "Collaborating",
    "text": "Collaborating\nYou can share your document with the “Share” button at the top right of your screen, using your collaborators’ email.\nIt is possible to give different rights to different collaborators: you could decide that one person can edit the document, whereas another one can only see it.\n\nReview and track changes\nThe “Review” button at the top of the screen will open a side panel showing the comments. Select some text and add a comment to the document you are collaborating on. Others can then reply to your comment.\nThis panel is also where tracked changes can be turned on. Changes will be shown in the same side panel as the comments. Collaborators can then accept or reject changes.\n\n\nChat\nFinally, a chat feature is available in Overleaf. You can open it with the top-right “Chat” button.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#history",
    "href": "LaTeX/overleaf_latex.html#history",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "History",
    "text": "History\nYou can see a history of versions by clicking the “History” button in the top right.\nSpecific versions can also be labelled, which might be useful to help navigate a complex history of versions, and direct collaborators to where major changes have happened.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#export-and-publish",
    "href": "LaTeX/overleaf_latex.html#export-and-publish",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "Export and publish",
    "text": "Export and publish\nTo export the document to PDF, click on the “Download PDF” button above the compiled view panel.\nOverleaf integrates some guidance to publish on a selection of platforms. You can see the publishers available by clicking on the “Submit” button in the top toolbar.\nTo make sure you can keep working on your document outside of Overleaf, or to submit the source files to a publisher, you will have to download the whole project, not just the compiled PDF. To do that, open the main menu (top right “Menu” button) and click on “Source”. You will then be able to download a zip archive containing all your .tex files, images, references files and others.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#templates",
    "href": "LaTeX/overleaf_latex.html#templates",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "Templates",
    "text": "Templates\nOverleaf provides many templates to choose from, which makes it easier to find a document style that looks right from the beginning and not spend too much time on finding the right packages and changing parameters in the preamble.\n\nGo to the main Overleaf page (up arrow button in the top toolbar)\nClick on “New Project”\nChoose from either:\n\nInstitution templates\nOne of the main templates listed\nThe full list of templates\n\nIn the template’s description page, click “Open as template”\n\nSee for example the Beamer template offered in the UQ templates (Beamer documents are used for creating presentation slides). This particular template also demonstrates the use of a LaTeX plotting package called pgfplots, which you can learn more about on Overleaf’s pgfplots documentation.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#account-integrations",
    "href": "LaTeX/overleaf_latex.html#account-integrations",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "Account integrations",
    "text": "Account integrations\nYour Overleaf account can be integrated with a variety of other services and platforms.\n\nGit and GitHub\nGit is a version control tool widely used to record a project’s history, notably for software development.\nFrom a project, using the top-right Menu button, it is possible to:\n\nClone the project repository to your own computer (and use Overleaf as a remote repository)\nLink the project to a GitHub repository\n\n\n\nORCiD\nORCiD is a unique identifier for researchers. Linking your Overleaf account to your ORCiD will automatically use it when submitting an article to publishers.\nThis option can be found in the account settings.\n\n\nReference managers\nOverleaf integrates Zotero and Mendeley to easily import references into projects.\nThis option can be found in the account settings. You will then be able to use the “From Zotero” or “From Mendeley” options in the “Upload” dialog to import your library as a .bib file.\nOnce a .bib file is available in the project, it is possible to use the biblatex package to cite references and create a bibliography. For example, adding this to the preamble:\n\\usepackage{biblatex}\n\\addbibresource{references.bib}\nAnd then referencing in the document:\nThis section is sourced \\cite{aguiar_invasion_2013}.\nThis other section as well \\cite{akkemik_archaeology_2012}.\n\\section{Bibliography}\n\\printbibliography\nIf you use EndNote, you should be able to export your library to a .bib file by using the BibTeX export style, and then upload it to your Overleaf project.\nMany options are available to customise your citations and bibliography. To learn more, see the Overleaf documentation on bibliographies.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#accessibility",
    "href": "LaTeX/overleaf_latex.html#accessibility",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "Accessibility",
    "text": "Accessibility\nUnfortunately, it is not currently easy to create fully accessible PDFs using LaTeX (i.e. tagged PDF, or following full PDF/UA specifications). Some tools do exist to make the output more universally accessible, including a tool called make4ht to create HTML instead of PDF, and a package called tagpdf to explore the possibility of creating tagged PDFs. However, they are still very experimental, and only trying to patch a deeper issue in how PDFs are structured.\nWhile there is work underway to integrate accessibility features in the underlying software, we are still a long way away from having access to these improvements on Overleaf, and the best that can be done for users is to style and structure our documents for improved accessibility. Here are some recommendations to keep in mind from the onset when working on any kind of document:\n\nMake sure all figures and tables are captioned, and that they are sufficiently described\nDo not include tables as pictures\nAvoid using text annotations, labels and titles inside pictures, and move that information to the main text\nIf pictures contain text, it might be a better option to use vector graphics (like the SVG format) rather than raster graphics (PNG, JPG, etc.)\nCheck the colours used throughout the document to make sure the contrast is sufficient, and the palettes are accessible to colourblind readers\nGive text descriptions to hyperlinks instead of only displaying the URL\nKeep the structure of the document simple\n\nIf you are interested in the topic of accessibility in LaTeX, here are some resources (many of which are rather technical):\n\nLaTeX Accessibility Working Group\nOverleaf’s technical paper on the challenges of PDF accessibility\nCurrent state and planned actions for accessible PDFs in LaTeX\nLaTeX Project publications on the topic of PDF accessibility",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#further-resources",
    "href": "LaTeX/overleaf_latex.html#further-resources",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "Further resources",
    "text": "Further resources\n\nLearn LaTeX in 30 minutes, by Overleaf\nOverleaf Webinars\nThe full, comprehensive Overleaf documentation\nLyX is an Open Source desktop LaTeX editor, if you want to use an offline alternative on your own computer.\nUQ LaTeX templates, including the Thesis template\nA Quick Guide to LaTeX: a handy cheatsheet with common commands\nTeX StackExchange: questions and answers about LaTeX",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "LaTeX/overleaf_latex.html#licence",
    "href": "LaTeX/overleaf_latex.html#licence",
    "title": "Overleaf: write and collaborate on \\(\\LaTeX\\) documents",
    "section": "Licence",
    "text": "Licence\nThis document and its source are released under the Creative Commons licence “Attribution 4.0 International” (CC BY 4.0). For the detailed licence text, please see the CC BY 4.0 page on the Creative Commons website.",
    "crumbs": [
      "Home",
      "![](/images/overleaf.svg){width=20} $\\LaTeX$ with Overleaf"
    ]
  },
  {
    "objectID": "Git/GitHub/GitHub_intro.html",
    "href": "Git/GitHub/GitHub_intro.html",
    "title": "GitHub Basics + Creating Analytics Projects Portfolios",
    "section": "",
    "text": "We will use Git through GitHub Desktop, you will need to create a GitHub Account and install GitHub Desktop.\nAccount creation instructions are available on this page.\nInstallation instructions are available on this page.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "GitHub and portfolios"
    ]
  },
  {
    "objectID": "Git/GitHub/GitHub_intro.html#setup",
    "href": "Git/GitHub/GitHub_intro.html#setup",
    "title": "GitHub Basics + Creating Analytics Projects Portfolios",
    "section": "",
    "text": "We will use Git through GitHub Desktop, you will need to create a GitHub Account and install GitHub Desktop.\nAccount creation instructions are available on this page.\nInstallation instructions are available on this page.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "GitHub and portfolios"
    ]
  },
  {
    "objectID": "Git/GitHub/GitHub_intro.html#material",
    "href": "Git/GitHub/GitHub_intro.html#material",
    "title": "GitHub Basics + Creating Analytics Projects Portfolios",
    "section": "Material",
    "text": "Material\n\nWhat is Git?\nIf you need to collaborate on a project, a script, some code or a document, there are a few ways to operate. Sending a file back and forth and taking turns is not efficient; a cloud-based office suite requires a connection to the Internet and doesn’t usually keep a clean record of contributions.\nVersion control allows users to:\n\nrecord a clean history of changes;\nkeep track of who did what;\ngo back to previous versions;\nwork offline; and\nresolve potential conflicts.\n\nProgrammers use version control systems to collaborativelly write code all the time, but it isn’t just for software: books, papers, small data sets, and anything that changes over time or needs to be shared can be stored in a version control system.\nA version control system is a tool that keeps track of changes for us, effectively creating different versions of our files. It allows us to decide which changes will be made to the next version (each record of these changes is called a commit), and keeps useful metadata about them. The complete history of commits for a particular project and their metadata make up a repository. Repositories can be kept in sync across different computers, facilitating collaboration among different people.\n\n\nWhat is GitHub?\nGitHub is a Git host server which stores your Git repository online. This means that you can easily use this platform to share your work, find the work of others, and collaborate. There are many other Git host servers such as GitLab and BitBucket.\n\n\nNavigating GitHub Online\n\nHow to explore GitHub\nGitHub can be quite intimidating the first time you come across it. Let’s break things down a bit.\nYou can search for repositories in a search engine, or on https://github.com/\nFor example, if you search for spotify artists analysis on GitHub you can see many projects relating to spotify.\nThe highest and most popular link listed is generally the one that you’re after.\nYou should find your way to this repository: https://github.com/khanhnamle1994/spotify-artists-analysis\nClick the link to have a look.\n\n\nRead the Readme\nA first look at a Github repository can be intimidating, but you should initially ignore the folder structure you see, and scroll down to the Readme section where you can usually find details surrounding what the project is, how to install/use it, and get more help.\n\n\nAbout\nThe about section on the right gives a brief overview of the repository. Below that it has links to:\n\nThe Readme\nThe License type (it’s important to give your code a license so others know what they can do with it, here’s a helpful resource for choosing the right one).\nPopularity (those who like this repo)\nWatching (those want to be aware of changes)\nForks (those who have created their own spinoffs)\n\n\n\nHistory\nNear the top of the repository you will see a clock icon followed by some numbers. This is the History of edits to this repository. You can click on this an see the entire history of changes made to this code. \n\n\nWhere the code is\nOnce you go back to the folder structure you can dig in to find the code that makes up this repository. If you navigate to Data-Processing.R you can see the code that underpins the data processing process.\n\n\nCloning code\nTo clone, or make a copy, of the code, you can simply click the **Code** button on the main repository screen, and import it into GitHub Desktop, allowing you to edit and make your own alterations to, and versions of the code before you.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "GitHub and portfolios"
    ]
  },
  {
    "objectID": "Git/GitHub/GitHub_intro.html#creating-a-repository",
    "href": "Git/GitHub/GitHub_intro.html#creating-a-repository",
    "title": "GitHub Basics + Creating Analytics Projects Portfolios",
    "section": "Creating a Repository",
    "text": "Creating a Repository\nOnce you have created a GitHub account, and have GitHub Desktop installed, open GitHub Desktop.\n\n\n\nimage\n\n\nWe have a few options here:\n\nYou can clone (make a copy of) a repository that you (or someone else) has previously created on the internet\nYou can create a repository within an already existing folder for a project you’re already working on\nYou can create a new folder and repository when you begin your project\n\nLet’s create a new folder for our project today by doing the following:\n\nClick + Create a New Repository on your hard drive... or go to File &gt; New Repository... or press Ctrl + N\nIn the Create a new repository window fill in the details for your repository:\n\n\nName: Portfolio\nDescription: A portfolio of my coding projects\nLocal path: In this case we’re going to create a new folder in our Documents click Choose... and select Documents - GitHub will create a new folder here for our project to be stored in. (generally you want to save this in the project folder you will be working from going forward)\nTick the Initialize this repository with a README box\nLeave Git ignore and License as None for now.\n\nNow that we’ve created our repository, let’s populate it with some files.\nOpen RStudio create a new R script file File &gt; New File &gt; R Script\nAdd a line of code to get started:\n# a basic R comment \nprint(\"Hello World\")\nSave your script File &gt; Save As... navigate to your portfolio folder and save your file as process\nIf you navigate back to GitHub, you will see that it has already identified that there is a new change in our repository. &gt; They appear green to demonstate that these changes have added something new, as opposed to deleting something. We can now begin the process of commiting and backing up these changes.\n\nTracking Changes\nInstead of viewing your version history as a series of documents with different changes (e.g. thesis_final.pdf, thesis_final_1.pdf, etc.), Git views your document as a compilation, or a stack, of different changes through time. This means that you can go back in time to view each change as it was commited to the main document.\nWhen we commit a change, it writes that change to the branch of our repository that we choose to, in this case, that is called main. This means that if you want to do some experimenting with your code, you can instead commit it to an experimental branch so that you don’t break your main code while you play. If you want more info about Branching, I have provided a link at the bottom of this document.\n\nBefore commiting your changes, it’s best practice to briefly describe your changes.\n\n\nDescribe your changes\nType a brief description into the box in the bottom right which currently reads Update process.R.\n\nType something along the lines of “Added a comment and line of code”. You can provide more detail in the Description box below if necessary.\n\n\nComments describing our commits are a very important part of Git, as they allow us to quickly visual changes at a glance.\n\n\n\nCommitting Changes\nNow we can commit our changes click Commit to **main** (this is where you could commit it to a different branch if it were an experimental change, but for today we’re just committing to main).\nWe can go to the History tab to look at this saved commit.\nThis has committed the version history to our local device, but if we want to share this with others, or store it online, we will need to Publish/Push the file to GitHub.\n\n\nPushing Changes\nTo do this we will first need to sign in to our GitHub.com account:\n\nNavigate to File &gt; Options... . Accounts &gt; Sign in &gt; Continue with browser\nSign in to GitHub in your browser\nIt will then take you back to GitHub Desktop with you signed in (you may need to click to allow this in your browser)\n\nYou can publish your repository in any one of three different ways:\n\nclicking the blue Publish Repository button\ngoing to Repository &gt; Push\nor pressing Ctrl + p\n\nYou can choose to keep your code private for now, and simply make it public later on. Click Publish Repository.\nYou will see that the Publish Repository button has changed to Fetch origin. Origin refers to the online repository where you code is kept, you should click this before starting to work on code, just in case changes have been made elsewhere that you need to pull to the device you’re working on.\nLet’s have a look at what it looks like when we edit a file.\nGo to R and edit our previous line to: print(\"Hello GitHub\")\nIf you return to GitHub, you will see the former text in red, and our new addition highlighted in green.\nBefore we can push this to GitHub, we need to:\n\nDescribe our edits “changed the print code”\nThen click Commit to **main**\nThen press Ctrl + P to push it to GitHub.\n\n\n\n\nAccessing and intereacting with your Git Repository Online\n\nView and edit online\nIf you click your profile picture in the top right of Github online, you can view all Your repositories.\nFrom here you can access your portfolio repository, and then the process.R file we pushed earlier.\nOnce you’ve navigated to the file, you can click the pencil button to edit the code online.\n\nMake some edits to the code\nProvide a description of your changes in the Commit changes section (these descriptions may feel unnecessary, but generally when you make changes, you’re changing a number of things, and this can be very very helpful).\nThen click the Commit changes button.\n\n\nOn the desktop you would need to push to the repository, as we are already in the external repository, you don’t need to do that, however next time you access the file in another location, you should Pull the data.\n\n\n\nFetch/Pull\nBefore doing any edits on a file in a repository that is worked on by many individuals or on different devices, it’s best to Fetch/Pull your repository. You can do this in Github Desktop by clicking Pull origin up the top. This will pull the changes from the online repository, and let you know if there are any new additons, deletions, or, if you have simultaneously changed the same section of code, any conflicts (which need to be fixed and resolved before continuing).\nWe can see the creation, updates, and commits on the left side of Github Desktop.\n\n\nConflicts\nIf we simultaneously make edits online, and then make edits on desktop without Fetching/Pulling first, we will have conflicts. Let’s create a conflict.\n\nEdit the document online, add # Edited online, then Commit\nEdit the document on desktop, add # Edited on Desktop, save the file, then Commit\nClick Push\n\n\nYou will receive an error that there are newer commits on the remote. Before we Push, we will need to Fetch\n\n\nClick Fetch\nClick Pull Origin\n\n\nA popup will appear asking you to resolve the conflicts in the document before you merge. We need to open the file to view and remove those conflicts.\n\n\nOpen the file with the conflict. You will see code similar to that below:\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n# Edited on Desktop\n=======\n# Edited Online\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 7e2e0c3b4a819b3cf3c285a47a862f975629bf8a\n\nHere we decide what we want to keep, and what we want to delete. Once we have decided what to keep, we need to remove the unwanted code, as well as the tags that Git has inserted. You will want to make sure the code conflicts are resolved, and that the code works. In this case, let’s keep both edits, and simply remove the tags that Git added. Your code should look like this:\n\nprint(\"Hello Github\")\n# I added this code on GitHub Online \n# Edited on Desktop\n# Edited Online\n\nSave your file.\nReturn to GitHub Desktop, and it should now update to show that there are no conflicts remaining. Click Continue merge\nThe conflicts are now resolved, and you can once again Push our edits to our GitHub repository.\n\nWe can now have a look at the history tab to see the history we’re starting to create.\n\n\nDelete\nLet’s navigate to the file location, and then delete the file so that we can see the process behind this. Once you delete the file, Github Desktop will show this as a Change where all lines of code were removed. We can now add comments, commit this change, and then push it to the repository.\n\n\nRevert\nEven though we have deleted the file locally, and pushed that deletion to the web based repository, we can revert that change within Github Desktop (you can also do this through the Git command line, but note that you cannot do this on Github online.\nWithin Github Desktop click the History tab, right click on the commit where the file was deleted, and select Revert changes in commit. This will bring our file back.\n\n\nGitignore\nBefore you push to a git repository, it’s worth making sure that you want to commit everything in that repository. For example, there may be private documents, or large datafiles you do not want to commit and upload. You can set these to be ignored with Gitignore.\nLet’s create some documents we want GitHub to ignore.\n\nNavigate to your Portfolio folder\nRight-click inside the folder and select New &gt; Text Document, and call it example.txt\n\nNow let’s have GitHub ignore this file. You can do this in Github Desktop by going to Repository &gt; Repository Settings... &gt; Ignored files and then entering the locations, names, or types of files you want to ignore. Let’s enter example.txt.\n\nYou can also cover all instances of a particular document type by entering something similar to *.dat (the asterix will mean that any file of that type will be ignored)",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "GitHub and portfolios"
    ]
  },
  {
    "objectID": "Git/GitHub/GitHub_intro.html#creating-a-good-readme-for-your-portfolio",
    "href": "Git/GitHub/GitHub_intro.html#creating-a-good-readme-for-your-portfolio",
    "title": "GitHub Basics + Creating Analytics Projects Portfolios",
    "section": "Creating a good Readme for your Portfolio",
    "text": "Creating a good Readme for your Portfolio\nThe Readme is often the first place people go when looking at a Git Repository, so it’s important to have useful information here, and displayed in a meaningful way. This is especially the case for your Portfolio.\n\nCreating a Portfolio\nYou can use GitHub to create and display your own work. This good example has their details, their achievements, and links to all their major projects: https://github.com/archd3sai/Portfolio\n\n\nMarkdown\nWhen editing a Readme file you can format it using a simple coding language called Markdown, as well as HTML coding.\nToday we’re going to finish by creating a simple Readme template.\n\nNavigate to your GitHub repository online.\nClick the pencil icon to edit your README.md\nCreate a heading by entering\n\n# Analytics Portfolio - &lt;your name&gt;\n\nYou can create headings using # symbols, # will create the largest heading, and ###### will create the smallest\n\n\nOn the next line, enter some normal text describing your portfolio\n\nThis **Portfolio** contains all of my Analytics projects\n\nYou can Bold text by putting it between a set of double asterisks, eg. *example text* becomes example text.\n\n\nCreate a dot point with a link to your contact details by entering\n\n- **Email**: [yourname@email.com](yourname@email.com)\n\nA - symbol will give you a bullet point. You can add links, including emails, by wrapping the link text in square brackets [ ], and then putting the link address in parentheses ( ).\n\n\nAdd a second level heading by entering\n\n## Projects\n\nInsert an image by entering the following HTML code:\n\n&lt;img align=\"left\" width=\"250\" height=\"150\" src=\"https://user-images.githubusercontent.com/67612228/184837530-9a4537b3-22f0-495c-90d1-6ccdcb4bc4bd.png\"&gt;\n\nScroll to the bottom and descibe your commit, and click **Commit** changes\n\nThere is a lot more customisation you can do, and you can find a complete breakdown of Git Markdown here.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "GitHub and portfolios"
    ]
  },
  {
    "objectID": "Git/GitHub/GitHub_intro.html#things-not-covered-today",
    "href": "Git/GitHub/GitHub_intro.html#things-not-covered-today",
    "title": "GitHub Basics + Creating Analytics Projects Portfolios",
    "section": "Things not covered today",
    "text": "Things not covered today\n\nBranching (creating experimental branches of your code)\nFulling cloning others’ code (this can vary from straightforward to complex)\nCollaboration\nMaking your Repo Public",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "GitHub and portfolios"
    ]
  },
  {
    "objectID": "Git/GitHub/GitHub_intro.html#more-guides",
    "href": "Git/GitHub/GitHub_intro.html#more-guides",
    "title": "GitHub Basics + Creating Analytics Projects Portfolios",
    "section": "More Guides",
    "text": "More Guides\n\nTerminal/GitBash\nGit in R\nCreating a website with Git and R",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "GitHub and portfolios"
    ]
  },
  {
    "objectID": "Git/GitHub/GitHub_intro.html#uq-user-groups-support-and-training",
    "href": "Git/GitHub/GitHub_intro.html#uq-user-groups-support-and-training",
    "title": "GitHub Basics + Creating Analytics Projects Portfolios",
    "section": "UQ User Groups, Support, and Training",
    "text": "UQ User Groups, Support, and Training\n\nUQ R User Group (UQ RUG)\nR Ladies\nUQ Library Training\nRCC’s Hacky Hour",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "GitHub and portfolios"
    ]
  },
  {
    "objectID": "Git/GitHub/GitHub_intro.html#legal",
    "href": "Git/GitHub/GitHub_intro.html#legal",
    "title": "GitHub Basics + Creating Analytics Projects Portfolios",
    "section": "Legal",
    "text": "Legal\nThis course borrows information from the Git Bash course Git version control for collaboration which is based on the longer course Version Control with Git developped by the non-profit organisation The Carpentries. The original material is licensed under a Creative Commons Attribution license (CC-BY 4.0), and this modified version uses the same license. You are therefore free to:\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material\n\n… as long as you give attribution, i.e. you give appropriate credit to the original author, and link to the license.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "GitHub and portfolios"
    ]
  },
  {
    "objectID": "Excel/excel.html",
    "href": "Excel/excel.html",
    "title": "Microsoft Excel",
    "section": "",
    "text": "Find below the resources for our Excel sessions.",
    "crumbs": [
      "Home",
      "![](/images/excel.svg){width=20} Excel"
    ]
  },
  {
    "objectID": "Excel/excel.html#introduction",
    "href": "Excel/excel.html#introduction",
    "title": "Microsoft Excel",
    "section": "Introduction",
    "text": "Introduction\n\nManual\nRecording",
    "crumbs": [
      "Home",
      "![](/images/excel.svg){width=20} Excel"
    ]
  },
  {
    "objectID": "Excel/excel.html#processing-data",
    "href": "Excel/excel.html#processing-data",
    "title": "Microsoft Excel",
    "section": "Processing data",
    "text": "Processing data\n\nManual\nWorkbook",
    "crumbs": [
      "Home",
      "![](/images/excel.svg){width=20} Excel"
    ]
  },
  {
    "objectID": "Excel/excel.html#further-functions",
    "href": "Excel/excel.html#further-functions",
    "title": "Microsoft Excel",
    "section": "Further functions",
    "text": "Further functions\n\nManual\nWorkbook",
    "crumbs": [
      "Home",
      "![](/images/excel.svg){width=20} Excel"
    ]
  },
  {
    "objectID": "Excel/excel.html#charting",
    "href": "Excel/excel.html#charting",
    "title": "Microsoft Excel",
    "section": "Charting",
    "text": "Charting\n\nManual\nWorkbook",
    "crumbs": [
      "Home",
      "![](/images/excel.svg){width=20} Excel"
    ]
  },
  {
    "objectID": "Acrobat/acrobat.html",
    "href": "Acrobat/acrobat.html",
    "title": "Adobe Acrobat",
    "section": "",
    "text": "Acrobat is a PDF viewer and editor.\nFind below the resources for our Acrobat sessions.",
    "crumbs": [
      "Home",
      "![](/images/acrobat.svg){width=20} Acrobat"
    ]
  },
  {
    "objectID": "Acrobat/acrobat.html#adobe-acrobat-essentials",
    "href": "Acrobat/acrobat.html#adobe-acrobat-essentials",
    "title": "Adobe Acrobat",
    "section": "Adobe Acrobat essentials",
    "text": "Adobe Acrobat essentials\n\nManual\nFiles",
    "crumbs": [
      "Home",
      "![](/images/acrobat.svg){width=20} Acrobat"
    ]
  },
  {
    "objectID": "Audacity/audacity.html",
    "href": "Audacity/audacity.html",
    "title": "Audacity: introduction to audio editing",
    "section": "",
    "text": "Audacity is an Open Source, cross-platform tool for audio editing.\nAmong other features, Audacity allows you to:\nAudacity started in 1999 and is currently being used as a professional-grade audio editor in many applications, including music and podcast production.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#what-well-learn",
    "href": "Audacity/audacity.html#what-well-learn",
    "title": "Audacity: introduction to audio editing",
    "section": "What we’ll learn",
    "text": "What we’ll learn\nThis training covers how to:\n\nimport audio into Audacity\nmove and delete sections\nclean up background noise\nswitch between sound visualisation tools\nuse a few essential effects\nexport to a variety of formats\nfind and publish audio online",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#download-some-audio",
    "href": "Audacity/audacity.html#download-some-audio",
    "title": "Audacity: introduction to audio editing",
    "section": "Download some audio",
    "text": "Download some audio\nOur sample audio files come from the Internet Archive. You can download the archived file here, and extract it wherever you would like to store your Audacity project.\nThe pages that describe the recordings are here:\n\nCarlos Valderrama Herrera - Peruvian Triste / Pan-American Waltz (1920): https://archive.org/details/78_a-peruvian-triste-b-pan-american-waltz_carlos-valderrama_gbia0001886a\nLucille Hegamin - Everybody’s Blues (1920): https://archive.org/details/Lucille_Hegamin-Everybodys_Blues",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#import-an-audio-file",
    "href": "Audacity/audacity.html#import-an-audio-file",
    "title": "Audacity: introduction to audio editing",
    "section": "Import an audio file",
    "text": "Import an audio file\nAudacity supports a variety of audio formats. We deal with MP3 files today.\nTo import an audio file: File &gt; Open... and choose the first file.\nYou can also drag-and-drop an audio file into Audacity. Try that with the second file.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#interface",
    "href": "Audacity/audacity.html#interface",
    "title": "Audacity: introduction to audio editing",
    "section": "Interface",
    "text": "Interface\nWe are dealing with two stereo clips here. The top half of a clip is the left channel, and the bottom half is the right channel.\nThe louder the sound, the higher the waveform. The quieter, the closer to the middle point.\nThe dark blue area shows the waveform peak of grouped samples (the loudest part), whereas the light blue area shows the Root Mean Square value of grouped samples (an guide to how loud it sounds).",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#controls",
    "href": "Audacity/audacity.html#controls",
    "title": "Audacity: introduction to audio editing",
    "section": "Controls",
    "text": "Controls\nWe have two audio files. If we press play (green button, or spacebar), they will be played at the same time.\nWe can focus on the longer track by muting the song, with the “Mute” button. It is then obvious which track is muted, as it is displayed in grey.\nWe can use the mouse wheel to scroll vertically through our audio clips. To zoom and pan the audio clips efficiently, we need to learn about the keyboard keys we can associate with the mouse wheel:\n\nCtrl + Scroll: zoom in and out\nShift + Scroll: navigate the timeline horizontally\n\nThese controls allow us to quickly navigate the clips and find the precise spots we want to work on. Make sure you practise them! You will notice that it is a lot more comfortable to work with Audacity when using a mouse.\nIf you zoom close enough to a soundwave, you will see that sound can’t be stored digitally as actual waves. Each dot is a sample point. Depending on the bitrate of your audio file (i.e. the quality), the encoding will be more or less precise.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#clipping-and-normalising",
    "href": "Audacity/audacity.html#clipping-and-normalising",
    "title": "Audacity: introduction to audio editing",
    "section": "Clipping and normalising",
    "text": "Clipping and normalising\nYou might see a red line on the Carlos Valderrama track (if not, use View &gt; Show Clipping). This denotes clipping, which means the soundwave goes over an intensity limit, which might result in artefacts when exporting the project.\nTo resolve clipping, we can normalise the track, which is equivalent to reducing the amplitude of the soundwave, for it to fit within a range. Click on the grey area next to the track to select it, and use:\nEffect &gt; Normalize...\nThat should have reduced the amplitude, and removed the red bars.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#compressing",
    "href": "Audacity/audacity.html#compressing",
    "title": "Audacity: introduction to audio editing",
    "section": "Compressing",
    "text": "Compressing\nSometimes, normalising is not enough. If a recording has a few very loud peaks, but the rest is very quiet, it might be worth it playing with compression. Select the whole clip and use:\nEffect &gt; Compressor...\nWe can use the default settings and see the effect.\nThe compressor amplifies quiet parts differently to louder parts, according to the settings. This means that we can make quieter sections a lot louder, while only amplifying the peaks a tiny bit – or even lowering their level. It ends up evening out the volume of the whole recording, which can be particularly useful if the audio is to be played in a noisy environment.\nAs you can expect, the compressor has to be used carefully as it reduces the dynamic range of a recording, i.e. it can render a sound less interesting because of less variability in the amplitude. Some music genres, like classical music, need a lot of dynamic range, so classical recordings does not make much sense.\nYou might have to normalise again after compressing – and keep an eye on clipping!",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#recording-audio",
    "href": "Audacity/audacity.html#recording-audio",
    "title": "Audacity: introduction to audio editing",
    "section": "Recording audio",
    "text": "Recording audio\nIf you have a microphone, make sure the device used is the right one in the microphone dropdown, and click the red dot to start recording.\nWe can introduce the first song by talking about the artist.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#envelope-tool",
    "href": "Audacity/audacity.html#envelope-tool",
    "title": "Audacity: introduction to audio editing",
    "section": "Envelope tool",
    "text": "Envelope tool\nWhen we want to change the amplitude of one small part of a recording, we are better off using the envelope tool. You can add points on the light blue bar that appears around your tracks, and drag them to change the amplitude more smoothly than when using the Amplify effect on a section of audio.\nTry it on the speech we recorded, on a section where the audio is way louder than the rest (useful when someone laughs, or coughs, or hits the microphone…).",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#splitting-audio",
    "href": "Audacity/audacity.html#splitting-audio",
    "title": "Audacity: introduction to audio editing",
    "section": "Splitting audio",
    "text": "Splitting audio\nThe Carlos Valderrama track is actually two songs. We can use the select tool to highlight the second song, use Ctrl + X to cut it, click out of the track, and use Ctrl + V to paste it into its own separate track.\nLet’s mute it for now.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#noise-reduction",
    "href": "Audacity/audacity.html#noise-reduction",
    "title": "Audacity: introduction to audio editing",
    "section": "Noise reduction",
    "text": "Noise reduction\nAs you can hear, there is quite a lot of noise in the first music track. Luckily, the recording starts with a few seconds of only noise before the piano comes in.\nWe can use this section to create a noise profile, and then use the noise reduction effect to try and clean part of the noise.\n\nSelect the section at the beginning of the recording that is only noise (you can then use the spacebar to play only that section and check if you only selected noise)\nUse “Effect &gt; Noise Reduction…”\nClick “Get Noise Profile”\nSelect the whole track\nUse “Effect &gt; Noise Reduction…” again\nClick OK\n\nYou can play with the effect’s settings to optimise the noise reduction. As you can expect, you have to find the right balance between removing lots of noise and not degrading the interesting part of the audio.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#removing-audio",
    "href": "Audacity/audacity.html#removing-audio",
    "title": "Audacity: introduction to audio editing",
    "section": "Removing audio",
    "text": "Removing audio\nNow that we have cleaned up some of the noise, we can remove the first few seconds of the clip so we get straight away to the interesting bit.\nWith the selection tool, we can select the (roughly) 2 first seconds, and remove them with Del. We can do the same thing with the end of the recording.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#fading-in-and-out",
    "href": "Audacity/audacity.html#fading-in-and-out",
    "title": "Audacity: introduction to audio editing",
    "section": "Fading in and out",
    "text": "Fading in and out\nTo make our recording start smoothly, we can fade the sound in.\nStill using our selection tool, select the first two seconds of the clip, and use:\nEffect &gt; Fade In.\nWe can do the same with the end of the recording, but using the “Fade Out” effect instead.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#moving-audio",
    "href": "Audacity/audacity.html#moving-audio",
    "title": "Audacity: introduction to audio editing",
    "section": "Moving audio",
    "text": "Moving audio\nLet’s unmute the jingle and the speech recording. We want our podcast to start with the jingle, then introduce the first song, and have the first song start at the end of the speech recording. We can do that by changing to the Time Shift Tool and sliding the clip to the right position.\nLet’s make sure the clips fade into each other by using the fading effects. We also want to normalise the tracks and use the volume sliders so the sound level is more uniform.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#using-the-spectrogram",
    "href": "Audacity/audacity.html#using-the-spectrogram",
    "title": "Audacity: introduction to audio editing",
    "section": "Using the spectrogram",
    "text": "Using the spectrogram\nYou can switch between visualisation tools by using the clip title dropdown menu.\nThe spectrogram view makes it possible to visualise frequencies rather than the level, and can help finding specific sounds in the recording, and removing particular frequencies.\nFor example, if there is a constant high-pitched hissing noise throughout a recording, the spectral selection option will allow the selection of the range of frequencies, and the Spectral edit multitool effect will remove it.\nTry changing the view of the jingle from waveform to spectrogram, and then:\n\nSelect &gt; Spectral &gt; Toggle spectral selection will allow you to select a frequency range by drawing a rectangle in the spectrogram (see how it turns green?). There is a beeping throughout the recording, at 5 kHz.\nEffect &gt; Spectral edit multitool will then edit that frequency range out of the recording.\n\nOther use cases include finding tongue clicks or saliva sounds in speech, to then remove them.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#saving-and-exporting",
    "href": "Audacity/audacity.html#saving-and-exporting",
    "title": "Audacity: introduction to audio editing",
    "section": "Saving and exporting",
    "text": "Saving and exporting\nSaving an Audacity project will save everything in the current state, so you can keep editing your audio files and putting everything together.\nIf you are finished with your project and you want to export a finished product, you can use File &gt; Export. A few common formats are listed, including lossy formats like MP3 and OGG, and lossless formats like WAV. Using Export Audio..., you can choose other formats FLAC, a lossless format that uses less space than WAV.\nTry exporting you project and giving it relevant tags so information can be displayed by audio players. By default, Audacity will use the tags from the first clip imported into the project.\nThe resulting file is a single stereo audio file. You can also Mix and Render tracks inside a project, which is the equivalent.\nRemember that if a track is muted, it won’t be exported!",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#where-to-find-audio-files",
    "href": "Audacity/audacity.html#where-to-find-audio-files",
    "title": "Audacity: introduction to audio editing",
    "section": "Where to find audio files?",
    "text": "Where to find audio files?\nAlways make sure that the licence is compatible with your use of the files.\n\nThe Internet Archive is a great source of recordings: https://archive.org/details/audio\nFreeSound.org is a wealth of sound snippets released under Creative Commons licences\nSoundCloud can be searched with Creative Commons licence filters\nhttp://freemusicarchive.org/ offers thousands of songs released under a variety of licences\nWikimedia Commons can also be searched for audio files: https://commons.wikimedia.org/wiki/Category:Audio_files\nhttp://dig.ccmixter.org/ specialises in making Creative Commons music discoverable.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "Audacity/audacity.html#where-to-publish",
    "href": "Audacity/audacity.html#where-to-publish",
    "title": "Audacity: introduction to audio editing",
    "section": "Where to publish?",
    "text": "Where to publish?\nOnce more, make sure you have the right to release the recording publicly.\nThe choice of platform used to publish an audio file will depend on the audience and the use.\nThe Internet Archive and Wikimedia Commons are non-profit organisations that are generally considered to be reliable hosts of media. SoundCloud is very popular to share audio files, in particular because of its social features, but it is run by a for-profit entity that has been having difficulties to survive in the last few years.\nFreesound is an initiative of the Music Technology Group, a research group of the Universitat Pompeu Fabra (UPF) in Barcelona. Having a project like this one backed up by a university’s infrastructure is a good sign of sustainability.\nBoth the Internet Archive and SoundCloud will allow you to embed a sound into another webpage, which might be useful.",
    "crumbs": [
      "Home",
      "![](/images/audacity.svg){width=20} Audacity"
    ]
  },
  {
    "objectID": "GenAI/genai.html",
    "href": "GenAI/genai.html",
    "title": "Generative AI",
    "section": "",
    "text": "Generative AI (or “GenAI”) refers to AI tools that use generative models trained to create different outputs, like text, images or videos.\nFind below the resources for our GenAI sessions.",
    "crumbs": [
      "Home",
      "![](/images/ChatGPT_logo.svg){width=20} GenAI"
    ]
  },
  {
    "objectID": "GenAI/genai.html#genai-tools-overview-uses-and-nuances",
    "href": "GenAI/genai.html#genai-tools-overview-uses-and-nuances",
    "title": "Generative AI",
    "section": "GenAI Tools Overview: Uses and Nuances",
    "text": "GenAI Tools Overview: Uses and Nuances\n\nSlides",
    "crumbs": [
      "Home",
      "![](/images/ChatGPT_logo.svg){width=20} GenAI"
    ]
  },
  {
    "objectID": "Git/git.html",
    "href": "Git/git.html",
    "title": "Git version control for collaboration",
    "section": "",
    "text": "We will use Git inside a command-line shell called Bash.\nInstallation instructions are available on this page.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#installation",
    "href": "Git/git.html#installation",
    "title": "Git version control for collaboration",
    "section": "",
    "text": "We will use Git inside a command-line shell called Bash.\nInstallation instructions are available on this page.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#what-is-git",
    "href": "Git/git.html#what-is-git",
    "title": "Git version control for collaboration",
    "section": "What is Git?",
    "text": "What is Git?\nIf you need to collaborate on a project, a script, some code or a document, there are a few ways to operate. Sending a file back and forth and taking turns is not efficient; a cloud-based office suite requires a connection to the Internet and doesn’t usually keep a clean record of contributions.\nVersion control allows users to:\n\nrecord a clean history of changes;\nkeep track of who did what;\ngo back to previous versions;\nwork offline; and\nresolve potential conflicts.\n\nProgrammers use version control systems to collaborativelly write code all the time, but it isn’t just for software: books, papers, small data sets, and anything that changes over time or needs to be shared can be stored in a version control system.\nA version control system is a tool that keeps track of changes for us, effectively creating different versions of our files. It allows us to decide which changes will be made to the next version (each record of these changes is called a commit), and keeps useful metadata about them. The complete history of commits for a particular project and their metadata make up a repository. Repositories can be kept in sync across different computers, facilitating collaboration among different people.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#configuring-git",
    "href": "Git/git.html#configuring-git",
    "title": "Git version control for collaboration",
    "section": "Configuring Git",
    "text": "Configuring Git\nOn a command line, Git commands are written as git verb, where verb is what we actually want to do.\nBefore we use Git, we need to configure it with some defaults, like our credentials and our favourite text editor. For example:\ngit config --global user.name \"Vlad Dracula\"\ngit config --global user.email \"vlad@tran.sylvan.ia\"\nThis user name and email will be associated with your subsequent Git activity, which means that any changes pushed to GitLab, GitHub, BitBucket or another Git host server in the future will include this information. This has to match your GitLab credentials.\ngit config --global core.editor \"nano -w\"\ngit config --list\nYou can always find help about git with the --help flag:\ngit --help\ngit config --help",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#creating-a-repository",
    "href": "Git/git.html#creating-a-repository",
    "title": "Git version control for collaboration",
    "section": "Creating a repository",
    "text": "Creating a repository\nFirst, let’s make sure we’re in the right directory. We can check the directory using the pwd command, and then change directory using the cd command. On windows we can change to our default home directory like so:\ncd /c/Users/&lt;yourusername&gt;\nWe can using ls to get a list of everything that is in our current directory.\nNow, let’s create a directory for our project and move into it:\nmkdir planets\ncd planets\nThis is the same as creating a new folder.\nThen we tell Git to make planets a repository — a place where Git can store versions of our files:\ngit init\nUsing the ls command won’t show anything new, but adding the -a flag will show the hidden files and directories too:\nls -a\nGit created a hidden .git directory to store information about the project (i.e. everything inside the directory where the repository was initiated).\nNow that we’ve initialised the git repositry, we can start using commands to manage versions. We can now check the status of our project with:\ngit status",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#tracking-changes",
    "href": "Git/git.html#tracking-changes",
    "title": "Git version control for collaboration",
    "section": "Tracking changes",
    "text": "Tracking changes\n\nHow do we record changes and make notes about them?\n\nYou should still be in the planets directory, which you can check with the pwd command.\nLet’s create a new text file that contains some notes about the Red Planet’s suitability as a base. We’ll use the nano text editor:\nnano mars.txt\nType the following text into it:\nCold and dry, but everything is my favorite colour\nWrite out with Ctrl+O and exit nano with Ctrl+X.\nWe can now use ls to check that the file has been created.\nYou can also check the contents of your new file with the cat command:\ncat mars.txt\nNow, check the status of our project:\ngit status\nGit noticed there is a new file. The “Untracked files” message means that there’s a file in the directory that Git isn’t keeping track of. We can tell Git to track a file using git add:\ngit add mars.txt\nYou may get a note saying warning: LF will be replaced by CRLF in mars.txt. This is highlighting the difference in the way that Linux systems and Windows systems handle carriage returns. And this can be recorded as a change when you go between operating systems, but only if you change those lines, and there is now a lot more cross compatibility, so you can actually just safely ignore this.\nNow we can use git status again to see what happenned:\ngit status\nGit now knows that it’s supposed to keep track of mars.txt, but it hasn’t recorded these changes as a commit yet. To get it to do that, we need to run one more command:\ngit commit -m \"Start notes on Mars as a base\"\nWhen we run git commit, Git takes everything we have told it to save by using git add and stores a copy permanently inside the special .git directory. This permanent copy is called a commit (or revision) and it is given a short identifier.\nWe use the -m flag (for “message”) to record a short descriptive comment that will help us remember what was done and why.\nIf we run git status now:\ngit status\n… we can see that the working tree is clean.\nTo see the recent history, we can use git log:\ngit log\ngit log lists all commits made to a repository in reverse chronological order. The listing for each commit includes the commit’s full identifier (which starts with the same characters as the short identifier printed by the git commit command earlier), the commit’s author, when it was created, and the log message Git was given when the commit was created.\n\nNow, let’s add a line to our text file:\nnano mars.txt\nAfter writing out and saving, let’s check the status:\ngit status\nWe have changed this file, but we haven’t told Git we will want to save those changes (which we do with git add) nor have we saved them (which we do with git commit). So let’s do that now. It is good practice to always review our changes before saving them. We do this using git diff. This shows us the differences between the current state of the file and the most recently saved version:\ngit diff\nThere is a quite a bit of cryptic-looking information in there: it contains the command used to compare the files, the names and identifiers of the files, and finally the actual differences. The + sign indicates which line was added.\nIt is now time to commit it:\ngit commit -m \"&lt;your comment&gt;\"\nThat didn’t work, because we forgot to use git add first. Let’s fix that:\ngit add mars.txt\ngit commit -m \"&lt;your comment&gt;\"\nUsing git add allows us to select which changes are going to make it into a commit, and which ones won’t. It sends them to what is called the staging area. In a way, git add specifies what will go in a snapshot (putting things in the staging area), and git commit then actually takes the snapshot.\nChallenge 1\nThe staging area can hold changes from any number of files that you want to commit as a single snapshot.\n\nAdd some text to mars.txt noting your decision to consider Venus as a base\nCreate a new file venus.txt with your initial thoughts about Venus as a base for you and your friends\nAdd changes from both files to the staging area, and commit those changes as one single commit.\n\nAdding and committing multiple files:",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#exploring-history",
    "href": "Git/git.html#exploring-history",
    "title": "Git version control for collaboration",
    "section": "Exploring history",
    "text": "Exploring history\n\nHow can we identify old versions of files, review changes and recover old versions?\n\nAs we saw in the previous lesson, we can refer to commits by their identifiers. You can refer to the most recent commit of the working directory by using the identifier HEAD.\nLet’s add a line to our file:\nnano mars.txt\nWe can now check the difference with the head:\ngit diff HEAD mars.txt\nWhich is the same as using git diff mars.txt. What is useful is that we can refer to previous commits, for example for the commit before HEAD:\ngit diff HEAD~1 mars.txt\nSimilarly, git show can help us find out what was changed in a specific commit:\ngit show HEAD~2 mars.txt\nWe can also use the unique 7-character identifiers that were attributed to each commit:\ngit diff XXXXXXX mars.txt\n\nHow do we restore older versions of our file?\n\nOverwrite your whole text with one single new line:\nnano mars.txt\ngit diff\nWe can put things back the way they were by using git checkout:\ngit checkout HEAD mars.txt\ncat mars.txt\ngit checkout checks out (i.e., restores) an old version of a file. In this case, we’re telling Git that we want to recover the version of the file recorded in HEAD, which is the last saved commit. If we want to go back even further, we can use a commit identifier instead:\ngit log -3\ngit checkout XXXXXXX mars.txt\ncat mars.txt\ngit status\nNotice that the changes are on the staged area. Again, we can put things back the way they were by using git checkout:\ngit checkout HEAD mars.txt\ncat mars.txt\nChallenge 2\nJennifer has made changes to the Python script that she has been working on for weeks, and the modifications she made this morning “broke” the script and it no longer runs. She has spent more than an hour trying to fix it, with no luck…\nLuckily, she has been keeping track of her project’s versions using Git! Which commands below will let her recover the last committed version of her Python script called data_cruncher.py?\n\ngit checkout HEAD\ngit checkout HEAD data_cruncher.py\ngit checkout HEAD~1 data_cruncher.py\ngit checkout &lt;unique ID of last commit&gt; data_cruncher.py\nBoth 2 and 4\n\nCheckout summary: \n\nRecap\n\ngit config: configure git\ngit init: initialise a git repository here\ngit status: see information about current state of the repository\ngit add: add a change from a file (or several) to the staging area\ngit commit -m \"...\": commit a change (or several) to our history\ngit log: see history\ngit show: show changes in one commit for one file\ngit checkout: roll back to previous version\ngit diff: difference between file on disk and commit in repository",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#ignoring-things",
    "href": "Git/git.html#ignoring-things",
    "title": "Git version control for collaboration",
    "section": "Ignoring things",
    "text": "Ignoring things\n\nHow can I tell git to ignore things?\n\nSometimes, we don’t want git to track files like automatic backup files or intermediate files created during an analysis.\nSay you create a bunch of .dat files like so:\ntouch a.dat b.dat c.dat\ngit status\nIf you don’t want to track them, create a .gitignore file:\nnano .gitignore\n… and add the following line to it:\n*.dat\nThat will make sure no file finishing with .dat will be tracked by git.\ngit status\ngit add .gitignore\ngit commit -m \"Ignore data files\"\ngit status",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#remotes",
    "href": "Git/git.html#remotes",
    "title": "Git version control for collaboration",
    "section": "Remotes",
    "text": "Remotes\nIf you haven’t already, now is the time to create a GitHub account. In our class, I’d ask you to share your username, so that we can collaborate later.\nGit is the software. GitHub is a platform to allow you to host the repository and share it with others. There are others such as GitLab, BitBucket, GitTea, and GitBucket.\n\nHow do I share my changes with others on the web?\n\nVersion control really becomes extra useful when we begin to collaborate with other people. We already have most of the machinery we need to do this; the only thing missing is to copy changes from one repository to another.\nIt is easiest to use one copy as a central hub, stored online.\nLet’s look at GitHub: https://github.com\nLet’s share our repository with the world. Log into GitHub and create a new repository called planets (“+ &gt; New repository” in the top toolbar). Make sure you select “Public” for the visibility level.\nOur local repository (on our computer) contains our recent work, but the remote repository on GitHub’s servers doesn’t.\nWe now need to connect the two: we do this by making the GitHub repository a remote for the local repository. The home page of the repository on GitHub includes the URL we need to identify it, under “…or push an existing repository from the command line”. Copy it to your clipboard, and in your local repository, run the following command (note that you will need to right click when using the Shell):\ngit remote add origin https://github.com/&lt;your_username&gt;/planets.git\nThe name origin is a local nickname for your remote repository. We could use something else if we wanted to, but origin is by far the most common choice.\nGitHub wants to make sure that we’re using the same name for our main branch as they do on GitHub. You will note that our branch is currently called master, we can change the branch name to main with the next line of code:\ngit branch -M main\nNow, we can push our changes from our local repository to the remote on GitHub. Try this:\ngit push\nGit does not know where it should push by default. See the suggested command in the error message? We can set the default remote with a shorter version of that:\ngit push -u origin main\nWe only need to do that once: from now one, Git will know that the default is the origin remote and the main branch.\nIt may request your credentials, which used to simply be your username and password, however, GitHub now requires you to create a Personal Access Token. 1. Click on your avatar in the top right of GitHub.com 1. Click settings 1. Scroll to the bottom and on the left, and click Developer settings. 1. Click Personal access tokens (either type is fine) 1. Click Generate new token (either is fine, classic is simpler) 1. You may need to authenticate yourself using TFA (you can also choose to use your password) 1. You can select what you need to be able to edit. If you’ve chosen classic, you can skip this and scroll to the bottom to Generate token. 1. Make sure you copy this token immediately and save it somewhere so you can reuse it.\nYou can now see on GitHub that your changes were pushed to the remote repository.\nYou can edit files directly on GitHub if you want. Try editing your READ.me by clicking on “Edit”.\nIf you do that, you will then need to pull changes from the remote repository to your local one before further editing:\ngit log\ngit pull\ngit log\nls\nIn summary: git push sends commited changes to a remote repository, whereas git pull gets commited changes from the remote to your local repository.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#collaborating",
    "href": "Git/git.html#collaborating",
    "title": "Git version control for collaboration",
    "section": "Collaborating",
    "text": "Collaborating\n\nHow do we use version control to collaborate?\n\nNow, let’s get into pairs: one person is the “Owner”, the other is the “Collaborator”.\nFirst, the Owner needs to give the collaborator editing access to the repository. Go to the Settings tab in your GitHub repository. On the left panel, you can click Collaborators and then Add people. Here you can enter usernames and email addresses.\nThe Collaborator can then accept the invitation.\nNext, the Collaborator needs to download a copy of the Owner’s repository to their machine, which is called “cloning a repository”. To do that, first make sure you move out of your personal repository:\ncd ..\nNow, you can clone the Owner’s repository (you can do this by clicking the green Code button on a repository), and you can give it a recognisable local name:\ngit clone https://github.com/&lt;owner_username&gt;/planets.git partner-planets\nThe Collaborator can now make changes in their clone of the Owner’s repository:\ncd partner-planets\nnano README.md\nAdd a section for your collaborators.\n## Contributors\n\nName\nName2\nThen add, commit, and push the change to the Owner’s repository on GitLab:\ngit add README.md\ngit commit -m \"added collaborators\"\ngit push\nWe didn’t have to create a remote called origin, or set the default upstream: that was done by default by Git when cloning the repository.\nYou can see that the changes are now live on GitHubb.\nThe Owner can now download the Collaborator’s changes from GitHub:\ngit pull origin main\nIf you collaborate on a remote repository, remember to pull before working!\nChallenge 3\nSwitch roles and repeat the process!\nChallenge 4\nUse the GitLab interface to add a comment to your partner’s commit and suggest something. See your notifications in “Activity” afterwards.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#conflicts",
    "href": "Git/git.html#conflicts",
    "title": "Git version control for collaboration",
    "section": "Conflicts",
    "text": "Conflicts\n\nWhat do I do when changes conflict with someone else’s?\n\nAs soon as people can work in parallel, they’ll likely step on each other’s toes. This will even happen with a single person: if we are working on a piece of software on both our laptop and a server in the lab, we could make different changes to each copy. Version control helps us manage these conflicts by giving us tools to resolve overlapping changes.\nTo see how we can resolve conflicts, we must first create one. The file mars.txt is currently in the same state in both copies of the planets repository.\nThe Collaborator can add a line to their partner’s copy, and push to GitLab:\nnano mars.txt\ngit add mars.txt\ngit commit -m \"Add a line in my friend's file\"\ngit push\nNow let’s have the Owner make a different change to their own copy without pulling from GitHub beforehand:\nnano mars.txt\nThe Owner can commit the change locally:\ngit add mars.txt\ngit commit -m \"Add a line in my own copy\"\nBut Git won’t let us push to GitHub:\ngit push\nGit rejects the push because it detects that the remote repository has new updates that have not been incorporated into the local branch. What we have to do is (1) pull the changes from GitHub, (2) merge them into the copy we’re currently working in, and then (3) push that. Let’s start by pulling:\ngit pull\nGit detects that changes made to the local copy overlap with those made to the remote repository, and therefore refuses to merge the two versions to stop us from trampling on our previous work. The conflict is marked in the affected file:\ncat mars.txt\nOur change is preceded by &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD. Git has then inserted ======= as a separator between the conflicting changes and marked the end of the content downloaded from GitLab with &gt;&gt;&gt;&gt;&gt;&gt;&gt;. (The string of letters and digits after that marker identifies the commit we’ve just downloaded.)\nIt is now up to the Owner to fix this conflict:\nnano mars.txt\nThey can now add and commit to their local repo, and then push the changes to GitHub:\ngit add mars.txt\ngit commit -m \"Merge changes from GitHub\"\ngit push\nGit keeps track of merged files. The Collaborator can now pull the changes from GitHub:\ngit pull\ngit log -3",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#hosting",
    "href": "Git/git.html#hosting",
    "title": "Git version control for collaboration",
    "section": "Hosting",
    "text": "Hosting\nGitHub? GitLab? BitBucket?\nExternal company, purchased domain and host, or local server at the lab?",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Git/git.html#licence",
    "href": "Git/git.html#licence",
    "title": "Git version control for collaboration",
    "section": "Licence",
    "text": "Licence\nThis short course is based on the longer course Version Control with Git developped by the non-profit organisation The Carpentries. The original material is licensed under a Creative Commons Attribution license (CC-BY 4.0), and this modified version uses the same license. You are therefore free to:\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material\n\n… as long as you give attribution, i.e. you give appropriate credit to the original author, and link to the license.",
    "crumbs": [
      "Home",
      "![](/images/git.svg){width=20} Git",
      "Git from the command line"
    ]
  },
  {
    "objectID": "Loop/loop.html",
    "href": "Loop/loop.html",
    "title": "Microsoft Loop",
    "section": "",
    "text": "Microsoft Loop is a collaborative workspace for teams. It supports many types of shareable components organised into pages and workspaces, editable in real time.",
    "crumbs": [
      "Home",
      "![](/images/loop.svg){width=20} Loop"
    ]
  },
  {
    "objectID": "Loop/loop.html#introduction",
    "href": "Loop/loop.html#introduction",
    "title": "Microsoft Loop",
    "section": "Introduction",
    "text": "Introduction\n\nWhat is Loop?\nLoop is a collaborative workspace for teams. It’s a flexible and versatile way for groups to organise and share content for things like meetings and projects. You can link Loop elements directly into other Microsoft applications such as Teams, Outlook and OneNote.\n\n\nLoop features\n\nLoop components (dynamic elements that update in real-time)\nLoop pages (lightweight collaborative documents)\nLoop workspaces (team projects in one place)\n\n\n\nLoop benefits\n\nReal-time editing\nSeamless collaboration across various Microsoft applications like Teams, Outlook, Word, and OneNote\nAbility to tag people directly\nYou can set tasks for people that link to Microsoft Planner",
    "crumbs": [
      "Home",
      "![](/images/loop.svg){width=20} Loop"
    ]
  },
  {
    "objectID": "Loop/loop.html#getting-started-with-loop",
    "href": "Loop/loop.html#getting-started-with-loop",
    "title": "Microsoft Loop",
    "section": "Getting started with Loop",
    "text": "Getting started with Loop\n\nStart off\n\nOpen Microsoft Loop: https://loop.microsoft.com\nCreate a new Workspace:\n\n\n\nName your workspace and invite the members you will be collaborating with\n\n\n\nThe basics\n\nCreate Pages in workspaces by clicking the “+” icon in the menu bar:\n\n\n\nType / or click on the “+” icon on the left to add elements to pages\nType @ to tag people, files, Loop pages or meetings\nType : to add emojis\nYou can add comments and reactions by clicking on the 6-dots handle on the left\nTurn elements you’ve added into Loop components by clicking the 3 dots icon and selecting “Create Loop component”:\n\n\nSome examples of Loop components that can be inserted:\n\nChecklist\nTable\nCode\nFile\nMeeting\nLoop page\n\n\n\nYour task\nFollow these steps:\n\nCreate a new Page in the demo workspace or one you’ve created.\nPopulate your page with some different elements. Try tagging people and adding some comments and reactions.",
    "crumbs": [
      "Home",
      "![](/images/loop.svg){width=20} Loop"
    ]
  },
  {
    "objectID": "Loop/loop.html#collaboration",
    "href": "Loop/loop.html#collaboration",
    "title": "Microsoft Loop",
    "section": "Collaboration",
    "text": "Collaboration\n\nHow real-time collaboration works in Loop\nYou can edit Loop pages and components directly from the Loop website, but you can also edit them in real time when they are linked in Teams, emails or documents.\nWhen you tag or assign tasks in Loop, the assigned person will receive an email where they can directly interact with the Loop page they were mentioned in.\nThere is a lot of flexibility with how you collaborate in Loop, you can tag people to prompt them to vote in a poll, or you can copy the poll as a component and post it directly to your Teams chat.\nSoon Loop will roll out Team-specific workspaces which should reduce some of the burden for managing user access to different Loop components and spaces.\n\n\nTasks apps\nTasks automatically link to Planner which you can access by selecting the Task apps button:\n\n\n\nPages in Copilot\nYou can save parts of your Chats with Microsoft Copilot as Loop pages. If you click on the “Pages” icon on the left side you should see Loop Pages from your workspace.\n\nYou can quickly add chats to pages in your workspace by selecting “Edit in Pages” at the bottom of a Copilot response:\n\nOnce you have opened a page in Copilot you can continue to add responses to the page by selecting “Add to Page”. You can also choose to add to another page you’ve already created or add to a brand new page.\n\n\n\nYour task\n\nIn the Loop page you previously created, Make a Loop component\nCopy and paste the component into a Teams chat\nYou can change the access permissions within the chat at the top of the component. People should already have existing access but keep in mind that you can share with people in the chat who might not already have access.\nFrom the Teams chat, edit your component by tagging yourself in it by typing @ followed by your name\nYou should see this update in the Loop page as well. You should also receive an email notifying you that you’ve been mentioned in a Loop\nIf you have access to your email on your device, you can edit your Loop page from there which will sync across in real time",
    "crumbs": [
      "Home",
      "![](/images/loop.svg){width=20} Loop"
    ]
  },
  {
    "objectID": "Loop/loop.html#best-practices-and-use-cases",
    "href": "Loop/loop.html#best-practices-and-use-cases",
    "title": "Microsoft Loop",
    "section": "Best practices and use cases",
    "text": "Best practices and use cases\n\nUsing Loop in different scenarios\nNow that you’re familiarised with Loop and how you can use it, you might have already identified some potential situations where you could use it in the workplace or elsewhere.\nIn our team, we use Loop as a central collaborative space that we use for meeting notes, as well as for tracking our projects and routine tasks. We link to the different spreadsheets and documents that we use to complete these tasks.\n\n\nUsing Loop to set up a meeting agenda\nYou can set up meetings in Microsoft Teams with a Loop-generated agenda. This can be useful if you want to quickly set-up a meeting with some discussion points and actions. To do this, navigate to the Calendar in Teams and select the “+ New meeting” button on the top right corner:\n\nFrom here you should see a pop-up window where you can fill out the meeting info. At the bottom of this window there should be an option to “Add an agenda”:\n\nThis will create a Loop component with a meeting agenda template:\n\nThis will by default be editable by all meeting participants and show in the sidebar in teams during the meeting for easy access.\nAs it is its own Loop component, you can copy it to other Microsoft applications, for instance you can set up a teams channel where you add this for people to refer back to.\n\n\nEmbed Loop in a SharePoint page\nYou can also embed Loop components into SharePoint pages:\n\nFrom here you can have a list of previous meetings with info on what was discussed with who, and any follow-up tasks that need to be completed.\n\n\nYour task\n\nCreate a new Page in Loop and try and make it into something you could potentially use in your team or for a particular task with a group of people. It doesn’t have to be work related.\nFeel free to use any of the inbuilt templates from Loop as well for a quick start.",
    "crumbs": [
      "Home",
      "![](/images/loop.svg){width=20} Loop"
    ]
  },
  {
    "objectID": "Loop/loop.html#further-resources",
    "href": "Loop/loop.html#further-resources",
    "title": "Microsoft Loop",
    "section": "Further resources",
    "text": "Further resources\n\nMicrosoft’s guide on Loop\nIf you work at UQ, join the UQ Microsoft 365 Viva Engage community for helpful guidance and discussion on various Microsoft 365 products such as Loop.",
    "crumbs": [
      "Home",
      "![](/images/loop.svg){width=20} Loop"
    ]
  },
  {
    "objectID": "OSM/OpenStreetMap.html",
    "href": "OSM/OpenStreetMap.html",
    "title": "OpenStreetMap: create and use spatial data",
    "section": "",
    "text": "See the slides in a separate tab.",
    "crumbs": [
      "Home",
      "![](/images/openstreetmap.svg){width=20} OpenStreetMap"
    ]
  },
  {
    "objectID": "OSM/OpenStreetMap.html#slides",
    "href": "OSM/OpenStreetMap.html#slides",
    "title": "OpenStreetMap: create and use spatial data",
    "section": "",
    "text": "See the slides in a separate tab.",
    "crumbs": [
      "Home",
      "![](/images/openstreetmap.svg){width=20} OpenStreetMap"
    ]
  },
  {
    "objectID": "OSM/OpenStreetMap.html#what-is-openstreetmap",
    "href": "OSM/OpenStreetMap.html#what-is-openstreetmap",
    "title": "OpenStreetMap: create and use spatial data",
    "section": "What is OpenStreetMap?",
    "text": "What is OpenStreetMap?\nOpenStreetMap (OSM) is a database of spatial data released under an Open Database Licence (ODbL) and supported by the OpenStreetMap Foundation.\n\n“You are free to copy, distribute, transmit and adapt our data, as long as you credit OpenStreetMap and its contributors. If you alter or build upon our data, you may distribute the result only under the same licence. The full legal code explains your rights and responsibilities.”\n\nMost of the data has been contributed by volunteers over the years, surveying areas by hand, tracing features from aerial imagery, or merging compatible datasets into OSM. Some people call OSM the “Wikipedia of spatial data”.\nMost of the data on OSM is either a point, a line or a polygon. They are described with tags. For example, to tag a polygon as a park, we can use the tag leisure = park, in which leisure is the key and park is the value.",
    "crumbs": [
      "Home",
      "![](/images/openstreetmap.svg){width=20} OpenStreetMap"
    ]
  },
  {
    "objectID": "OSM/OpenStreetMap.html#contributing-to-osm",
    "href": "OSM/OpenStreetMap.html#contributing-to-osm",
    "title": "OpenStreetMap: create and use spatial data",
    "section": "Contributing to OSM",
    "text": "Contributing to OSM\nIf you don’t already have an OSM account, head to the OSM website and create one.\nLooking at the map, can you spot something that is missing, or that needs updating? Once you are logged into your account, you can click the “Edit” button at the top of the page and start modifying the data.\n\nLocal knowledge\nDid you make a note of something near where you live or work that is missing from OSM data? Or does something need updating?\nLet’s try and contribute data to the project now!\n\nHead to the OSM website\nFind the location you want to edit\nClick on “Edit”, which opens the default OSM editor: iD. It offers you a walkthrough if you want to familiarise yourself with the interface.\nTo add a new feature:\nAdd a point / a line / an area that corresponds to what you want to add. When tracing a line or an area, click a second time on the last point to confirm you are finished tracing.\nIn the left-hand sidebar, search for the kind of feature you want to add after tracing it. Once selected, you will see the recommended tags you can fill in.\nTo edit an existing feature:\nClick on an existing feature to edit its tags in the left-hand sidebar.\n\niD also makes it easy to find documentation about the feature on the OSM Wiki: there is an “i” button next to most tagging boxes, so we can quickly read a short description of the key-tag pair.\nYou can keep editing more features if there is more to edit. Once you have finished editing the map, you can upload your changeset:\n\nClick the “Save” button at the top right\nIn the left-hand sidebar:\nAdd a changeset comment to let other editors know what you changed\nAdd a “Sources” field to record where the information comes from. For example, if you added information because you are familiar with the area, you can add the value “Survey” as a source.\nClick “Upload”\n\nYou have now added data to OpenStreetMap!\n\n\nMissing Maps\nAnother way to add useful data to the database is by contributing to a particular OSM project: Missing Maps, an effort to map areas of the world where up-to-date maps are urgently needed. This project was founded by Médecins Sans Frontières, several Red Cross organisations and the Humanitarian OSM Team.\nWe will use the HOT Task Manager, a tool that makes it very easy to contribute to this effort.\n\nHead to the HOT OSM Task Manager\nSelect a project to work on. We will agree on one specific one to work on it together.\nMake sure you read the instructions\nMake sure your are logged in with your OSM account\nClick on a task (a square) that is available (“Ready”).\nClick on “Start Mapping”\nKeep the default “iD Editor” selected and click on “Start Editor”\nA new tab will open with the iD editor, allowing you to map whatever the projects focuses on\n\nMake sure you stick to the boundaries of the task, and remember what the instructions said about what to map and what imagery you should use – you can always go back to them on the right hand side.\nOnce you have added a few features, you can:\n\nSubmit your changeset (top right button)\nThe changeset comment and the source are automatically populated with relevant hashtags and values\nMake sure you close the task with the right hand-side panel if you don’t want to continue working on it:\n\n\nIf the task is complete, mark it as such\nIf the task is not complete, mark it as such\nYou can leave a message to other contributors if needed\n\nAfter a task is marked as complete, someone will “validate” it. So do not worry if you are not entirely confident about your edits: another contributor will check for you.",
    "crumbs": [
      "Home",
      "![](/images/openstreetmap.svg){width=20} OpenStreetMap"
    ]
  },
  {
    "objectID": "OSM/OpenStreetMap.html#using-the-data-in-qgis",
    "href": "OSM/OpenStreetMap.html#using-the-data-in-qgis",
    "title": "OpenStreetMap: create and use spatial data",
    "section": "Using the data in QGIS",
    "text": "Using the data in QGIS\nWe want to now analyse the spread of libraries in Brisbane/Meanjin, using OpenStreetMap data. How do different suburbs differ in access to libraries?\nLet’s open QGIS, create a new project and save it in a newly created directory.\n\nBasemap\nWe can straight away use an OpenStreetMap basemap in QGIS: Browser panel (on the left) &gt; XYZ Tiles &gt; double-click on OpenStreetMap\nQuickMapServices also offers alternative renderings of OSM data - as well as satellite imagery. Install the plugin and search for “openstreetmap”.\nThis is great for locating ourselves on the planet, but we can’t really make use of the underlying data because it is a rendered version of it, a static image made of pixels.",
    "crumbs": [
      "Home",
      "![](/images/openstreetmap.svg){width=20} OpenStreetMap"
    ]
  },
  {
    "objectID": "OSM/OpenStreetMap.html#vector-data",
    "href": "OSM/OpenStreetMap.html#vector-data",
    "title": "OpenStreetMap: create and use spatial data",
    "section": "Vector data",
    "text": "Vector data\nTo get vector data into QGIS (and the features’ associated tags), we can go to the OSM website and click “export”.\nWe can then drag and drop this file into our layers. The issues with this are:\n\nWe get all the features at once\nThe attribute table of the features is very messy\n\nThere is a better way to get exactly what we’re looking for.\n\nQuickOSM\nThe QGIS community offers various plugins related to OSM data. For downloading OSM data, we can use a QGIS plugin called “QuickOSM”. After installing it, you can go to “Vector &gt; Quick OSM” and create a query to pull in vector data.\n\n\nFinding libraries in Brisbane\nLet’s try looking for libraries in Brisbane.\nLet’s search for:\n\nThe key amenity\nThe value library\nIn Meanjin or Brisbane\nClick “Run query”\n\nYou sometimes have to refine you query, in particular if it doesn’t automatically match the location you are interested in (for example by being more precise, e.g. Brisbane, Queensland, Australia).\n\n\nProcess the data\nYou should now have extra layers in your QGIS project: one for libraries that were mapped as polygons (probably the building shape), and another one for libraries mapped as points.\nTo focus on the area where the data is, you can right-click on a layer and click “Zoom to Layer”.\n\nChanging the transparency of the basemap layer can help make the vector data more obvious.´Double-click on the “OpenStreetMap” layer, go to the “Transparency” tab, and set the opacity to about 60%.\n\n\nReproject the data\nWe are going to trace buffers 2 km around our libraries. To be able to use this kind of unit, we will have to reproject our data so the measurements are accurate.\nOne way to do this is:\n\nFirst, set the project’s Coordinate Reference System to one relevant to the area of study:\nGo to Project &gt; Properties… &gt; CRS\nFind and apply the Projected Coordinate System named “GDA2020 / MGA zone 56” (code EPSG:7856)\nThen, go to Vector &gt; Data Management Tools &gt; Reproject Layer…\nSelect one of library layers (in “Input layer”) and set the Target CRS to the Project CRS we just set up.\nClick “Run”\nRepeat the three last steps for the other vector layer\n\n\n\nDraw buffers around the libraries\nWe can now draw buffers using a specific radius:\n\nGo to Vector &gt; Geoprocessing Tools &gt; Buffer…\nUse one of the reprojected layers as the input\nSet a distance of 2 km\nClick run\nRepeat for the other reprojected layer\n\nTo finish with a cleaner look, we can merge the two layers (now that they are both the same kind of geometry), and dissolve the buffers into a single multipolygon:\n\nGo to Vector &gt; Data Management Tools &gt; Merge Vector Layers…\nIn Input layers, click the three dot and select the two buffered layers\nClick “Run”\nGo to Vector &gt; Geoprocessing Tools &gt; Dissolve…\nSelect the merged layer as the Input layer\nClick “Run”\n\nThe resulting layer is our final library buffer that we can style to change colours and make more transparent to see the basemap through it.\n\n\nMake layers permanent\nNoticed how most of our layers have an icon next to their name? This indicates that they are temporary scratch layers, meaning they are only in memory and not saved as a file. If you close QGIS, even if you save your project, theses layers will not persist.\nTo make the important layers permanent, for example for the final buffer and the original OSM library data layers:\n\nRight-click on a layer, click “Make permanent”\nPick a location and filename by clicking the three dots\nClick “OK”\n\n\n\n\nExport a map\nTo export a map, you will have to use the Layout Manager (5th icon in the main toolbar).\nWhen creating documents and visualisations based on OSM data, you will have to respect the licence requirements, and mention where the data comes from.\nThe following statement should suffice in most cases:\n\n© OpenStreetMap contributors: https://www.openstreetmap.org/copyright",
    "crumbs": [
      "Home",
      "![](/images/openstreetmap.svg){width=20} OpenStreetMap"
    ]
  },
  {
    "objectID": "OSM/OpenStreetMap.html#further-resources",
    "href": "OSM/OpenStreetMap.html#further-resources",
    "title": "OpenStreetMap: create and use spatial data",
    "section": "Further resources",
    "text": "Further resources\n\nOSM links\n\nOther ways to get data:\n\nBrowse the data and export particular categories with OpenStreetBrowser\nCreate more advanced queries with Overpass Turbo\nUse an R package like osmdata or osmextract\n\nKeep in touch with contributors around Brisbane by looking at the local wiki page\nRead the OSM user diaries\nContinue chipping in with “humanitarian mapping” with Missing Maps\n\n\n\nQGIS links\n\nOfficial QGIS training manual",
    "crumbs": [
      "Home",
      "![](/images/openstreetmap.svg){width=20} OpenStreetMap"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html",
    "href": "OpenRefine/openrefine.html",
    "title": "OpenRefine: clean up and augment data",
    "section": "",
    "text": "OpenRefine is an open source program self-described as “a power tool for dealing with messy data”.\nIt can help you:\n\nGet an overview of a data set, and explore subsets of it\nResolve inconsistencies in a data set, for example standardising date formatting or fixing typos\nSplit data up into more granular parts, for example splitting up cells with multiple authors into separate cells\nMatch local data up to other data sets, for example in matching local subjects against the Library of Congress Subject Headings\nEnhance a data set populating it with data from other sources\n\nSome common scenarios might be:\n\nWhere you want to know how many times a particular value (name, publisher, subject) appears in a column in your data\nWhere you want to know how values are distributed across your whole data set\nWhere you have a list of dates which are formatted in different ways, and want to change all the dates in the list to a single common date format\nWhere you have a list of names or terms that differ from each other but refer to the same people, places or concepts\nWhere you have several bits of data combined together in a single column, and you want to separate them out into new columns.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#openrefine",
    "href": "OpenRefine/openrefine.html#openrefine",
    "title": "OpenRefine: clean up and augment data",
    "section": "",
    "text": "OpenRefine is an open source program self-described as “a power tool for dealing with messy data”.\nIt can help you:\n\nGet an overview of a data set, and explore subsets of it\nResolve inconsistencies in a data set, for example standardising date formatting or fixing typos\nSplit data up into more granular parts, for example splitting up cells with multiple authors into separate cells\nMatch local data up to other data sets, for example in matching local subjects against the Library of Congress Subject Headings\nEnhance a data set populating it with data from other sources\n\nSome common scenarios might be:\n\nWhere you want to know how many times a particular value (name, publisher, subject) appears in a column in your data\nWhere you want to know how values are distributed across your whole data set\nWhere you have a list of dates which are formatted in different ways, and want to change all the dates in the list to a single common date format\nWhere you have a list of names or terms that differ from each other but refer to the same people, places or concepts\nWhere you have several bits of data combined together in a single column, and you want to separate them out into new columns.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#what-you-will-learn-today",
    "href": "OpenRefine/openrefine.html#what-you-will-learn-today",
    "title": "OpenRefine: clean up and augment data",
    "section": "What you will learn today",
    "text": "What you will learn today\n\nImport and export data\nWhat “facets” and “filters” are\nEdit cells\nEdit columns\nGroup and transform data\nReconcile a column with Wikidata",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#installation",
    "href": "OpenRefine/openrefine.html#installation",
    "title": "OpenRefine: clean up and augment data",
    "section": "Installation",
    "text": "Installation\nThis lesson was designed using OpenRefine 3.8 but should be compatible with other supported versions.\nOpenRefine requires to have a Java Runtime Environment (JRE) installed. Depending on your operating system, you might need to install one from the Java website, or use the Windows kit with embedded Java.\nTo install and run OpenRefine on your operating system, please follow the instructions on the official website.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#create-a-project",
    "href": "OpenRefine/openrefine.html#create-a-project",
    "title": "OpenRefine: clean up and augment data",
    "section": "Create a project",
    "text": "Create a project\nOpenRefine can deal with formats like TSV, CSV, ODS, XLS and XLSX, JSON, RDF, XML and more.\nExercise 1 – Create a project and import data\n\nIn the “Create project” screen that opens when starting OpenRefine, select Web Addresses (URL)\nEnter the URL to our data: https://raw.githubusercontent.com/LibraryCarpentry/lc-open-refine/gh-pages/data/doaj-article-sample.csv\nClick “Next”\nClick in the Character encoding box and select UTF-8\nThis screen is also where you can ignore rows, name the project, assign tags…\nClick on Create Project\n\nYou might not have to change the character encoding, but if you see inaccuracies in how special characters are displayed, you will have to find the right one for your data.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#layout-of-openrefine",
    "href": "OpenRefine/openrefine.html#layout-of-openrefine",
    "title": "OpenRefine: clean up and augment data",
    "section": "Layout of OpenRefine",
    "text": "Layout of OpenRefine\n\nHow is data organised? How do I navigate it?\n\nOpenRefine displays data in a tabular format: rows usually are records, and columns are a type of information.\nYou can decide how many rows you want to show at once, and navigate the pages with the controls above the table.\nDrop-down menus next to the column headers allow you to do operations on the whole column.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#reorganising-the-data",
    "href": "OpenRefine/openrefine.html#reorganising-the-data",
    "title": "OpenRefine: clean up and augment data",
    "section": "Reorganising the data",
    "text": "Reorganising the data\nYou can reorder and remove columns by clicking the drop-down menu at the top of the first column (labelled ‘All’), and choosing Edit columns &gt; Re-order / remove columns….\nYou can then drag and drop column names to reorder the columns, or remove columns completely if they are not required. For example, move the Publisher column closer to the beginning, and remove the ISSNs column.\nYou can also sort the rows by clicking on the drop-down menu for the column you want to sort on, and choosing Sort.The sorting is not permanent, and a new Sort drop-down menu appears at the top, which allows you to modify the current sort, remove it, or make the sort permanent.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#facetting-and-filtering",
    "href": "OpenRefine/openrefine.html#facetting-and-filtering",
    "title": "OpenRefine: clean up and augment data",
    "section": "Facetting and filtering",
    "text": "Facetting and filtering\n\nWhat are filters and facets in OpenRefine? How can they help me navigate data? How can I correct common data issues?\n\nFacets and filters will allow you to gain insights on your data, zero-in onto issues you want to fix by hand, or only apply automated edits to one subset of your dataset.\nFacet and filter information always appears in the left-hand panel in the OpenRefine interface, and they can be combined for more complex searches.\n\nFilters\nYou can apply a text filter to your data to do a search and restrict the displayed items to it.\nIn the column drop-down menu, you can use the Text filter item and specify any string you want to filter for.\nFor example, try filtering the Subjects column with the term physic. The number of matching rows is displayed above the table.\n\n\nFacets\nA facet groups all the values that appear in a column, and allows you to subset the data and edit values across many records at the same time.\nThe simplest type of Facet is called a ‘Text facet’. It groups all the text values in a column and lists each value with the number of records it appears in.\nLet’s create a Text Facet for a column:\n\nClick on the drop-down menu at the top of the Publisher column and choose Facet &gt; Text Facet\nFilter the data by clicking on a value\nAdd more values with include\nYou can invert your selection\nClick reset to deselect all values.\n\nExercise 2 – Explore licenses\n\nWhich licences are used for articles in this dataset? Which one is the most common? How many articles don’t have a licence?\n\n\nIn the License column drop-down menu, choose Facet &gt; Text facet\nCC BY has the highest count\n6 articles don’t have a License value, marked (blank)\nNotice that filtering the data with this facet will update the values in the Publisher facet. This makes it easy to see which publishers use which licences.\nClose both facets with the “Remove All” button\n\nThere are more facets available, like numeric and timeline facets that produce graphs, or even custom facets for more advanced operations, like facets that return a logical value (“true” or “false”).\nExercise 3 – Facet by blank\nFind all the publications without a DOI (digital object identifier) with the Facet by blank. How many are there?\n\nFacet &gt; Customized facet &gt; Facet by blank\n\nThis is more efficient than a text facet, as it will only give two categories: “true” for blank cells, “false” for cells with a value. You could then focus on the blank cells to populate the missing data by hand (an “Edit” button appears when you hover over a cell).\n\nWhen you filter your data with facets or text filters, remember that any operation that you carry out will only apply to the matches.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#amending-data",
    "href": "OpenRefine/openrefine.html#amending-data",
    "title": "OpenRefine: clean up and augment data",
    "section": "Amending data",
    "text": "Amending data\nUsing a text facet, you can edit values for a whole subset in one action with the edit option. This is very useful for fixing small typos or variations in spelling.\nExercise 4 – Correct values\nUse a text facet on the Language column to replace English values with the language code EN.\n\nFacet &gt; Text facet\nHover over the term English and click edit\nReplace it with EN and click Apply\n\nAll the occurrences of English are replaced by EN, and the facet auto-updates to only show the remaining values.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#split-multi-valued-cells",
    "href": "OpenRefine/openrefine.html#split-multi-valued-cells",
    "title": "OpenRefine: clean up and augment data",
    "section": "Split multi-valued cells",
    "text": "Split multi-valued cells\n\nHow do I deal with cells that contain multiple values?\n\nAuthor names are currently separated with the pipe symbol |. We can use the Split multi-valued cells function to put them in their own cells.\nExercise 5 – Split multi-valued cells\n\nOpen the drop-down menu for the Authors column\nSelect Edit cells &gt; Split multi-valued cells\nUse the pipe character | as a separator\nClick OK\n\nNote that we now have a lot more rows, and that they are still numbered sequentially.\nHow would we join data from several cells together, for example after cleaning the data up? We can do the opposite operation, with Edit cells &gt; Join multi-valued cells, and deciding which character we want to use to separate values.\n\nMake sure you choose the right delimiter for the kind of data you deal with!\n\nExercise 6 – Practise splitting and joining\nHow would you split and then join again the data in the Subjects column?",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#view-modes",
    "href": "OpenRefine/openrefine.html#view-modes",
    "title": "OpenRefine: clean up and augment data",
    "section": "View modes",
    "text": "View modes\nOpenRefine has two view modes for viewing data: Rows and Records. In Records mode, OpenRefine can link together multiple rows as belonging to the same record.\nSwitch to the records mode. Note how the numbering has changed: several rows are related to the same record, because of the splitting we just did.\nIf you now filter for an author name, notice how the “rows” and “records” view will change how much is displayed? We can stick to “records” to have all the associated information.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#clustering",
    "href": "OpenRefine/openrefine.html#clustering",
    "title": "OpenRefine: clean up and augment data",
    "section": "Clustering",
    "text": "Clustering\n\nWhat can I use to identify and replace varying forms of the same data?\n\nFixing variations by hand like we did for the languages works fine for small numbers of values, but if there are hundreds or more, it becomes very impractical.\nThe Cluster function uses an algorithm to group together similar, but inconsistent values in a given column and lets you merge these inconsistent values into a single value you choose.\nThis is very effective where you have data with minor variations in values, e.g. names of people, organisations, places, classification terms…\nExercise 7 – clean up the author names\n\nMake sure your authors are already split into different rows\nChoose Edit cells &gt; Cluster and edit from the Authors column\nUsing the defaults (key collision Method and fingerprint Keying Function), click “Cluster” and work through the clusters of values, merging them into a single value where appropriate\n\nBy default, OpenRefine uses the most common value to merge the data, but we can choose which value we prefer by clicking on it, or we can write our own replacement value.\n\nYou can also try changing the clustering method being used – which ones work well? Do they identify extra clusters?\n\n\nThe best clustering algorithm will depend on the kind of data your are processing. For more information on available algorithms, see the documentation page on clustering methods.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#transformations",
    "href": "OpenRefine/openrefine.html#transformations",
    "title": "OpenRefine: clean up and augment data",
    "section": "Transformations",
    "text": "Transformations\nFacets, filters and clustering already allow us to explore and clean up our data.\nTo do more advanced operations, like splitting data into several columns, standardising a format without losing the original values, or extracting a data type from a string, we need transformations.\nTransformations are ways of manipulating data in columns. They are normally written in a special language called GREL (General Refine Expression Language).\nSome common transformations are accessible directly in the menus, for example to change the case of the values, or to remove bounding whitespace characters.\nExercises 8 – Correct publisher data\n\nCreate a text facet on the Publisher column\nNote that in the values there are two that look identical – why does this value appear twice?\nOn the Publisher column, use the drop-down menu to select Edit cells &gt; Common transforms &gt; Collapse consecutive whitespace\nLook at the publisher facet now – has it changed? (if it hasn’t changed, try clicking the Refresh option to make sure it updates)\n\n\nWriting transformations\nThe transformation screen is available through Edit cells &gt; Transform..., in which you can write your custom GREL command with various functions and preview its effect.\nGREL supports two types of syntax:\n\nvalue.function(options)\nfunction(value, options)\n\nEither is valid, and which is used is completely down to personal preference. In these notes the first syntax is used, as it is easier to read when writing long commands.\nThe most simple “transformation” is value, which will keep the data as it is. We need to add extra functions to the command to actually modify the original value.\nExercise 9 – Put titles into Title Case\nUse Facets and the GREL function toTitlecase() to put some titles in Title Case:\n\nFacet by Publisher\nSelect “Akshantala Enterprises” and “Society of Pharmaceutical Technocrats” (To select multiple values in the facet, use the “include” link that appears to the right of the facet.)\nSee that the Titles for these are all in uppercase\nClick the drop-down menu on the Title column\nChoose Edit cells &gt; Transform...\nIn the Expression box type value.toTitlecase()\nIn the Preview tab, see the effect of running this will be\nClick “OK”\n\nNote that this transformation is also available in the Common transforms.\n\nBack in the Transform dialog, you can now see a history of transformations, and save your favourite ones by clicking on the star next to them. The Help tab is also a helpful reference.\n\n\n\nUndo and redo actions\nIn the left-hand panel, you can open the Undo / Redo tab to access all the steps taken so far, and undo steps by clicking the last step you want to preserve.\nThe Extract... button allows you to save a set of steps as a JSON (JavaScript Object Notation) script, to re-apply them later on this or any other dataset. To execute an extracted set of steps, click the Apply button.\n\n\nTransforming strings, numbers, dates and boolean\n\nHow do I transform different data types?\n\nData types and regular expressions will allow you to write more complex GREL transformations.\nAll data in OpenRefine has a “type”. The most common is “string”, which is a piece of text. Data types supported are:\n\nString\nNumber\nDate-time\nBoolean (logical)\nArray\n\nDate and numbers: we currently have a Date column where the data is represented as a string. If we wanted to sort according to this data, it would not end up chronological. We therefore need to convert the values to a date data type for OpenRefine to interpret them properly.\nExercise 10 – Reformat the date\n\nMake sure you remove all Facets and Filters\nOn the Date column, use the drop-down menu to select Edit cells &gt; Common transforms &gt; To date\nNote how the values are now displayed in green and follow a standard convention for their display format (ISO8601) - this indicates they are now stored as date-time data types in OpenRefine. We can now carry out functions that are specific to Dates.\nIn the Date column drop-down, select Edit column &gt; Add column based on this column. Using this dialog, you can create a new column, while preserving the old column.\nIn the ‘New column name’ type “Formatted Date”\nIn the ‘Expression’ box, type the GREL expression value.toString(\"dd MMMM yyyy\")\n\nHaving a column with the proper data type will also allow us to use features like the timeline facet.\nBooleans: a boolean is a binary value that can either be “true” or “false”. They are often used in GREL expressions, for example:\nvalue.contains(\"test\")\n… generates a boolean value which depends on whether the cell value contains the string “test” anywhere.\nThat allows use to build more complex transformations, for example by carrying out a transformation only if a test is successful:\nif(value.contains(\"test\"),\"replacement text\",value)\n… replaces a cell value with the string “replacement text” only if the value in the cell contains the string “test” anywhere.\n\n\nHandling arrays\nThere is another inconsistency in our “Authors” column: a few names are using the order &lt;last name&gt;, &lt;first name&gt;, and all others use the &lt;first name&gt; &lt;last name&gt; order. We’d like to stick to the same order for all, but to do that, we need to talk about arrays.\n\nHow do I use arrays in a GREL expression?\n\nAn array is a list of values. Arrays can be sorted, de-duplicated and manipulated in other ways in GREL expressions. They usually are the result of a transformation, and cells can’t contain them.\nFor example, if a cell contains the following string:\n“Monday,Tuesday,Wednesday,Thursday,Friday,Saturday,Sunday”\n… it can be transformed into an array with the split function:\nvalue.split(\",\")\n… which would result in the following array:\n[“Monday”,”Tuesday”,”Wednesday”,”Thursday”,”Friday”,”Saturday”,”Sunday”]\nCombining the split function with a sort function like so:\nvalue.split(\",\").sort()\n… would result in an array of days of the week sorted alphabetically:\n[“Friday”,”Monday”,”Saturday”,”Sunday”,”Thursday”,”Tuesday”,”Wednesday”]\nYou can output a value from an array by specifying its position in the list. To extract the first value of the array created by the split function:\nvalue.split(\",\")[0]\n\nMany programming languages, like GREL, start indexing at 0, which is why we have to use the 0th position to return the first element of the array.\n\nIf you want to go back to one single string, in order to store the result in cells, you can use the join function:\nvalue.split(\",\").sort().join(\",\")\nExercise 11 – Reverse author names\nWe can focus on the rows that have a comma in their author name:\n\nUsing the “Authors” column drop-down menu, use a Text filter\nFilter for the comma character: “,”.\n\nNow that we have narrowed down to the lines with a comma in the author name, we can use the split() function.\n\nOn the Authors column use the drop-down menu and select Edit cells &gt; Transform\nIn the Expression box type value.split(\", \"). This will create an array of separate last name and first name, removing the comma and space.\n\nTo get the author name in the natural order, you can reverse the array and join it back together with a space to create the string you need:\n\nIn the Expression box, add to the existing expression until it reads value.split(\", \").reverse().join(\" \")\nIn the Preview section, you should be able see this has reversed the array, and joined it back into a string, without any comma\nClick OK to apply your transformation, and notice how your filter shows 0 results, because all commas have been removed in the “Authors” column.\n\n\nFor more elaborate array creation, we can use the match() function along with Regular Expressions. Regular Expressions, or “Regex”, use a syntax to match pretty much any pattern of characters.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#reconciliation",
    "href": "OpenRefine/openrefine.html#reconciliation",
    "title": "OpenRefine: clean up and augment data",
    "section": "Reconciliation",
    "text": "Reconciliation\nWe can associate one of our columns to an external database by using OpenRefine’s reconciliation feature:\nExercise 12 – Reconcile publisher data against Wikidata\n\nOn the “Publisher” column, select Reconcile &gt; Start reconciling.... The default service listed is Wikidata.\nThe tool will automatically try to match the column to a Wikidata type, but we can specify exactly the one we want to use.\nAfter checking the existing matches, and completing by hand the missing ones, we can add extra data to our project by using Edit column &gt; Add columns for reconciled values. For example, try adding the headquarters location of the publishers into a new column.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#exporting-the-data-and-saving-a-project",
    "href": "OpenRefine/openrefine.html#exporting-the-data-and-saving-a-project",
    "title": "OpenRefine: clean up and augment data",
    "section": "Exporting the data and saving a project",
    "text": "Exporting the data and saving a project\n\nHow do I export transformed data?\n\nOpenRefine can export a transformed dataset into a variety of formats. You can access the menu with the top-right Export button. The Custom Tabular Exporter offers extra options, like selecting a subset of columns, reordering them, removing headers, selecting a format and uploading as a Google Spreadsheet.\nYour project is automatically saved every 15 minutes and can be reopened with Open Project when you next start OpenRefine. However, if you need to share or save a snapshot of your project, you can use Export &gt; Export project to download an archive.\n\nWhen you quit OpenRefine, make sure you interrupt the OpenRefine process properly with Ctrl + C (for both Windows and Linux) or Command + Q (for MacOS) in the terminal, to guarantee that your project changes are saved. See the official documentation on starting and exiting OpenRefine.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#useful-links",
    "href": "OpenRefine/openrefine.html#useful-links",
    "title": "OpenRefine: clean up and augment data",
    "section": "Useful links",
    "text": "Useful links\nOpenRefine learning:\n\nFull lesson this one is based on: https://librarycarpentry.github.io/lc-open-refine/\n\n“Advanced functions” chapter: https://librarycarpentry.github.io/lc-open-refine/13-looking-up-data/index.html\n\nVideo walk throughs: http://openrefine.org/\nVideo tutorial on reconciliation and Wikidata contribution: https://www.youtube.com/watch?v=wfS1qTKFQoI\nGetting started with OpenRefine by Thomas Padilla: http://thomaspadilla.org/dataprep/\nCleaning Data with OpenRefine by Seth van Hooland, Ruben Verborgh and Max De Wilde: http://programminghistorian.org/lessons/cleaning-data-with-openrefine\nFree your Metadata website: http://freeyourmetadata.org/\nOpenRefine Blog: http://openrefine.org/category/blog.html\nOfficial OpenRefine documentation:\n\nGREL documentation: https://docs.openrefine.org/manual/expressions#grel-general-refine-expression-language\nClustering in Depth: https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth\nA compilation of “OpenRefine recipes”: https://github.com/OpenRefine/OpenRefine/wiki/Recipes\n\n\nRegular Expressions:\n\nUnderstanding Regular Expressions: https://github.com/OpenRefine/OpenRefine/wiki/Understanding-Regular-Expressions\nRegEx + GREL cheatsheet: https://code4libtoronto.github.io/2018-10-12-access/GoogleRefineCheatSheets.pdf\n\nOther data sources for reconciliation:\n\nhttps://github.com/OpenRefine/OpenRefine/wiki/Reconcilable-Data-Sources\n\nExamples of OpenRefine uses:\n\nIdentifying potential headings for Authority work using III Sierra, MS Excel and OpenRefine: http://epublications.marquette.edu/lib_fac/81/\nData Munging Tools in Preparation for RDF: Catmandu and LODRefine by Christina Harlow: http://journal.code4lib.org/articles/11013\nBlog posts on using OpenRefine from Owen Stephens: http://www.meanboyfriend.com/overdue_ideas/tag/openrefine/?orderby=date&order=ASC\n\nAnother dataset to play around with:\n\nhttp://www.thomaspadilla.org/data/dataprep/authors-people.csv",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "OpenRefine/openrefine.html#licence",
    "href": "OpenRefine/openrefine.html#licence",
    "title": "OpenRefine: clean up and augment data",
    "section": "Licence",
    "text": "Licence\nThis lesson is based on a Library Carpentry lesson, available here: https://librarycarpentry.github.io/lc-open-refine/ It is also released under a Creative Commons Attribution license. The following is a human-readable summary of (and not a substitute for) the full legal text of the CC BY 4.0 license.\nYou are free:\n\nto Share – copy and redistribute the material in any medium or format\nto Adapt – remix, transform, and build upon the material\n\nfor any purpose, even commercially.\nUnder the following terms:\n\nAttribution – You must give appropriate credit (mentioning that your work is derived from work that is Copyright © Stéphane Guillou and, where practical, linking to https://gitlab.com/stragu/DSH), provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.",
    "crumbs": [
      "Home",
      "![](/images/openrefine.svg){width=20} OpenRefine"
    ]
  },
  {
    "objectID": "Photoshop/photoshop.html",
    "href": "Photoshop/photoshop.html",
    "title": "Adobe Photoshop",
    "section": "",
    "text": "Photoshop is a raster graphics editor used for photo-editing and digital art.\nFind below the resources for our Photoshop sessions.",
    "crumbs": [
      "Home",
      "![](/images/photoshop.svg){width=20} Photoshop"
    ]
  },
  {
    "objectID": "Photoshop/photoshop.html#photoshop-introduction",
    "href": "Photoshop/photoshop.html#photoshop-introduction",
    "title": "Adobe Photoshop",
    "section": "Photoshop: introduction",
    "text": "Photoshop: introduction\n\nManual\nFiles",
    "crumbs": [
      "Home",
      "![](/images/photoshop.svg){width=20} Photoshop"
    ]
  },
  {
    "objectID": "Premiere_Pro/premiere_pro.html",
    "href": "Premiere_Pro/premiere_pro.html",
    "title": "Adobe Premiere Pro",
    "section": "",
    "text": "Premiere Pro is a video editor.\nFind below the resources for our Premiere Pro sessions.",
    "crumbs": [
      "Home",
      "![](/images/premiere_pro.svg){width=20} Premiere Pro"
    ]
  },
  {
    "objectID": "Premiere_Pro/premiere_pro.html#premiere-pro-introduction",
    "href": "Premiere_Pro/premiere_pro.html#premiere-pro-introduction",
    "title": "Adobe Premiere Pro",
    "section": "Premiere Pro introduction",
    "text": "Premiere Pro introduction\n\nManual\nFiles",
    "crumbs": [
      "Home",
      "![](/images/premiere_pro.svg){width=20} Premiere Pro"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html",
    "href": "Python/2-data_processing/data_processing.html",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "",
    "text": "TipUpcoming workshop(s) available!\n\n\n\nThe next workshop is on Fri Mar 06 at 09:30 AM.\nBook in to the next offering now.\nAlternatively, check our calendar for future events.\nIn this second workshop we will cover\nThis hands-on course – directed at intermediate users – looks at using the pandas module to transform and visualise tabular data.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html#setting-up",
    "href": "Python/2-data_processing/data_processing.html#setting-up",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "Setting up",
    "text": "Setting up\n\nScripts and projects\nRecall that we typically write code in scripts and store them in a project. We’ll do the same here.\n\nCreate / open a project. If you made one last week, feel free to continue working there. Otherwise, press Projects &gt; New project... and name your project, perhaps “python_data_processing”.\nCreate a new script with ctrl+N, File &gt; New file... or the new file button.\n\n\n\nIntroducing pandas\nPandas is a Python module that introduces dataframes to Python. It gives us the tools we need to clean and transform data with Python.\nTo be able to use the functions included in pandas, we have to first import it:\n\nimport pandas as pd\n\npd is the usual nickname for the pandas module.\n\nIf you get an error, like No module named 'pandas', you’ll need to install it first, using either conda install pandas or pip install pandas, depending on your Python installation.\n\n\nThe DataFrame object\nPandas is built upon one key feature: the DataFrame class. In Python we have different built-in types, like int for integers and string for characters. Pandas introduces a new type, DataFrame, which stores data like a spreadsheet.\n\n\n\nSetting up the workspace\nTo make life easy, we should set up our workspace well.\n\nOpen your project folder using your file explorer, and create a new folder called “data”.\nDownload the data for today’s session\nMove the file into your new “data” folder\nNext, open your project in Spyder, and create a new script called “analysis.py”.\nOpen the “Files” tab in Spyder and check that you see two objects:\n\nThe file “analysis.py”\nThe folder “data”\n\n\n\n\nImporting data\nPandas offers a simple way to access data with its read.csv() function. We’ll save it into the variable df_raw:\n\ndf_raw = pd.read_csv(\"data/Players2024.csv\")\n\n\nYou can also provide a URL instead of a file path!\n\n\n\nAside - File Paths and backslashes\nJust a quick detour to discuss file paths of which there are two types: absolute and relative\n\nAbsolute\nAbsolute file paths always start at the “top” of your file system, e.g. one of the drives (like C:) for Windows users, so they are never ambiguous. It’s like providing your full street address from country to street number.\nC:/Users/my_username/research/data/really_important_secret_data.csv\n\n\nRelative\nRelative file paths start from your current working directory, which is usually the top folder of a Spyder project. For files in my current folder, I just provide their name - like referring to another house on your street as “number 7”. Let’s assume we’re in the “research” folder.\nfile_in_my_current_folder.csv\nWe can go to down folders from our current location:\ndata/really_important_secret_data.csv\nAnd we can go up folders from our current location\n../../this_file_is_two_levels_up.csv\nOr a combination of the two (e.g. up one, then down into a different folder)\n../not_research/this_file_is_not_research.csv\nWhat matters is that the relative reference depends on where your code is and will break if you move the script!\n\n\nBackslashes\nOne last note: Windows uses backslashes for their file paths\nC:\\Users\\...\nBut Python uses backslashes as an escape character. For example, \"\\n\" is a newline, \"\\u1234\" is the unicode character U+1234 and confusingly \"\\\\\" is a single backslash. The easist way to get around this is by prefixing R to all strings: this makes them raw.\n\nwindows_url = R\"C:\\\\Users\\...\"\n\n\n\n\nInitial look at the data\nLet’s get back to data.\nWe can investigate the size of the data thanks to the shape attribute attached to all pandas dataframes:\n\ndf_raw.shape\n\n(5935, 7)\n\n\nThe dataset contains dozens of columns. What are their names?\n\ndf_raw.columns\n\nIndex(['name', 'birth_date', 'height_cm', 'positions', 'nationality', 'age',\n       'club'],\n      dtype='object')\n\n\nLet’s subset our data to focus on a handful of variables.\n\n\nCreating a backup\nData analysis in Python is safe because our variables are copies of the data - we aren’t actually changing the files until we explicitly overwrite them. However, Python also has no undo, so if I delete something in my analysis, I can’t get it back - I have to start all over again.\nOne way to mitigate this issue is by making a copy of the data\n\ndf = df_raw.copy()\n\nNow we have two variables: df is what we’ll use, and df_raw stores the raw data. If we ever need to restart, we can simply run df = df_raw.copy().",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html#accessing-and-filtering-data",
    "href": "Python/2-data_processing/data_processing.html#accessing-and-filtering-data",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "Accessing and Filtering Data",
    "text": "Accessing and Filtering Data\nSo how do we access our data in Python? We use a type of indexing introduced by pandas, which revolves around using square brackets after the dataframe: df[...].\n\nAccessing columns\nTo access a column, index with its name: df[\"column_name\"]. For example,\n\ndf[\"name\"]\n\n0                 James Milner\n1          Anastasios Tsokanis\n2                Jonas Hofmann\n3                   Pepe Reina\n4                Lionel Carole\n                 ...          \n5930    Oleksandr Pshenychnyuk\n5931              Alex Marques\n5932               Tomás Silva\n5933               Fábio Sambú\n5934            Hakim Sulemana\nName: name, Length: 5935, dtype: object\n\n\nreturns the “name” column. We can access multiple by providing a list of names\n\n# Save the names in a list and then index\ncolumn_names = [\"name\", \"club\"]\ndf[column_names]\n\n# This is equivalent to\ndf[[\"name\", \"club\"]]\n\n\n\n\n\n\n\n\nname\nclub\n\n\n\n\n0\nJames Milner\nBrighton and Hove Albion Football Club\n\n\n1\nAnastasios Tsokanis\nVolou Neos Podosferikos Syllogos\n\n\n2\nJonas Hofmann\nBayer 04 Leverkusen Fußball\n\n\n3\nPepe Reina\nCalcio Como\n\n\n4\nLionel Carole\nKayserispor Kulübü\n\n\n...\n...\n...\n\n\n5930\nOleksandr Pshenychnyuk\nZAO FK Chornomorets Odessa\n\n\n5931\nAlex Marques\nBoavista Futebol Clube\n\n\n5932\nTomás Silva\nBoavista Futebol Clube\n\n\n5933\nFábio Sambú\nBoavista Futebol Clube\n\n\n5934\nHakim Sulemana\nRanders Fodbold Club\n\n\n\n\n5935 rows × 2 columns\n\n\n\nIf we want to do anything with it (like statistics or visualisation), it’s worth saving the column(s) as a new variable\n\ndf_subset = df[[\"name\", \"club\"]]\n\n\n\nAccessing rows\nThere’s a few ways to access rows. The easiest is by slicing, df[start_row : end_row]. For example, if you want rows 5 to 10,\n\ndf[5 : 10]\n\n\n\n\n\n\n\n\nname\nbirth_date\nheight_cm\npositions\nnationality\nage\nclub\n\n\n\n\n5\nLudovic Butelle\n1983-04-03\n188.0\nGoalkeeper\nFrance\n41\nStade de Reims\n\n\n6\nDaley Blind\n1990-03-09\n180.0\nDefender\nNetherlands\n34\nGirona Fútbol Club S. A. D.\n\n\n7\nCraig Gordon\n1982-12-31\n193.0\nGoalkeeper\nScotland\n41\nHeart of Midlothian Football Club\n\n\n8\nDimitrios Sotiriou\n1987-09-13\n185.0\nGoalkeeper\nGreece\n37\nOmilos Filathlon Irakliou FC\n\n\n9\nAlessio Cragno\n1994-06-28\n184.0\nGoalkeeper\nItaly\n30\nAssociazione Calcio Monza\n\n\n\n\n\n\n\n\nNote that the end row is not included\n\nIf you want to access a single row, we need to use df.loc[] or df.iloc[]. These are the go-to methods for accessing data if the above indexing isn’t sufficient.\n\ndf.loc[] accesses rows by label (defaults to row number but could be anything)\ndf.iloc[] accesses rows by row number exclusively\n\nBy default they line up, so\n\ndf.loc[5]\ndf.iloc[5]\n\nname           Ludovic Butelle\nbirth_date          1983-04-03\nheight_cm                188.0\npositions           Goalkeeper\nnationality             France\nage                         41\nclub            Stade de Reims\nName: 5, dtype: object\n\n\nare often (but not always) the same.\nFinally, we can filter specific rows by a condition on one of the variables, e.g. only rows where variable \\(\\text{age} &gt; 25\\).\n\ndf[df[\"age\"] &gt; 25]\n# Or any other condition\n\n\n\n\n\n\n\n\nname\nbirth_date\nheight_cm\npositions\nnationality\nage\nclub\n\n\n\n\n0\nJames Milner\n1986-01-04\n175.0\nMidfield\nEngland\n38\nBrighton and Hove Albion Football Club\n\n\n1\nAnastasios Tsokanis\n1991-05-02\n176.0\nMidfield\nGreece\n33\nVolou Neos Podosferikos Syllogos\n\n\n2\nJonas Hofmann\n1992-07-14\n176.0\nMidfield\nGermany\n32\nBayer 04 Leverkusen Fußball\n\n\n3\nPepe Reina\n1982-08-31\n188.0\nGoalkeeper\nSpain\n42\nCalcio Como\n\n\n4\nLionel Carole\n1991-04-12\n180.0\nDefender\nFrance\n33\nKayserispor Kulübü\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n5155\nLeo Scienza\n1998-09-13\n175.0\nAttack\nBrazil\n26\n1. Fußballclub Heidenheim 1846\n\n\n5236\nMohamed Brahimi\n1998-09-17\n181.0\nAttack\nFrance\n26\nFK Fakel Voronezh\n\n\n5287\nNicolás Marotta\n1996-12-23\n186.0\nDefender\nArgentina\n27\nAthens Kallithea Football Club\n\n\n5471\nDaniel Sosah\n1998-09-21\n179.0\nAttack\nNiger\n26\nFK Kryvbas Kryvyi Rig\n\n\n5478\nEgas Cacintura\n1997-10-29\n174.0\nMidfield\nAngola\n26\nDinamo Makhachkala\n\n\n\n\n2757 rows × 7 columns\n\n\n\nAs with the column case, it’s useful to save this as a variable\n\ndf_filtered = df[df[\"age\"] &gt; 15]",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html#basic-statistics",
    "href": "Python/2-data_processing/data_processing.html#basic-statistics",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "Basic statistics",
    "text": "Basic statistics\nHow might we perform some basic statistics on our data?\nTo check what kind of data each column is stored as, we can use the dtypes attribute:\n\ndf.dtypes\n\nname            object\nbirth_date      object\nheight_cm      float64\npositions       object\nnationality     object\nage              int64\nclub            object\ndtype: object\n\n\n\nIn general, pandas will bring in numbers with float64 and non-numeric data with object.\n\nThe describe() method is useful for descriptive statistics about our numerical columns:\n\ndf.describe()\n\n\n\n\n\n\n\n\nheight_cm\nage\n\n\n\n\ncount\n5935.000000\n5935.000000\n\n\nmean\n182.986352\n25.501769\n\n\nstd\n7.478313\n4.455595\n\n\nmin\n17.000000\n15.000000\n\n\n25%\n178.000000\n22.000000\n\n\n50%\n183.000000\n25.000000\n\n\n75%\n188.000000\n29.000000\n\n\nmax\n206.000000\n42.000000\n\n\n\n\n\n\n\nHowever, it will only show the two first ones and two last ones. We can focus on a specific column instead, for example one that was hidden previously:\n\ndf[\"age\"].describe()\n\ncount    5935.000000\nmean       25.501769\nstd         4.455595\nmin        15.000000\n25%        22.000000\n50%        25.000000\n75%        29.000000\nmax        42.000000\nName: age, dtype: float64\n\n\nOr a categorical column:\n\ndf[\"nationality\"].describe()\n\ncount      5935\nunique      135\ntop       Spain\nfreq        402\nName: nationality, dtype: object\n\n\n\nFor a categorical column, the information shown is different: for example, how many unique values there are, and what the most common value is.\n\nWhat if you want specific statistics about a particular column? Usually there are methods available:\n\n# Applicable to all columns\ndf[\"nationality\"].count()\ndf[\"nationality\"].unique()\n\n# For numeric columns only\ndf[\"height_cm\"].min()\ndf[\"height_cm\"].max()\ndf[\"height_cm\"].mean()\ndf[\"height_cm\"].median()\ndf[\"height_cm\"].std()\n# ...\n\nnp.float64(7.478312588515905)\n\n\nWe can use these methods to filter our data. For example, the row which has the maximum value of variable \\(x\\) is\n\nx_max = df[\"height_cm\"].max()\ndf[df[\"height_cm\"] == x_max]\n\n# Or in one line\ndf[df[\"height_cm\"] == df[\"height_cm\"].max()]\n\n\n\n\n\n\n\n\nname\nbirth_date\nheight_cm\npositions\nnationality\nage\nclub\n\n\n\n\n4179\nKevin Gadellaa\n2003-04-08\n206.0\nGoalkeeper\nNetherlands\n21\nFootball Club Utrecht\n\n\n4810\nIsaak Touré\n2003-03-28\n206.0\nDefender\nFrance\n21\nUdinese Calcio\n\n\n5565\nDenys Tvardovskyi\n2003-06-13\n206.0\nGoalkeeper\nUkraine\n21\nFC Shakhtar Donetsk",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html#activity-1",
    "href": "Python/2-data_processing/data_processing.html#activity-1",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "Activity 1",
    "text": "Activity 1\nRun the following lines:\n\nprint(df[\"height_cm\"].min())\nprint(df[\"positions\"].unique())\n\n17.0\n['Midfield' 'Goalkeeper' 'Defender' 'Attack' 'Missing']\n\n\nNotice anything odd? There’s some dubious data - remove the dodgy entries.\n\nHint: Nobody is 17cm tall, and we don’t want to keep anyone with the “Missing” position either\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\n# Remove rows with unreasonable heights\ndf = df[df[\"height_cm\"] &gt; 100]\n\n# Remove the rows with position = \"Missing\"\ndf = df[df[\"positions\"] != \"Missing\"]\n\n\n\n\nIf that was too quick try to reduce your dataset to \\(\\le 3\\) variables (columns) and \\(\\le 100\\) rows using conditions by filtering down to a particular subset of your data. Make sure you keep the age and height_cm columns.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html#adding-and-removing-columns",
    "href": "Python/2-data_processing/data_processing.html#adding-and-removing-columns",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "Adding and removing columns",
    "text": "Adding and removing columns\nSometimes we need to add new columns. It’s the same process as overwriting existing columns - let’s make a new column called “zeroes” where every row is 0\n\ndf[\"zeroes\"] = 0\n\nWe can also send in a column, for example\n\ndf[\"copy_of_names\"] = df[\"name\"]\n\nPerhaps most usefully, we can manipulate the column we send in. For example, the deviation from the mean \\[|\\bar{x} - x_i|\\] can be computed for each row’s height:\n\ncol_x = df[\"height_cm\"]\navg_x = df[\"height_cm\"].mean()\n\ndf[\"deviation_from_mean_height\"] = abs(col_x - avg_x)\n\n# Or all together on one line,\ndf[\"deviation_from_mean_height\"] = abs(df[\"height_cm\"] - df[\"height_cm\"].mean())\n\nwhere abs(...) takes the absolute value\nNotice that we subtracted a value from a column. We can also perform mathematics with multiple columns:\n\ndf[\"product\"] = df[\"age\"]*df[\"height_cm\"]\n\nLet’s remove these new columns that we don’t need with the method df.drop(columns = [...]):\n\ndf = df.drop(columns = [\"zeroes\", \"copy_of_names\", \"deviation_from_mean_height\", \"product\"])",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html#summaries",
    "href": "Python/2-data_processing/data_processing.html#summaries",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "Summaries",
    "text": "Summaries\nAfter cleaning up our data, we need to analyse it. This usually involves some kind of aggregation. For example, what is the average \\(x\\) per year? requires aggregating over variable \\(x\\) for each year.\nFirst, we need to group by a specific variable\n\ngb = df.groupby(\"age\")\n\nThis thing in itself is a pretty abstract Python object, best thought of as a dataframe where we’ve identified a grouping variable.\nNext, we need to apply some aggregation to it (the groupby tells it to do it for each year)\n\navg_height_by_age = gb[\"height_cm\"].agg(\"mean\")\n\nOf course, we could have done this in one line:\n\navg_height_by_age = df.groupby(\"age\")[\"height_cm\"].agg(\"mean\")\n\nThis is a really useful tool, because now we have something we can visualise. As the next session will show us, the visualisation tools generally just take in numbers and turn them into dots. We need to do the stats beforehand.\nAs a taster, try running\n\navg_height_by_age.plot()",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html#exporting-results",
    "href": "Python/2-data_processing/data_processing.html#exporting-results",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "Exporting results",
    "text": "Exporting results\nThe last step in the process is saving the data. Let’s say we want to take that final dataframe and export it to a csv. That’s what the df.to_csv() method is for\n\navg_height_by_age.to_csv(\"data/avg_height_by_age.csv\")\n\nThis will save the dataframe to a .csv file and place it in the data folder.",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html#activity-2",
    "href": "Python/2-data_processing/data_processing.html#activity-2",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "Activity 2",
    "text": "Activity 2\nNow that you’ve explored our “Players” dataset, why not try something larger? Download the gapminder dataset and explore. Try to use the following three techniques:\n\nFilter the data by a condition\nAggregate over a particular variable\nVisualise your result\n\nFor step 3., you’ll either want to reduce your data to two columns and use .plot(), or specify your axes with .plot(x = \"x_variable\", y = \"y_variable\").\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nOne possible solution with aggregation is\n\n# Import the data\ngapminder = pd.read_csv(\"data/gapminder.csv\")\n\n# Select specific columns\nsubset = gapminder[[\"continent\", \"year\", \"pop\"]]\n\n# Max pop per continent of all time\ncontinents = subset.groupby(\"continent\").agg(\"max\")\n\ncontinents.plot(y = \"pop\", kind = \"bar\")",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/2-data_processing/data_processing.html#conclusion",
    "href": "Python/2-data_processing/data_processing.html#conclusion",
    "title": "Python Training (2 of 4): Managing Data",
    "section": "Conclusion",
    "text": "Conclusion\nToday we looked at a lot of Python features, so don’t worry if they haven’t all sunk in. Programming is best learned through practice, so keep at it! Here’s a rundown of the concepts we covered\n\n\n\n\n\n\n\nConcept\nDesctiption\n\n\n\n\nImporting data\nThe pandas package provides the pd.read_... functions to import data, like pd.read_csv(). Save it in a variable.\n\n\nAccessing and filtering rows and columns\nUse square brackets for basic accessing and filtering, e.g. df[\"column_a\"] or df[df[\"column_b\"] &gt; 5].\n\n\nBasic statistics\nA number of basic statistical functions can be applied to columns, e.g. df[\"column_a\"].max().\n\n\nAdding and removing columns\nAdd columns by pretending they’re already there and assigning into them, df[\"new_column\"] = ..., and remove them with df = df.drop(columns = [...]).\n\n\nSummaries and grouping\nUse df.groupby(\"variable_a\").agg(\"statistic_b\") to aggregate over your data.\n\n\nExporting\nUse df.to_csv(\"file_path\") to export your data\n\n\n\n\nNext session\nThanks for completing this introductory session to Python! You’re now ready for our next session, introductory visualisation, which looks at using the seaborn package for making visualisations.\nBefore you go, don’t forget to check out the Python User Group, a gathering of Python users at UQ.\nFinally, if you need any support or have any other questions, shoot us an email at training@library.uq.edu.au.\n\n\nResources\n\nOfficial pandas documentation\n\nGetting started\n10 Minutes to pandas\nUser guide\n\nMore visualisation modules:\n\nAltair\nBokeh\nVega\nMatplotlib\n\nOur compilation of useful Python links",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "2. Managing Data"
    ]
  },
  {
    "objectID": "Python/4-stats_and_further_vis/stats_and_vis.html",
    "href": "Python/4-stats_and_further_vis/stats_and_vis.html",
    "title": "Python training (4 of 4): Statistics and Further Visualisation",
    "section": "",
    "text": "TipUpcoming workshop(s) available!\n\n\n\nThe next workshop is on Fri Mar 20 at 09:30 AM.\nBook in to the next offering now.\nAlternatively, check our calendar for future events.\nThis session is aimed as an overview of how to perform some statistical modelling with Python. It is a Python workshop, not a statistics workshop - if you’d like to better understand the statistical models, or need help deciding what’s best for you, please consult a statistics resource or contact a statistician.\nIn this session, we’ll cover\nWe’ll use two new modules:",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "4. Statistics and Further Visualisation"
    ]
  },
  {
    "objectID": "Python/4-stats_and_further_vis/stats_and_vis.html#setup",
    "href": "Python/4-stats_and_further_vis/stats_and_vis.html#setup",
    "title": "Python training (4 of 4): Statistics and Further Visualisation",
    "section": "Setup",
    "text": "Setup\n\nSpyder version\nBefore we begin, please check which version of Spyder you’re using (you can see this in the Anaconda Navigator, or in Help &gt; About Spyder). If it’s less than 6, you should update Spyder before continuing. However, if you’re in a workshop, this will take too long - instead, use the following workaround if your plots don’t work:\nplt.show()\nMore on this later.\n\n\nModules and data\nLet’s import all our modules for today:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport scipy.stats as stats\nimport seaborn as sns\nimport statsmodels.formula.api as smf\n\nWe’ll be working from our “Players2024” dataset again. If you don’t have it yet,\n\nDownload the dataset.\nCreate a folder in in the same location as your script called “data”.\nSave the dataset there.\n\nTo bring it in and clean it up,\n\ndf = pd.read_csv(\"data/Players2024.csv\")\ndf = df[df[\"positions\"] != \"Missing\"]\ndf = df[df[\"height_cm\"] &gt; 100]",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "4. Statistics and Further Visualisation"
    ]
  },
  {
    "objectID": "Python/4-stats_and_further_vis/stats_and_vis.html#descriptive-statistics",
    "href": "Python/4-stats_and_further_vis/stats_and_vis.html#descriptive-statistics",
    "title": "Python training (4 of 4): Statistics and Further Visualisation",
    "section": "Descriptive Statistics",
    "text": "Descriptive Statistics\nWe’ll start with sample size. All dataframes have most descriptive statistics functions available right off the bat which we access via the . operator.\nTo calculate the number of non-empty observations in a column, say the numeric variable df[\"height_cm\"], we use the .count() method\n\ndf[\"height_cm\"].count()\n\nnp.int64(5932)\n\n\n\nMeasures of central tendancy\nWe can compute measures of central tendancy similarly. The average value is given by\n\ndf[\"height_cm\"].mean()\n\nnp.float64(183.04130141604855)\n\n\nthe median by\n\ndf[\"height_cm\"].median()\n\nnp.float64(183.0)\n\n\nand the mode by\n\ndf[\"height_cm\"].mode()\n\n0    185.0\nName: height_cm, dtype: float64\n\n\n\n.mode() returns a dataframe with the most frequent values as there can be multiple.\n\n\nVisualisation\nLet’s visualise our statistics as we go. We can start by producing a histogram of the heights with seaborn:\n\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarningIf your plots don’t appear…\n\n\n\n\n\n…AND you don’t have an error, then you might have a Spyder version with a bug.\nThe simplest workaround is to run plt.show() every time you make a plot. If you have time, you should update Spyder.\n\n\n\nWe can use matplotlib to annotate the locations of these statistics. Let’s save them into variables and then make the plot again. The important function is plt.vlines, which enables you to create vertical line(s) on your plot. We’ll do it once for each so that we get separate legend entries. We’ll need to provide the parameters\n\nx =\nymin =\nymax =\ncolors =\nlinestyles =\nlabel =\n\n\n# Save the statistics\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", linestyles = \"dashed\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", linestyles = \"solid\", label = \"Mode\")\n\n# Create the legend with the labels\nplt.legend()\n\n\n\n\n\n\n\n\n\n\n\nActivity 1: Measures of Central Tendency\nLet’s extend this visualisation to include the median and mode, as well as change the median’s linestyle to be “dotted”.\n\nVisualise the median and mode, like the average, with different colours.\nGo to the documentation for plt.vlines() to change the median’s linestyle.\n\nThen, try to use\n\nplt.legend() to remove the border on the legend.\nplt.xlabel() to clean up the \\(x\\)-label.\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe following code is one possible solution.\n\n# Save the statistics\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", label = \"Mode\")\n\n# Create the legend with the labels\nplt.legend(frameon = False)\nplt.xlabel(\"Height (cm)\")\n\nText(0.5, 9.066666666666652, 'Height (cm)')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTipIncluding mathematical symbols with \\(\\LaTeX\\).\n\n\n\n\n\nYou can tell matplotlib to execute certain \\(\\LaTeX\\) commands in the text you plot, enabling inclusion of mathematics in your plots. This is particularly useful for certain units.\nLet’s imagine we want to use \\(x_{height}\\) for the \\(x\\)-label. Using dollar signs,\nplt.xlabel(\"$x_{height}$ (cm)\")\n\n# Save the statistics\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", label = \"Mode\")\n\n# Create the legend with the labels\nplt.legend(frameon = False)\nplt.xlabel(\"$x_{height}$ (cm)\")\n\nText(0.5, 9.066666666666652, '$x_{height}$ (cm)')\n\n\n\n\n\n\n\n\n\n\\(\\LaTeX\\) commands use backslashes \\, and sometimes this conflicts with Python string’s escape characters. To avoid this, prefix your conflicting strings with R, as in\nplt.xlabel(R\"$x_{height}$ (m$\\times 10^{-2}$)\")\n\n# Save the statistics\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", label = \"Mode\")\n\n# Create the legend with the labels\nplt.legend(frameon = False)\nplt.xlabel(R\"$x_{height}$ (m$\\times 10^{-2}$)\")\n\nText(0.5, 9.066666666666652, '$x_{height}$ (m$\\\\times 10^{-2}$)')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasures of variance\nWe can also compute measures of variance. The minimum and maximum are as expected\n\ndf[\"height_cm\"].min()\ndf[\"height_cm\"].max()\n\nnp.float64(206.0)\n\n\nThe range is the difference\n\ndf[\"height_cm\"].max() - df[\"height_cm\"].min()\n\nnp.float64(46.0)\n\n\nQuantiles are given by .quantile(...) with the fraction inside. The inter-quartile range (IQR) is the difference between 25% and 75%.\n\nq1 = df[\"height_cm\"].quantile(0.25)\nq3 = df[\"height_cm\"].quantile(0.75)\nIQR = q3 - q1\n\nA column’s standard deviation and variance are given by\n\ndf[\"height_cm\"].std()\ndf[\"height_cm\"].var()\n\nnp.float64(46.7683158241558)\n\n\nAnd the standard error of the mean (SEM) with\n\ndf[\"height_cm\"].sem()\n\nnp.float64(0.08879229764682213)\n\n\nYou can calculate the skewness and kurtosis (variation of tails) of a sample with\n\ndf[\"height_cm\"].skew()\ndf[\"height_cm\"].kurt()\n\nnp.float64(-0.4338044567190438)\n\n\nAll together, you can see a nice statistical summary with\n\ndf[\"height_cm\"].describe()\n\ncount    5932.000000\nmean      183.041301\nstd         6.838736\nmin       160.000000\n25%       178.000000\n50%       183.000000\n75%       188.000000\nmax       206.000000\nName: height_cm, dtype: float64\n\n\n\n\nActivity 2: Inter-quartile range\nLet’s take our previous visualisation and shade in the IQR using the function plt.fill_between(). You’ll want to complete this activity in a few steps:\n\nCopy the code from the previous plot\nSave the quartiles Q1 and Q3 in variables\nRead the documentation for plt.fill_between()\nTry to use it.\n\nYou’ll first want to use the function with the following parameters\n\nx =\ny1 =\ny2 =\n\ni.e.,\nplt.fill_between(x = ..., y1 = ..., y2 = ...)\nOnce you’ve got it working, try adding the following two - alpha = (for the opacity, a number between 0-1) - label = (for the legend)\n\n\n\n\n\n\nTipHint\n\n\n\nThe documentation for plt.fill_between() specifies that\n\nx should be array-like. For us, this means a list.\ny1 and y2 should be array-like or float. This means either a list or a single decminal number.\n\n\n\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\nThe following code is one possible solution.\n# Save the quartiles\nheight_Q1 = df[\"height_cm\"].quantile(0.25)\nheight_Q3 = df[\"height_cm\"].quantile(0.75)\n\n# Shade in the IQR\nplt.fill_between(x = [height_Q1, height_Q3], y1 = 0, y2 = 500, alpha = 0.2, label = \"IQR\")\nAll together with the original plot, this looks like\n\n# Save the statistics\nheight_tot = df[\"height_cm\"].count()\nheight_avg = df[\"height_cm\"].mean()\nheight_med = df[\"height_cm\"].median()\nheight_mod = df[\"height_cm\"].mode()\n\n# Save the quartiles\nheight_Q1 = df[\"height_cm\"].quantile(0.25)\nheight_Q3 = df[\"height_cm\"].quantile(0.75)\n\n# Make the histogram\nsns.displot(df, x = \"height_cm\", binwidth = 1)\n\n# Annotate the plot with vertical and horizontal lines\nplt.vlines(x = height_avg, ymin = 0, ymax = 500, colors = \"r\", linestyles = \"dashed\", label = \"Average\")\nplt.vlines(x = height_med, ymin = 0, ymax = 500, colors = \"k\", linestyles = \"dotted\", label = \"Median\")\nplt.vlines(x = height_mod, ymin = 0, ymax = 500, colors = \"orange\", linestyles = \"solid\", label = \"Mode\")\n\n# Shade in the IQR\nplt.fill_between(x = [height_Q1, height_Q3], y1 = 0, y2 = 500, alpha = 0.2, label = \"IQR\")\n\n# Create the legend with the labels\nplt.legend()",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "4. Statistics and Further Visualisation"
    ]
  },
  {
    "objectID": "Python/4-stats_and_further_vis/stats_and_vis.html#inferential-statistics",
    "href": "Python/4-stats_and_further_vis/stats_and_vis.html#inferential-statistics",
    "title": "Python training (4 of 4): Statistics and Further Visualisation",
    "section": "Inferential Statistics",
    "text": "Inferential Statistics\nInferential statistics requires using the module scipy.stats.\n\nSimple linear regressions\nLeast-squares regression for two sets of measurements can be performed with the function stats.linregress()”\n\nstats.linregress(x = df[\"age\"], y = df[\"height_cm\"])\n\nLinregressResult(slope=np.float64(0.025827494764561896), intercept=np.float64(182.38260451315895), rvalue=np.float64(0.01682597901197298), pvalue=np.float64(0.19506275453364344), stderr=np.float64(0.019930266529602007), intercept_stderr=np.float64(0.5159919571772644))\n\n\nIf we store this as a variable, we can access the different values with the . operator. For example, the p-value is\n\nlm = stats.linregress(x = df[\"age\"], y = df[\"height_cm\"])\nlm.pvalue\n\nnp.float64(0.19506275453364344)\n\n\n\nVisualisation\nLet’s look at implementing the linear regression into our scatter plot from before. Using the scatterplot from before,\n\nsns.relplot(df, x = \"age\", y = \"height_cm\")\n\n\n\n\n\n\n\n\nwe’ll need to plot the regression as a line. For reference,\n\\[ y = \\text{slope}\\times x + \\text{intercept}\\]\nSo\n\nsns.relplot(df, x = \"age\", y = \"height_cm\")\n\n# Construct the linear regression\nx_lm = df[\"age\"]\ny_lm = lm.slope*x_lm + lm.intercept\n\n# Plot the line plot\nsns.lineplot(x = x_lm, y = y_lm, color = \"r\")\n\n\n\n\n\n\n\n\nFinally, we can include the details of the linear regression in the legend by specifying them in the label. We’ll need to round() them and str() them (turn them into strings) so that we can include them in the message.\n\nsns.relplot(df, x = \"age\", y = \"height_cm\")\n\n# Construct the linear regression\nx_lm = df[\"age\"]\ny_lm = lm.slope*x_lm + lm.intercept\n\n# Round and stringify the values\nslope_rounded = str(round(lm.slope, 2))\nintercept_rounded = str(round(lm.intercept, 2))\n\n# Plot the line plot\nlinreg_label = \"Linear regression with\\nslope = \" + slope_rounded + \"\\nintercept = \" + intercept_rounded\nsns.lineplot(x = x_lm, y = y_lm, color = \"r\", label = linreg_label)\n\n\n\n\n\n\n\n\n\n\n\n\\(t\\)-tests\nWe can also perform \\(t\\)-tests with the scipy.stats module. Typically, this is performed to examine the statistical signficance of a difference between two samples’ means. Let’s examine whether that earlier groupby result for is accurate for heights, specifically, are goalkeepers taller than non-goalkeepers?\nThe function stats.ttest_ind() requires us to send in the two groups as separate columns, so we’ll need to do a bit of reshaping.\nLet’s start by creating a new variable for goalkeeper status, and then separate the goalkeepers from the non-goalkeepers in two variables\n\ndf[\"gk\"] = df[\"positions\"] == \"Goalkeeper\"\n\ngoalkeepers = df[df[\"gk\"] == True]\nnon_goalkeepers = df[df[\"gk\"] == False]\n\nThe \\(t\\)-test for the means of two independent samples is given by\n\nstats.ttest_ind(goalkeepers[\"height_cm\"], non_goalkeepers[\"height_cm\"])\n\nTtestResult(statistic=np.float64(35.2144964816995), pvalue=np.float64(7.551647917141636e-247), df=np.float64(5930.0))\n\n\nYielding a p-value of \\(8\\times 10^{-247}\\approx 0\\), indicating that the null-hypothesis (heights are the same) is extremely unlikely.\n\n\nActivity 2: Visualisation\nWe can also visualise these results with boxplots, showing the distributions and their statistical summary. Go to the seaborn documentation for sns.catplot() to try figure out how.\n\n\n\n\n\n\nNoteSolution\n\n\n\n\n\n\nsns.catplot(df, x = \"gk\", y = \"height_cm\", kind = \"box\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nMore complex modelling\nIf you need to do more advanced statistics, particularly if you need more regressions, you’ll likely need to turn to a different package: statsmodels. It is particularly useful for statistical modelling.\nWe’ll go through three examples\n\nSimple linear regressions (like before)\nMultiple linear regressions\nLogistic regressions\n\nWhat’s nice about statsmodels is that it gives an R-like interface and summaries.\n\nSimple linear regressions revisited\nLet’s perform the same linear regression as before, looking at the “age” and “height variables”. Our thinking is that players’ heights dictate how long they can play, so we’ll make \\(x = \\text{height}\\) and \\(y = \\text{age}\\).\nThe first step is to make the set up the variables. We’ll use the function smf.ols() for ordinary least squares. It takes in two imputs:\n\nThe formula string, in the form y ~ X1 + X2 ...\nThe data\n\nWe create the model and compute the fit\n\nmod = smf.ols(\"height_cm ~ age\", df)\nres = mod.fit()\n\nDone! Let’s take a look at the results\n\nres.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nheight_cm\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.679\n\n\nDate:\nTue, 20 Jan 2026\nProb (F-statistic):\n0.195\n\n\nTime:\n15:46:47\nLog-Likelihood:\n-19821.\n\n\nNo. Observations:\n5932\nAIC:\n3.965e+04\n\n\nDf Residuals:\n5930\nBIC:\n3.966e+04\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n182.3826\n0.516\n353.460\n0.000\n181.371\n183.394\n\n\nage\n0.0258\n0.020\n1.296\n0.195\n-0.013\n0.065\n\n\n\n\n\n\n\n\nOmnibus:\n86.537\nDurbin-Watson:\n1.995\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n56.098\n\n\nSkew:\n-0.098\nProb(JB):\n6.58e-13\n\n\nKurtosis:\n2.566\nCond. No.\n151.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nThat’s a lot nicer than with scipy. We can also make our plot from before by getting the model’s \\(y\\) values with res.fittedvalues\n\nsns.relplot(data = df, x = \"age\", y = \"height_cm\")\nsns.lineplot(x = df[\"age\"], y = res.fittedvalues, color = \"r\")\n\n\n\n\n\n\n\n\n\n\nGeneralised linear models\nThe statsmodels module has lots of advanced statistical models available. We’ll take a look at one more: Generalised Linear Models. The distributions they include are\n\nBinomial\nPoisson\nNegative Binomial\nGaussian (Normal)\nGamma\nInverse Gaussian\nTweedie\n\nWe’ll use the binomial option to create logistic regressions.\nLogistic regressions examine the distribution of binary data. For us, we can compare the heights of goalkeepers vs non-goalkeepers again. Let’s convert our gk column from True \\(\\rightarrow\\) 1 and False \\(\\rightarrow\\) 0 by converting to an int:\n\ndf[\"gk\"] = df[\"gk\"].astype(int)\n\nNow, we can model this column with height. Specifically,\n\\[ \\text{gk} \\sim \\text{height}\\]\nStart by making the model with the function smf.glm(). We need to specify the family of distributions; they all live in sm.families, which comes from a different submodule that we should import:\n\nimport statsmodels.api as sm\nmod = smf.glm(\"gk ~ height_cm\", data = df, family = sm.families.Binomial())\n\nNext, evaluate the results\n\nres = mod.fit()\n\nLet’s have a look at the summary:\n\nres.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ngk\nNo. Observations:\n5932\n\n\nModel:\nGLM\nDf Residuals:\n5930\n\n\nModel Family:\nBinomial\nDf Model:\n1\n\n\nLink Function:\nLogit\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-1583.5\n\n\nDate:\nTue, 20 Jan 2026\nDeviance:\n3167.0\n\n\nTime:\n15:46:48\nPearson chi2:\n4.02e+03\n\n\nNo. Iterations:\n7\nPseudo R-squ. (CS):\n0.1879\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-53.2336\n1.927\n-27.622\n0.000\n-57.011\n-49.456\n\n\nheight_cm\n0.2745\n0.010\n26.938\n0.000\n0.255\n0.294\n\n\n\n\n\nFinally, we can plot the result like before\n\nsns.relplot(data = df, x = \"height_cm\", y = \"gk\")\nsns.lineplot(x = df[\"height_cm\"], y = res.fittedvalues, color = \"black\")\n\n\n\n\n\n\n\n\n\n\n\nActivity 3: Digging deeper\nWe’ve come to the end of the series, so to conclude we’ll finish up with an open-ended activity. Like in workshop 2, download the gapminder dataset.\nThen, spend the remaining time analysing and visualising the data! If you’re stuck, see if you can produce a visualisation and a statistic that show the same thing.\nDon’t forget some data transformation tips:\n\nUse gapminder = df.read_csv(...) to load the data\nUse gapminder.columns to see the columns\nUse gapminder[\"column_name\"] to pick out an individual column\nUse gapminder[gapminder[\"column_name] == ...] to filter for a specific value",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "4. Statistics and Further Visualisation"
    ]
  },
  {
    "objectID": "Python/4-stats_and_further_vis/stats_and_vis.html#conclusion",
    "href": "Python/4-stats_and_further_vis/stats_and_vis.html#conclusion",
    "title": "Python training (4 of 4): Statistics and Further Visualisation",
    "section": "Conclusion",
    "text": "Conclusion\nPython definitely has powerful tools for statistics and visualisations! If any of the content here was too challenging, you have other related issues you’d like to discuss or would simply like to learn more, we the technology training team would love to hear from you. You can contact us at training@library.uq.edu.au.\nHere’s a summary of what we’ve covered\n\n\n\n\n\n\n\nTopic\nDescription\n\n\n\n\nDescriptive statistics\nUsing built-in methods to pandas series (via df[\"variable\"].___ for a dataframe df) we can apply descriptive statistics to our data.\n\n\nUsing matplotlib to include statistical information\nUsing plt.vlines() and plt.fill_between(), we can annotate the plots with lines showing statistically interesting values.\n\n\nInferential statistics\nUsing the scipy.stats and statsmodels modules, we can perform statistical tests and modelling.\n\n\n\n\nResources\n\nOfficial scipy.stats documentation\nOfficial statsmodels documentation\nOfficial seaborn documentation\nOfficial matplotlib documentation\nOur compilation of useful Python links",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "4. Statistics and Further Visualisation"
    ]
  },
  {
    "objectID": "Python/useful_links.html",
    "href": "Python/useful_links.html",
    "title": "Further resources for Python users",
    "section": "",
    "text": "Python data science handbook by Jake VanderPlas\nScipy Lecture Notes to learn numerical computing, data science and plotting with Python\n10 Minutes to pandas, an introduction to pandas’ capabilities\nProgramming with Python and Plotting and Programming in Python, two lessons by The Carpentries\n\n\n\n\n\nOfficial Python 3 Tutorial (for users already familiar with programming concepts)\nPython for Everybody (PY4E), by Charles R. Severance (no programming background needed)\nPython courses on LinkedIn Learning (use your UQ credentials to access the full catalogue)\nPython Programming on WikiBooks\nThe Hitchhiker’s Guide to Python",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "Further resources"
    ]
  },
  {
    "objectID": "Python/useful_links.html#books-and-tutorials",
    "href": "Python/useful_links.html#books-and-tutorials",
    "title": "Further resources for Python users",
    "section": "",
    "text": "Python data science handbook by Jake VanderPlas\nScipy Lecture Notes to learn numerical computing, data science and plotting with Python\n10 Minutes to pandas, an introduction to pandas’ capabilities\nProgramming with Python and Plotting and Programming in Python, two lessons by The Carpentries\n\n\n\n\n\nOfficial Python 3 Tutorial (for users already familiar with programming concepts)\nPython for Everybody (PY4E), by Charles R. Severance (no programming background needed)\nPython courses on LinkedIn Learning (use your UQ credentials to access the full catalogue)\nPython Programming on WikiBooks\nThe Hitchhiker’s Guide to Python",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "Further resources"
    ]
  },
  {
    "objectID": "Python/useful_links.html#documentation",
    "href": "Python/useful_links.html#documentation",
    "title": "Further resources for Python users",
    "section": "Documentation",
    "text": "Documentation\n\nOfficial Python 3 documentation",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "Further resources"
    ]
  },
  {
    "objectID": "Python/useful_links.html#questions-and-answers",
    "href": "Python/useful_links.html#questions-and-answers",
    "title": "Further resources for Python users",
    "section": "Questions and answers",
    "text": "Questions and answers\n\nPython Questions and Answers on StackOverflow",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "Further resources"
    ]
  },
  {
    "objectID": "Python/useful_links.html#practice-python",
    "href": "Python/useful_links.html#practice-python",
    "title": "Further resources for Python users",
    "section": "Practice Python",
    "text": "Practice Python\n\nExercism’s Python track\nPython Challenge, an interactive riddle\nPractice Python with beginner exercises\nCodingBat’s Python problems",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "Further resources"
    ]
  },
  {
    "objectID": "Python/useful_links.html#at-uq",
    "href": "Python/useful_links.html#at-uq",
    "title": "Further resources for Python users",
    "section": "At UQ",
    "text": "At UQ\n\nSee the next sessions at the Library\nJoin the monthly UQ Python User Group (UQPUG) to collaborate and share with other Python users\nAsk questions to other researchers during the weekly Hacky Hour\nContact your unit’s statistician",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "Further resources"
    ]
  },
  {
    "objectID": "Python/useful_links.html#training-outside-the-library",
    "href": "Python/useful_links.html#training-outside-the-library",
    "title": "Further resources for Python users",
    "section": "Training outside the Library",
    "text": "Training outside the Library\n\nFind more training providers, at and outside UQ, in our “Training Elsewhere” page",
    "crumbs": [
      "Home",
      "![](/images/python.svg){width=20} Python",
      "Further resources"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "",
    "text": "This is an intermediate level tutorial. Before completing this tutorial, we recommend our QGIS: Introduction to Mapping tutorial. This tutorial is designed for QGIS 3.40. If you need to install it on your computer, go to the QGIS website.\nWe will start as always by creating a good folder structure to work within. This folder is where our project, our data, and creations will live. Folder structure is very important for keeping your data tidy, as well as for ease of sharing your project with others. You simply need to zip the project folder if you need to share the whole thing.\n\nOpen QGIS and create a new project with Project &gt; New.\nLet’s now save our project: Project &gt; Save.\nCreate a new folder, let’s call it “qgis_fieldwork”.\nInside that folder, create these folders:\n\n“data” - for all the data we will use to make our maps, split into:\n\n“raw” - raw data from your research or the internet\n“processed” - any data you’ve modified\n\n“output” - for any maps or images we export\n“DCIM” - this is a new one, it’s for storing images from our phone!\n“UQ_Fieldwork” - this is also new, it’s for exporting our project to QField\n\nFinally, let’s save our .qgz project file here, named “qgis_fieldwork_map.qgz”\n\nYour .qgz file should always be in the highest level folder, so it’s only looking down into folders for data, not back out.\n\nThis might seem unnecessary now, but things quickly get out of control and hard to find if you don’t have a good folder structure.\n\n\nQField plugs directly in to QGIS, and allows you to link a QGIS project with your mobile phone for data collection.\nAndroid Download iOS Download\n\n\n\n\n\n\nNoteYou can also explore Avenza, which uses GeoPDFs\n\n\n\n\n\nFor Avenza Maps you just need to add a GeoPDF and you can collect data on it. It’s less functional than QField, but it’s a great fallback tool. Android Download iOS Download\nThere are alternatives out there, such as Input (MerginMaps).",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#install-qfield",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#install-qfield",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "",
    "text": "QField plugs directly in to QGIS, and allows you to link a QGIS project with your mobile phone for data collection.\nAndroid Download iOS Download\n\n\n\n\n\n\nNoteYou can also explore Avenza, which uses GeoPDFs\n\n\n\n\n\nFor Avenza Maps you just need to add a GeoPDF and you can collect data on it. It’s less functional than QField, but it’s a great fallback tool. Android Download iOS Download\nThere are alternatives out there, such as Input (MerginMaps).",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#dem",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#dem",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "DEM",
    "text": "DEM\nA Digital Elevation Model (DEM) is a common example of raster data, i.e. grid data that contains a value in each cell (a bit like the pixels in a coloured picture).\nFor this tutorial, we are using a DEM sourced from ELVIS - Geoscience Australia’s ELeVation Information System.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#aerial-imagery",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#aerial-imagery",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Aerial Imagery",
    "text": "Aerial Imagery\nThere are a few places you can acquire aerial photography.\n\nLocal Raster Files\nAs a UQ student, you have access to very high resolution imagery from NearMap. You can activate an account here to download georeferenced aerial imagery. You can even access an array of imagery going back in time to around 2010.\nWe won’t use NearMap today, but you’re welcome to explore it.\n\n\nMapserver Raster Files\nToday we’re going to use a World Imagery XYZ Tile from ESRI.\n\nScroll down the Browser panel until your see XYZ Tiles\nRight click XYZ Tiles and select New Connection...\n\nIn Name type ESRI World Imagery\nIn URL paste:\nhttps://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\nIncrease the Max. Zoom Level to 20\n\nThis value depends on what is available from the given service in different parts of the world. Increasing that value beyond 20 for this map in Brisbane will show “Map data not yet available” when you zoom in very close.\n\nClick OK\n\nIn the Browser panel, expand XYZ Tiles and double click on ESRI World Imagery",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#lot-plans",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#lot-plans",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Lot Plans",
    "text": "Lot Plans\nYou can access a wide variety of QLD Government Data, including Spatial Data such as lot plans and vegetation maps, from QLD Spatial.\nToday we have extracted a selection from Property boundaries Queensland\nThere are three ways to access data from QSpatial.\n\nDownload all of the data from a layer\nSelect a portion of a layer for download using the My List function\nLive load it into your project using a Mapserver.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#buffer-the-lines",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#buffer-the-lines",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Buffer the Lines",
    "text": "Buffer the Lines\n\nGo to Vector &gt; Geoprocessing Tools &gt; Buffer\nChoose “Lines” as the Input Layer\nSet the Distance to 10 metres\n\nWe can also make the buffer squared if we want, but we will keep it with the rounded default\n\nTick the Dissolve result box\n\nThis will merge the polygons from our output, making for a cleaner look. The Dissolve tool also exists as a standalone tool.\n\nClick the three dots ... next to the Buffered field, navigate to the project’s processed data folder, and type in UQ_Boundary_Buffer and click Save\nClick Run",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#explore-projections",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#explore-projections",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Explore Projections",
    "text": "Explore Projections\nHopefully it’s clear why we needed to convert from a Geographic Coordinate System to a Projected Coordinate System, but why is it important that we chose the local projection EPSG:23856 - GDA94 / MGA zone 56?\nLet’s see what would happen if we did our buffer on those lines in a different projection.\n\nReproject Data\n\nGo to Vector &gt; Data Management Tools &gt; Reproject Layer...\nChoose the Lines in Input layer\nSet the Target CRS to EPSG:3857 - WGS84 / Pseudo-Mercator\n\nThis is a very common projection that Google and your phones use\n\nClick Run\n\nWe don’t need to permanently save this, so we can leave it as a temporary layer\n\n\n\n\nBuffer Reprojected Lines\n\nGo to Vector &gt; Geoprocessing Tools &gt; Buffer\nChoose “Reprojected” as the Input Layer\nSet the Distance to 10 metres\nClick Run\n\nMove the new Buffered layer order so that it draws over the UQ_Boundar_Buffer layer. Notice anything interesting?\nThe new Buffered layer is about 1m smaller!\nThis is because EPSG:3857 - WGS84 / Pseudo-Mercator is a global projection that tries to preserve shapes. Although the units are in metres, the scaling factor changes as you move away from the equator to account for the curvature of the Earth.\nIt can still be useful to use this projection to take points. But you have to be careful when you’re making measurements and calculations, for those, a local projection is usually better.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#style-the-boundary-buffer-polygon",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#style-the-boundary-buffer-polygon",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Style the Boundary buffer polygon",
    "text": "Style the Boundary buffer polygon\n\nOpen the Layer Styling panel by pressing F7 (or fn+F7)\nSelect the UQ_Boundar_Buffer layer from either the Layer Styling panel, or the Layers panel.\nUnder Fill, click Simple Fill\n\nClick the dropdown next to Fill color and select a colour that reminds us not to go there, like orange\n\nChange the Opacity to something around 30%\n\n\n\n\nSave your project\nIt’s not essential at this stage, it’s just a good reminder to save your work regularly so you don’t lose things when things go wrong.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#add-another-field",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#add-another-field",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Add Another Field",
    "text": "Add Another Field\nIt would be nice to be able to take photos when we collect our data, let’s add another field.\n\nDouble click on the new fieldwork_data layer\nClick on the Fields tab\nClick the yellow pencil button to Toggle Editing Mode\nClick the now visible yellow New field button\n\nName: Photo\nType: Text (string)\nClick OK\n\nClick the yellow pencil button to Toggle Editing Mode\nClick Save\nClick OK",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#attributes-form-preparation",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#attributes-form-preparation",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Attributes Form Preparation",
    "text": "Attributes Form Preparation\nNow that we have a layer with all the desired fields, we can now edit the layer form to customise how things are entered.\n\nDouble click on the new collected_data layer\nClick on the Attributes Form tab\nAt the very top of the window, click Autogenerate and change it to Drag and Drop Designer\n\nOn the left you will see all of our Fields, we can drag and drop these into the Form Layout section - this section lays out how our form will look when we enter new data.\n\nDrag and Drop the Photo Field to the Form Layout so that it is sitting above DateTime\nClick fid and press the red minus button - we want to keep the field, but don’t need to see this in our form\nClick ID\n\nUnder Constraints tick Not Null and Enforce not null constraint\nWe don’t need to change anything else for ID, we just want to make it mandatory\n\nClick Category\n\nUnder Widget Type click Text Edit and change it to Value Map\nAdd the following to Values and Descriptions:\n\nEX and Excellent\nGD and Good\nPR and Poor\nOT and Other\n\nIt is good practice to have an Other field if you’re using a “Not Null” constraint, as it allows the user to note when something doesn’t meet the given Categories.\n\n\nUnder Constraints tick Not Null and Enforce not null constraint\n\nClick Photo\n\nUnder Widget Type click Text Edit and change it to Attachment\nUnder Path change Default path to @project_home + ‘/DCIM’\n\nChange Absolute Path to Relative to Project Path\n\nTick **Use a hyperlink for document path (read-only)\nUnder Integrated Document Viewer change Type from No Content to Image\n\nClick DateTime\n\nUnder General untick Editable - we don’t want to edit this field\nScroll to the bottom, under Defaults change the Default value to $now\nIf this worked, there will be a date and time next to Preview\n\nClick Lat\n\nUnder General untick Editable - we don’t want to edit this field\nScroll to the bottom, under Defaults change the Default value to $y\nThe Preview will show NULL, but that’s okay, we don’t have data yet\n\nClick Long - Under General untick Editable - we don’t want to edit this field - Scroll to the bottom, under Defaults change the Default value to $x\n\nClick OK\n\nWe should now have a functional form, let’s test it in QGIS before we send it to our phones\n\nClick the yellow pencil button to Toggle Editing on\nClick the green dots (or press Ctrl + .) to Add Point Feature\nClick on the map to add a point\nThe Name should be mandatory, the Description editable, Photo have an add option, DateTime should have the current date and time, and Lat and Long should have numbers like “-27.494066205142065” and “153.0153703956312”\nClick Cancel\nClick the yellow pencil button to Toggle Editing off",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#tracks-attributes-form-preparation",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#tracks-attributes-form-preparation",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Tracks Attributes Form Preparation",
    "text": "Tracks Attributes Form Preparation\nLet’s prepare the tracks DateTime as we did with the collected_data - Double click on the new tracks layer - Click on the Attributes Form tab - At the very top of the window, click Autogenerate and change it to Drag and Drop Designer\nOn the left you will see all of our Fields, we can drag and drop these into the Form Layout section - this section lays out how our form will look when we enter new data.\n\nClick fid and press the red minus button - we want to keep the field, but don’t need to see this in our form\nClick DateTime\n\nUnder General untick Editable - we don’t want to edit this field\nScroll to the bottom, under Defaults change the Default value to $now\nIf this worked, there will be a date and time next to Preview\n\nClick OK",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#install-qfield-plugin",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#install-qfield-plugin",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Install QField Plugin",
    "text": "Install QField Plugin\n\nNavigate to Plugins &gt; Manage and Install Plugins...\nSearch for “QField”\nClick on QField Sync\nClick Install Plugin\nOnce it has installed, click Close\n\nYou can now access the QField Sync Plugin through the menu.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#prepare-data-for-qfield",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#prepare-data-for-qfield",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Prepare Data for QField",
    "text": "Prepare Data for QField\nAt this point we need to clean up our Layers. You don’t have to remove everything, but it’s worth deciding what layers you want to remove, and what scratch layers you want to make permanent.\n\nHide any layers you want to keep, but don’t want to export to QField\nMake sure your layer order makes sense, I recommend the following layers visible in this order:\n\ncollected_data\ntracks\nUQ_Boundary_Buffer\nUQ_Contours_5m\nUQ_DEM_1m\nUQ_Hillshade\nWorld Imagery Tile",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#export-to-qfield",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#export-to-qfield",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Export to QField",
    "text": "Export to QField\n\nNavigate to Plugins &gt; QFieldSync &gt; Package for QField\nIf you get a Message window popup, click Next\nGive your project an appropriate title UQ_Fieldwork\nUnder Packaged Project Filename navigate to your UQ_Fieldwork folder and click Save\nUnder Advanced untick every folder except DCIM\n\nWe want a copy of this folder for our photos to be saved to. The rest of our data will be packaged into the QField folder.\n\nAt the bottom of the window click Configure current project...\n\nClick the Cable Export tab (if we were using QFieldCloud, we would click that option now instead)\nClick the Toggle Layers eye symbol on the right\n\nIn the Action column change Copy to Remove from project for any layers you don’t want to copy to QField.\n\nScroll down and tick Geofencing\n\nSet Geofencing areas layer to UQ_Boundary_Buffer\n\nClick OK\n\nClick Create\n\nWe now need to zip the QField folder and send it to our phone!\n\nNavigate to your Project folder, right click on the UQ_Fieldwork folder and compress it to a zip file.\nSend this file to your phone via email, cloud, or using a cable.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#opening-the-qfield-project",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#opening-the-qfield-project",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Opening the QField Project",
    "text": "Opening the QField Project\nAndroid - Open QField - Skip any popups - Tap Open local file - Tap the green plus button in the bottom right - Tap Import project from ZIP - Navigate to your zip file. In my case, I had to go to the top left menu, and tap on Downloads - The file should open in QField. You may need to tap on the outer folder, and then the Project itself.\niOS - Open the zip file and extract it to: On My iPhone/QField/Imported Projects - Open QField - Skip any popups - Tap Open local file - Tap Imported projects - Open the UQ_Fieldwork folder until you reach the UQ_Fieldwork Project file. - The project should open in QField.\nYou can now access your map.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#using-qfield",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#using-qfield",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Using QField",
    "text": "Using QField\n\nTap the menu in the top right to see your layers.\nTap the eye to hide or unhide layers\nIf you tap on a layer, and then tap the pencil icon in the bottom right, you can edit a layer.\nLet’s do this for our data_collected layer\nTap on the map on the right to return to the map, your cursor will now be a cross-hair\nNavigate around and then press the green + button to add a point\nFill out the details, and then tap the tick in the top left.\nYou’ve collected data!\n\n\nTurning on Tracks\n\nTap the menu in the top right to see your layers.\nPress and hold on your Tracks layer\n\nWhen the new menu appears, tap Setup tracking\nHere you can choose the interval for when a new point will be placed for your tracks\n\nAs we’re inside and may not move much, turn on Time requirement and set a short Minimum time of 15 seconds\nTap Start tracking\n\nIf you have your phone’s GPS on, and you zoom in on yourself, your should see a tracking line appear.\n\n\n\n\nTurning off Tracks\n\nPress and hold on your Tracks layer\n\nWhen the new menu appears, tap Stop tracking\n\n\nTo export your data back to QGIS, you can either copy the whole project folder in your phones storage back to your computer, or:\n\nGo to the menu\nTap the folder with a cog on it\nTap the three dots next to your data_collected.gpkg file\nSend it to your email\n\nThere’s more to explore, such as the very very helpful QFieldCloud, and I encourage you to do so.\nIf you have any questions, reach out to the UQ Library Training Team at training@library.uq.edu.au",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#exporting-to-avenza-maps",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#exporting-to-avenza-maps",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "Exporting to Avenza Maps",
    "text": "Exporting to Avenza Maps\n\n\n\n\n\n\nNote\n\n\n\n\n\nClick on Show Layout Manager in the toolbar or use Project &gt; Layout Manager. Create a new layout called “Avenza”. We can now see the Layout window.\nNormally we would add many elements to our layout if we were exporting it for print such as the map, a legend, a scale bar, a north arrow…\nIn this case however, we are simply interested in our map. Let’s add the map to the canvas:\n\nGo to the Layout tab, scroll down to ‘Resize Layout to Content’, click ‘Resize layout’\nBefore we export, let’s turn off any layers we aren’t using in QGIS to save space\nClick the Refresh View button up the top\nNow we are ready to export.\nGo to ‘Layout &gt; Export as PDF…’ and save your map.\nThe ‘PDF Export Options’ window will open\nTick the ‘Create Geospatial PDF (GeoPDF)’ box\nClick ‘Save’\n\nYou can repeat this process with the DEM and Hillshade to export out another kind of map.\nNow you simply need to export the pdf file(s) to your phone. You can email it, send it through the cloud, or transfer it using a cable.\nWhen you first open Avenza Maps it will ask you to create an account, but you can import up to three maps without doing so, you can avoid creating an account by selecting the ‘x’ in the top right corner. * Allow Avenza Maps to access your device location * Select the orange ‘+’ in the bottom right and select ‘Download or import a map’ * Choose ‘From Storage Locations’ (if requested, give Avenza Maps the permissions to access your files) * Do the same for the other map, if you created one. * Once it has been imported, tap on the map. * You can now move the map around with your finger, and pinching to zoom. * Tapping the placemark icon in the bottom left will add a placemark in the middle of the crosshairs. * Tapping the 3 dots in the bottom right will allow you to add GPS tracking and draw and measure distances.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/fieldwork/QGIS_fieldwork.html#autoid",
    "href": "QGIS/fieldwork/QGIS_fieldwork.html#autoid",
    "title": "QGIS: Fieldwork Data Collection",
    "section": "AutoID",
    "text": "AutoID\nWe can use some fancy code to make the ID automated\n\nDouble click on the new collected_data layer\n\nClick on the Attributes Form tab\nClick ID\n\nUnder General untick Editable - we don’t want to edit an automated field\nUnder Defaults click the Expression Builder button\n\nPaste in this code:\n\n\n\n\nconcat(\n    \"cat\",\n    '-',\n    lpad(\n        to_string(\n            coalesce(\n                aggregate(\n                    'DataCollection',        -- layer name\n                    'max',                    -- aggregate function\n                    to_int(\n                        regexp_substr(\"ID\", '\\\\d+')\n                    ),                        -- numeric part of auto_id\n                    \"cat\" = attribute(@parent, 'cat')\n                ) + 1,\n                1\n            )\n        ),\n        3,\n        '0'\n    )\n)",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "5. Fieldwork Data Collection"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html",
    "href": "QGIS/raster/QGIS_raster.html",
    "title": "QGIS: raster analysis",
    "section": "",
    "text": "This is an intermediate level tutorial. Before completing this tutorial, we recommend our QGIS: Introduction to Mapping tutorial. This tutorial is designed for QGIS 3.40. If you need to install it on your computer, go to the QGIS website.\nWe will start by creating a good folder structure to work within. This folder is where our project, our data, and creations will live. Folder structure is very important for keeping your data tidy, as well as for ease of sharing your project with others. You simply need to zip the project folder if you need to share the whole thing.\n\nOpen QGIS and create a new project with Project &gt; New.\nLet’s now save our project: Project &gt; Save.\nCreate a new folder, let’s call it “qgis_raster”.\n\n\n\n\nInside that folder, create these folders:\n\n“data” - for all the data we will use to make our maps, split into:\n\n“raw” - raw data from your research or the internet\n“processed” - any data you’ve modified\n\n“output” - for any maps or images we export\n“temp” - this isn’t necessary, but when you’re playing around and testing, it stops things getting messy.\n\nFinally, let’s save our .qgz project file here, named “qgis_raster_map.qgz”\n\n\n\n\n\n\n\n\n\nYour .qgz file should always be in the highest level folder, so it’s only looking down into folders for data, not back out.\nThis might seem necessary now, but things quickly get out of control and hard to find if you don’t have a good folder structure.\nLet’s finally add an OpenStreetMap basemap to locate ourselves on the globe:\n\nBrowser panel &gt; XYZ Tiles &gt; OpenStreetMap (double-click, or drag and drop into the Layers panel).",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#setting-up",
    "href": "QGIS/raster/QGIS_raster.html#setting-up",
    "title": "QGIS: raster analysis",
    "section": "",
    "text": "This is an intermediate level tutorial. Before completing this tutorial, we recommend our QGIS: Introduction to Mapping tutorial. This tutorial is designed for QGIS 3.40. If you need to install it on your computer, go to the QGIS website.\nWe will start by creating a good folder structure to work within. This folder is where our project, our data, and creations will live. Folder structure is very important for keeping your data tidy, as well as for ease of sharing your project with others. You simply need to zip the project folder if you need to share the whole thing.\n\nOpen QGIS and create a new project with Project &gt; New.\nLet’s now save our project: Project &gt; Save.\nCreate a new folder, let’s call it “qgis_raster”.\n\n\n\n\nInside that folder, create these folders:\n\n“data” - for all the data we will use to make our maps, split into:\n\n“raw” - raw data from your research or the internet\n“processed” - any data you’ve modified\n\n“output” - for any maps or images we export\n“temp” - this isn’t necessary, but when you’re playing around and testing, it stops things getting messy.\n\nFinally, let’s save our .qgz project file here, named “qgis_raster_map.qgz”\n\n\n\n\n\n\n\n\n\nYour .qgz file should always be in the highest level folder, so it’s only looking down into folders for data, not back out.\nThis might seem necessary now, but things quickly get out of control and hard to find if you don’t have a good folder structure.\nLet’s finally add an OpenStreetMap basemap to locate ourselves on the globe:\n\nBrowser panel &gt; XYZ Tiles &gt; OpenStreetMap (double-click, or drag and drop into the Layers panel).",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#install-the-saga-plugin",
    "href": "QGIS/raster/QGIS_raster.html#install-the-saga-plugin",
    "title": "QGIS: raster analysis",
    "section": "Install the SAGA Plugin",
    "text": "Install the SAGA Plugin\nWe need to install a plugin so we can access certain geoprocessing tools (the built-in SAGA provider has been removed in version 3.30). Go to Plugins &gt; Manage and Install Plugins... and Search... for “SAGA”. From the list on options choose Processing Saga NextGen Provider then in the bottom right, click Install Plugin. You might also need to install SAGA (version 9 or above) on your computer, if not already available.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#get-some-elevation-data",
    "href": "QGIS/raster/QGIS_raster.html#get-some-elevation-data",
    "title": "QGIS: raster analysis",
    "section": "Get some elevation data",
    "text": "Get some elevation data\nA Digital Elevation Model (DEM) is a common example of raster data, i.e. grid data that contains a value in each cell (a bit like the pixels in a coloured picture).\nFor this tutorial, we are using a DEM sourced from the USGS website.\n\nGo to https://earthexplorer.usgs.gov/\nClick the World Features box, and then search for “Brisbane” in the “Feature Name” search box\nClick Show and select the first result\nZoom onto an area of interest around Brisbane and click “Use Map”\nClick the “Data Sets” button and then Digital Elevation &gt; SRTM, select “SRTM 1 Arc-Second Global” and click “Results”\n\n“SRTM” stands for “Shuttle Radar Topography Mission”. It provides global elevation data collected in 2000 by the space shuttle Endeavour.\nOur area covers two separate raster files. We can click on the foot icon to see the footprint of each file, and the picture icon to see what the DEM looks like.\nUse the download button to download each file into your project directory. You will need a login for that, which is free but can take a bit of time. You can instead download the two raster files as an archive from our GitHub repository here.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#merge-the-two-dem-layers",
    "href": "QGIS/raster/QGIS_raster.html#merge-the-two-dem-layers",
    "title": "QGIS: raster analysis",
    "section": "Merge the two DEM layers",
    "text": "Merge the two DEM layers\nIf you downloaded the archive, make sure you place it into the project directory so you can find it in “Project Home” and easily load the raster files: from the Browser panel, we can go into the data archive and drag and drop each .tif file into the Layers panel.\nSee the visible line between the two raster tiles? That is because the two separate raster files have different maximum and minimum values, so use different shades for different elevations. We have to merge them to make sure they use the same colour scale.\nTo do that, we use the Raster &gt; Miscellaneous &gt; Merge... tool to create one single layer from them.\n\nFirst, select both DEM layers for the “Input layers”\nMake sure the option “Place each input file into a separate band” is off, as we want to end up with one single-band layer\nWe can save the output on disk instead of only creating a temporary file (for example, name it SRTM_DEM_merged and save it inside your project directory)\nClick “Run”\n\n\nYou will need to have GDAL installed for this to work.\n\nWe can now remove the two original raster files.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#reproject-the-dem",
    "href": "QGIS/raster/QGIS_raster.html#reproject-the-dem",
    "title": "QGIS: raster analysis",
    "section": "Reproject the DEM",
    "text": "Reproject the DEM\nIn the merged layer’s Properties (Right click &gt; Properties... &gt; Source), we can see that the Coordinate Reference System (CRS) in use is EPSG:4326 - WGS 84. It is the one that QGIS detected when opening the file. This Geographic Reference System is good for global data, but if we want to focus on a more precise area around Brisbane/Meanjin, and want our analyses to be accurate, we should reproject the data to a local Projected Reference System (PRS). We will also need a projection that uses Metres, rather than Degrees (as EPSG:4326 does). A good PRS for around Brisbane/Meanjin is “EPSG:7856 - GDA2020 / MGA zone 56”. We can’t change that here, we instead will need to use the Warp (Reproject) tool.\n\nUse the tool Raster &gt; Projections &gt; Warp (Reproject)\nuse the merged layer as an input\npick “EPSG:7856 - GDA2020 / MGA zone 56” as the Target CRS\n\nyou may need to click the Select CRS button \nuntick No CRS\nuse the filter to search for “7856”\nYou should be able to find “EPSG:7856 - GDA2020 / MGA zone 56” under Predefined Coordinate Reference Systems\n\n\n\nIf you want to learn more about the static datum GDA2020 (for “Geocentric Datum of Australia 2020”), an upgrade from the previous, less precise GDA94, see e.g. a description from Geoscience Australia",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#clip-the-dem",
    "href": "QGIS/raster/QGIS_raster.html#clip-the-dem",
    "title": "QGIS: raster analysis",
    "section": "Clip the DEM",
    "text": "Clip the DEM\nWe now use Raster &gt; Extraction &gt; Clip Raster by Extent to focus on a smaller area of interest.\nMake sure the DEM is selected in the Input layer, and set the clip extent with ... &gt; Draw on Canvas. We want to select an area that is inland, and contains both some of the D’Aguilar National Park, and a section of the Brisbane river (you can untick the DEM in the Layers panel to reveal the basemap).\nIf you don’t save to file directly, remember two things:\n\nrename your clipped layer so it is more descriptive than the generic “Clipped (extent)”\nyou are currently using a temporary, scratch layer. It will be discarded if you exit QGIS. It is very useful for temporary intermediate files, but it can be safer to save copies of your intermediate data while you work, just in case! You can right-click on the layer and use Export &gt; Save As..",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#change-the-symbology",
    "href": "QGIS/raster/QGIS_raster.html#change-the-symbology",
    "title": "QGIS: raster analysis",
    "section": "Change the symbology",
    "text": "Change the symbology\nWe can style our DEM with a terrain colour palette:\n\ndouble-click on the clipped DEM layer\ngo to the “Symbology” tab\nchange the Render type to “Singleband pseudocolor”\nby default, it uses the min/max values, which is what we want\nwe can change the “Color ramp” to something more suitable with the drop-down menu and Create new color ramp... &gt; Catalog: cpt-city &gt; Topography &gt; Elevation, for example.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#add-a-hillshade",
    "href": "QGIS/raster/QGIS_raster.html#add-a-hillshade",
    "title": "QGIS: raster analysis",
    "section": "Add a hillshade",
    "text": "Add a hillshade\nAdding a hillshade makes your visualisation of elevation more readable and visually pleasing by giving an artificial lighting look to your map.\n\nRight-click on the DEM layer and “Duplicate layer”\nRename the duplicated layer “hillshade”\nOpen the Symbology menu for the hillshade layer\nChange the “Render type” to “Hillshade”\nThe defaults should work well, but you can play with the settings, like the Altitude and the Azimuth\nMake sure you apply some transparency to the pseudocolour DEM, and place the hillshade layer underneath (in Properties &gt; Symbology &gt; Transparency)\nDown the bottom of the window under Resampling change the Zoomed: in and out from “Nearest Neighbour” to “Cubic” (this will remove some of the grid-like patterns you might see in your Hillshade layer otherwise)\n\n\nAnother method is to use the hillshade tool instead of the symbology: Raster &gt; Analysis &gt; Hillshade....",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#create-a-watershed-layer",
    "href": "QGIS/raster/QGIS_raster.html#create-a-watershed-layer",
    "title": "QGIS: raster analysis",
    "section": "Create a watershed layer",
    "text": "Create a watershed layer\nUsing the Strahler tool, we can create a watershed raster that shows where water would flow according to the DEM.\nOpen the Processing Toolbox (cog icon) and try using the SAGA Next Gen tool called “Strahler order” using the clipped DEM as an input (a temporary file is fine for now).\nLook at the result. It looks like there are few issues with our data. You may get a question mark symbol  next to your data, when hovered over it will say “There is no coordinate reference system set!”. To resolve this, issue, simply left-click on the question mark symbol and assign “EPSG:7856 - GDA2020 / MGA zone 56” as the CRS.\nHowever there is still another issue with our data. A common problem with DEMs is that they have sinks and spikes that will make further analyses more difficult. This will mean the analysis is unable to best find where the water would flow. To resolve this, we need to use another tool to smooth out our raster before using the Strahler order tool.\nWe can use the “Fill sinks (Wang & Liu)” tool to fill the sinks in our clipped DEM.\n\nWhen we do that, we might have to play with the “Minimum slope” value. A value of 0.01 degrees should work well if the layer was reprojected to a suitable Projected Reference System.\nAs an output, we only need to tick the “Filled DEM” (first one in the list), which we can also save to file. You can however keep the “Watershed Basins” output to check that your minimum slope is high enough.\n\nIf we re-run the Strahler order tool on the filled DEM, we will be able to see more useful data.\nWe can now colour the layer with “Singleband pseudocolor” to highlight the bigger streams. A palette that goes from white (for low values) to a dark colour (for high values) should work well. You can also set the smaller streams to be transparent to filter them out.\n\nAnother way to filter out the noise of the smaller streams and highlight the major streams in the network, we can use the Raster &gt; Raster Calculator tool and use a formula like: \"name_of_layer@1\" &gt;= 6 (the value will depend on how many levels exist in the Strahler layer). We need to save to file to be able to do that (name it “strahler_filtered”, for example). This will assign the value 1 to the cells matching the condition, and 0 to what is under the limit.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#channel-network-and-drainage-basin",
    "href": "QGIS/raster/QGIS_raster.html#channel-network-and-drainage-basin",
    "title": "QGIS: raster analysis",
    "section": "Channel network and drainage basin",
    "text": "Channel network and drainage basin\nAnother analysis we can do is use the “Channel network and drainage basins” tool to calculate the flow direction, channels and drainage basins.\n\nMake sure you run this tool on the filled DEM.\nWe might have to change the threshold to a higher one if the output includes too many small basins and channels. As the threshold is related to the Strahler order number, the middle point of your previous Strahler order values is a usually a good default.\n\nYou might want to play with different threshold values depending on what you’re looking for. If you want to look at the main drainage basins of a region, you need to set the threshold higher. If you want to find all the small channels, you need to set the threshold lower. It’s worth trying a few options to find what works best for your dataset and the story you’re trying to tell.\nAs an output, we only want to load (and save to file) the two non-optional outputs:\n\nChannels\nDrainage basins (shapefile) (to differentiate the two “Drainage basins” options, you can check what format it saves the file as)\n\nThis is an example of creating vector data from raster data!\nWe can now play with the symbology for those elements. For example:\n\nTry using different colours for each basin, by classifying by ID, or remove the fill (Simple fill &gt; Fill Style &gt; No Brush) so you can see the colours of the elevation colours underneath. You can also make the borders more obvious by changing the width of the stroke.\nChange the colour of the channels.\n\nChange the symbol from “Single Symbol” to Graduated.\nSet the Value to “Order”\nChoose an appropriate colour ramp\nChange the Mode down the bottom of the window to “Equal Interval”\nClick Apply\n\nYou can also further differentiate minor channels from major ones by using a “Data defined override” for the Width value:\n\nClick on the bar next to Symbol \nClick on the “Data defined override” icon  next to the Width field\nUse the “Assistant”\n“Source” needs to be the column “ORDER” (which corresponds to the Strahler order)\nClick the double-arrow icon to “Fetch value range from layer”\nChange the “Size from” and “to” values to suitable values",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#viewshed",
    "href": "QGIS/raster/QGIS_raster.html#viewshed",
    "title": "QGIS: raster analysis",
    "section": "Viewshed",
    "text": "Viewshed\nIf you want to know what can be seen from a certain point in a landscape you can use a DEM to perform a Viewshed Analysis.\nOpen the Processing Toolbox (cog icon) and search for the GDAL tool called “Viewshed”.\n\nInput Layer: Choose your Reprojected DEM layer\nClick the three dots next to Observer location to select a point on the map\nObserver height, DEM units: 1.6 (the average human eye height is around 1.6m, choose a height you think might be appropriate)\nTarget height, DEM units: 1 (this means that the points around the observer will be obscured by any surface that is higher than 1m. If you want to know what ground your observer will see, choose something closer to 0)\nMaximum distance from observer to compute visibility: 5000 (5km is the distance you can see on flat ground due to the curvature of the earth, this would change if you were higher up, for which you can find calculators and a formula)\n\nThis will produce a black and white raster showing what can be seen, and what can’t. Go to the Layer Styling Menu, and change it to Singleband Pseudocolor, change the Mode to Equal Interval, and set the Classes to 2. Change one of the Colors to transparent to highlight either what can be see, or what cannot.\nAs an extra feature. You can do the reverse analysis with a Viewshed. That is, you could use this to know what landmarks are visible in a landscape. Set the Observer Height to that of a building or tree in the landscape, and the Target Height to be that of people in the landscape, and you’ll know where it can be seen from.\nIf you want to get more sophisticated with your Viewshed analyses, there is, of course, a plugin called Visibility Analysis which will allow you to use one or multiple points as observers, take in account of the Earth’s curvature, see what vector points can see other vector points, and more.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#d-maps",
    "href": "QGIS/raster/QGIS_raster.html#d-maps",
    "title": "QGIS: raster analysis",
    "section": "3D maps",
    "text": "3D maps\nA 3D viewer is integrated in QGIS: View &gt; 3D Map Views &gt; New 3D Map View\nIn the 3D map window, make sure to first:\n\nClick the Options wrench icon, choose Configure &gt; Terrain, set the “Type” to “DEM (Raster layer)”, and set the “Elevation” to your clipped DEM.\nExaggerate the relief with the “Vertical scale” setting (try 3).\n\nTo see the 3D effect, you will have to use your Ctrl or Shift keys on your keyboard while panning with the mouse to change the angle of view.\nTo avoid seeing gaps in the rendering, you can go back to your Terrain options and set the “Tile resolution” and “Skirt height” to higher values.\n\nA useful plugin for 3D maps is qgis2threeJS, which might be handy to add a 3D map to a website.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#exporting",
    "href": "QGIS/raster/QGIS_raster.html#exporting",
    "title": "QGIS: raster analysis",
    "section": "Exporting",
    "text": "Exporting\nUse the Layout Manager to create a new layout, and insert both a 2D map and a 3D map.\nWhen inserting the 3D map, it will tell you that the “scene is not set”. You will have to import the 3D scene settings from your view: Item Properties &gt; Scene Settings &gt; Copy Scene Settings from a 3D View... and then select your 3D map from the drop-down.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#saving-your-project",
    "href": "QGIS/raster/QGIS_raster.html#saving-your-project",
    "title": "QGIS: raster analysis",
    "section": "Saving your project",
    "text": "Saving your project\nNotice the little icon next to some of your layers? We previously created “temporary scratch layers”. This is useful if you keep processing data and creating new layers that you want to discard afterwards. In our case, we do want to keep the “cities” and “rivers” layers, so we need to save them to a file. If we try to close QGIS with scratch layers loaded, it will give you a warning that they will be lost in the process.\nYou can click on the scratch layer icon to save the file. In the dialog, we can give the layers a File name (in our project’s home directory) and click OK.\nYou can save your project with the floppy disk icon, or using Ctrl + S, and the project should be visible in a list as soon as you open QGIS again.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/raster/QGIS_raster.html#feedback",
    "href": "QGIS/raster/QGIS_raster.html#feedback",
    "title": "QGIS: raster analysis",
    "section": "Feedback",
    "text": "Feedback\nPlease visit our website to provide feedback and find upcoming training courses we have on offer.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "3. Raster analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html",
    "href": "QGIS/vector/QGIS_vector.html",
    "title": "QGIS: Vector Analysis",
    "section": "",
    "text": "Vector data is made up of points, lines, and/or polygons. They are made up of precise points with individual coordinates. Vector data is best contrasted with Raster data which has a grid of values evenly spaced apart, connected to one coordinate. Rasters are efficient at displaying large amounts of data, where vector data is very precise.\nThe Map School has some useful explainers of what Vector data is.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#what-is-vector-data",
    "href": "QGIS/vector/QGIS_vector.html#what-is-vector-data",
    "title": "QGIS: Vector Analysis",
    "section": "",
    "text": "Vector data is made up of points, lines, and/or polygons. They are made up of precise points with individual coordinates. Vector data is best contrasted with Raster data which has a grid of values evenly spaced apart, connected to one coordinate. Rasters are efficient at displaying large amounts of data, where vector data is very precise.\nThe Map School has some useful explainers of what Vector data is.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#what-are-we-doing-today",
    "href": "QGIS/vector/QGIS_vector.html#what-are-we-doing-today",
    "title": "QGIS: Vector Analysis",
    "section": "What are we doing today?",
    "text": "What are we doing today?\nTo look at vector data, we’re going to use the example of koala populations and protected areas, and use some analyses to see how they interact. The QLD Government has set koala protection as a priority for the State, but how do their priorities match up with the data? We can use QGIS and spatial analysis to ask questions of the data.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#setting-up",
    "href": "QGIS/vector/QGIS_vector.html#setting-up",
    "title": "QGIS: Vector Analysis",
    "section": "Setting Up",
    "text": "Setting Up\nThis is an intermediate level tutorial. Before completing this tutorial, we recommend our QGIS: Introduction to Mapping tutorial. This tutorial is designed for QGIS 3.40. If you need to install it on your computer, go to the QGIS website.\nWe will start as always by creating a good folder structure to work within. This folder is where our project, our data, and creations will live. Folder structure is very important for keeping your data tidy, as well as for ease of sharing your project with others. You simply need to zip the project folder if you need to share the whole thing.\n\nOpen QGIS and create a new project with Project &gt; New.\nLet’s now save our project: Project &gt; Save.\nCreate a new folder, let’s call it “qgis_vector”.\n\n\n\n\nInside that folder, create these folders:\n\n“data” - for all the data we will use to make our maps, split into:\n\n“raw” - raw data from your research or the internet\n“processed” - any data you’ve modified\n\n“output” - for any maps or images we export\n“temp” - this isn’t necessary, but when you’re playing around and testing, it stops things getting messy.\n\nFinally, let’s save our .qgz project file here, named “qgis_vector_map.qgz”\n\n\n\n\n\n\n\n\n\nYour .qgz file should always be in the highest level folder, so it’s only looking down into folders for data, not back out.\nThis might seem unnecessary now, but things quickly get out of control and hard to find if you don’t have a good folder structure.\nLet’s finally add an OpenStreetMap basemap to locate ourselves on the globe:\n\nBrowser panel &gt; XYZ Tiles &gt; OpenStreetMap (double-click, or drag and drop into the Layers panel).",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#gather-some-data",
    "href": "QGIS/vector/QGIS_vector.html#gather-some-data",
    "title": "QGIS: Vector Analysis",
    "section": "Gather some data",
    "text": "Gather some data\nWe’re going to explore a number of different online spatial data repositories. Please download the full dataset here, and extract it into the qgis_vector folder you created earlier.\nI will quickly show you where all of this data came from.\n\nOnline Community Spatial Repositories\n\nKoala Sighting Data (encounters)\nIf we’re looking at Koalas, we should get some occurence/sighting data.\nWe’re getting our species observation data today from the Atlas of Living Australia. This is an Australia Biodiversity occurrence database. It pulls data from a variety of different sources, including government data, individual collectors and community groups. This means that this data will contain sampling bias and will often simply represent encounters, rather than using robust sampling and collection methods. So, while we need to use this data with caution, it’s still a useful dataset!\nYou need to create an account and request the exact dataset you need, so to speed things up today, we’ve provided the data already cleaned and processed the data in the download link above.\nSome similar online repositories include the Global Biodiversity Information Facility (GBIF) and iNaturalist\n\n\nCleaning and processing the ALA data\nWhat do we mean by processed? Well, the ALA dataset has 206 columns by default. This means that each occurrence has 206 associated cells, and when multiplied by ~200,000 sightings, our data gets huge (&gt;300mb!). To save time (and storage space!) today we have already deleted 200 of those columns (bringing the dataset down to 15mb).\n\n\n\nQLD Government Spatial Data\nWe’ve seen the QSpatial data portal in previous sessions, and today we will be getting two lots of data from here.\n\nKoala Priority Areas\nKoala Priority Areas are areas in SEQ which have been identified as key areas for conservation as part of the South East Queensland Koala Conservation Strategy 2019-2024. You can search for “Koala Priority Areas” in QSpatial, or by going directly to the data.\nLet’s get a resource to compare with these QLD government priority areas…\n\n\n\nFederal Environment Data\n\nProtected Areas\nThe Federal Environment Department has a variety of different spatial datasets that you can browse through. Today we are going to be using the Collaborative Australian Protected Areas Database (CAPAD) 2020, which is a compilation of government, Indigenous and privately protected areas for Australia. You can search for “CAPAD” in on the Environment Department website, or by going directly to the data.\nFinally let’s get some data to put all of our protected areas and observations into context…\n\n\n\nAustralian Bureau of Statistics Data\nThe ABS is a huge source of data, however, it can be a bit difficult to find that data, and use it in a spatial context.\n\nDigital Boundary Files - SA2 - Suburb data\nThe ABS has a variety of ways that it splits its data up. These Digital Boundary Files are very useful for classifying data. They generally classify all of Australia into discrete Statistical Areas. Level 1 are the smallest, and Level 4 are the coarsest. (notably, the link above also has non-ABS Structures/boundary files such as Electoral areas and Postcodes). Today we are going to use the Statistical Area 2 data, which effectively represent suburbs, but we can take it a little further with more ABS data.\n\nPopulation Data\nThe ABS have a lot of useful data, today we will be using their population data. They provide it in excel format, as GeoPackages (by SA2 and LGAs), and as population grid raster files. Today we will be using their SA2 GeoPackages. This means we have our digital boundaries, and population in one!\n\nIn the zip file of today’s data we have already trimmed it down to just QLD to save on file size\n\n\n\n\n\nData Summary\nYou should have:\n\nPoint data for analysis\n\nKoala Sightings\n\nBoundary Files to spatially categorise our data\n\nKoala Priority Areas\nProtected Areas\nSA2 Areas (with human population)",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#set-the-project-projection",
    "href": "QGIS/vector/QGIS_vector.html#set-the-project-projection",
    "title": "QGIS: Vector Analysis",
    "section": "Set the Project Projection",
    "text": "Set the Project Projection\nWe need to choose the projection for our current session of QGIS. Today we will be focusing on South East Queensland (SEQ), so we will choose GDA2020 / MGA zone 56. We will go into projections in more detail soon.\n\nGo to Project &gt; Properties select the CRS tab.\nIn the filter section, type “GDA2020 56”\nFrom the Coordinate Reference System list, select EPSG:7856 - GDA2020 / MGA zone 56\n\n\nYou will notice that the OpenStreetMap basemap looks very warped, except for the East Coast of Australia. This is because the projection we have selected is very focused on reducing distortion within the bounds of the projection’s area (Eastern Australia).",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#load-in-our-data",
    "href": "QGIS/vector/QGIS_vector.html#load-in-our-data",
    "title": "QGIS: Vector Analysis",
    "section": "Load in our data",
    "text": "Load in our data\nFor most of our data, we will simply be able to double click on it in the Project Home folder within the Browser window. When you load in this data, QGIS will give you a warning that your Project Projection is different to the data you’re importing. Simply click cancel on this window. We will be fixing this issue later. Make sure your Project Projection remains as EPSG:7856.\nLoad in:\n\nSA2_ERP_2021_QLD.gpkg (suburb data)\nCAPAD2020_terrestrial_QLD.gpkg (Protected Areas)\nkoala_priority_area.gpkg\n\nBut what about the Koala Encounters location data? We need to handle this differently, as it is currently not in a spatial format, but in a csv file.\n\nImporting CSV data\n\nGo to Layer &gt; Add Layer &gt; Add Delimited Text Layer...\nClick the three dots ... next to the File name field, navigate to the project folder, and select koala_reduced\nClick the Geometry Definition drop down\n\nThis should automatically identify decimalLongitude and the X field, and decimalLatitude as the Y field.\nYou may need to set the Geometry CRS. For this data from the ALA it is EPSG:4326 - WGS84\n\nClick Add\n\n\nIt’s usually the case that data like this is in EPSG:4326 - WGS84, as that is the standard used by most GPS units and Google Maps. However, it’s still worth checking those details on the website you’ve downloaded it from. If your data doesn’t come with a predefined projection, and the spatial portal doesn’t specify, but it uses a Google Maps style interface, it’s probably using EPSG:4326 - WGS84",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#projections",
    "href": "QGIS/vector/QGIS_vector.html#projections",
    "title": "QGIS: Vector Analysis",
    "section": "Projections",
    "text": "Projections\n\nWhat are projections?\nTo turn the geoid/spheroid shape of the Earth into a flat map, we need to squish, stretch, and distort the map to make it flat. The mathematical equations used to do this are what we’re talking about when we say “projections”. Imagine it like a soccer ball, if we have to squash it to make it flat, it’s not going to look nice and square like our maps do. So we pull and stretch it to make it flat and rectangular. There will always be some kind of distortion when we stretch our map like this. This is why the Mercator Projection makes Greenland look large, and Africa look smaller than it really is.\n\n\nWhy are projections important to us?\nWell, when we make these distortions, we have to compromise somewhere, and that means our lengths, or size or direction will be different to what it really is. To avoid this kind of distortion, often local projections are used. There are fewer compromises needed when focused on a small area. By using a local projection, we don’t need to worry about keeping Greenland looking the right shape if we’re focused on Brisbane. Going back to Soccer balls, if we cut out a single panel from the ball, it will be much easier to make that flat.\nThe trouble with using data of different projections is that they might be slightly off around the edges, giving us different total areas in a polygon, or showing a point outside a boundary, when it’s really inside. To avoid this, it’s often best to convert all of your data to using the same projection.\nToday we’re going to use a suitable local projection: EPSG:7856 - GDA2020 / MGA zone 56\n\n\nReproject\nFor each of our layers, do the following:\n\nGo to Vector &gt; Data Management Tools &gt; Reproject Layer...\nChoose the layer in Input layer\nSet the Target CRS to EPSG:7856 - GDA2020 / MGA zone 56\n\nIf that option isn’t available, click  (Select CRS), and type “GDA2020 56” into the filter, the option should now appear under the Predefined Coordinate Reference Systems section.\n\nClick the three dots ... next to the Reprojected section, and click Save to File…\nNavigate to your data &gt; processed folder and save the file there. For example, save SA2_ERP_2021 as SA2_Reproj, CAPAD2020_terrestrial_QLD as CAPAD_Reproj, koala_reduced as Koalas_Reproj and koala_priority_area as KPA_Reproj\nClick Run\n\nYou won’t notice any difference, as QGIS is helpfully doing “on the fly projections” to make the layers sit nicely together. But now that you’ve reprojected your data, you can safely do your analyses.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#subset-our-sa2-data-down-to-seq---select-features-using-an-expression",
    "href": "QGIS/vector/QGIS_vector.html#subset-our-sa2-data-down-to-seq---select-features-using-an-expression",
    "title": "QGIS: Vector Analysis",
    "section": "Subset our SA2 data down to SEQ - Select features using an expression",
    "text": "Subset our SA2 data down to SEQ - Select features using an expression\nThe following code will allow you to select the SA2 features that are in SEQ.\n\nRight click on the reprojected SA2 layer, and select Open Attribute Table\nFrom the Attribute Table that opens, click the Select features using an expression button: \nIn the Select by Expression window that opens, paste the code from below into the Expression field, and then click Select Features in the bottom right of the window.\n\n \"SA4_name_2021\" =  'Gold Coast' \n OR\n \"SA4_name_2021\" =  'Sunshine Coast' \n  OR\n \"SA4_name_2021\" =  'Toowoomba' \n OR\n  \"GCCSA_name_2021\"  LIKE  '%Brisbane%' \n\nThis is SQL code, which is great for querying databases. This code selects any row that matches any of the criteria (this OR that). The first three look for exact matches, the last one looks to match the pattern given. The % acts like wildcard, so it’s looking for any row that contains Brisbane.\n\n\nClose the Select by Expression window and Attribute Table\nYou should see the SEQ SA2 areas highlighted in yellow (you may need to turn off other layers or zoom in).\nTo permanently save this selection, right click on the reprojected SA2 layer, and select Export &gt; Save Selected Features As...\nSave your file as SA2_SEQ\nMake sure the CRS stays as EPSG:7856 - GDA2020 / MGA zone 56, then click OK\n\nIf you haven’t done so already, untick and hide all the original layers we aren’t using any more.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#analysis-spatial-overlaps",
    "href": "QGIS/vector/QGIS_vector.html#analysis-spatial-overlaps",
    "title": "QGIS: Vector Analysis",
    "section": "Analysis: Spatial Overlaps",
    "text": "Analysis: Spatial Overlaps\nLet’s find out how much of our Koala Priority areas are already under federally recognised protection. To do this we will use the Intersection tool. This tool is similar to the Clip tool, but rather than just cutting out the overlapping area, it also combines the Attribute Tables of the two layers.\n\nIntersection\n\nGo to Vector &gt; Geoprocessing Tools &gt; Intersection\nUnder Input Layer select CAPAD_Reproj\nUnder Overlay Layer select KPA_Reproj\nClick Run\n\nWe get an error Feature (26) from “CAPAD_Reproj” has invalid geometry. This is caused by little issues in the polygon layer. Sometimes when polygons are drawn or exported from online sources, they will create errors, and sometimes little slither polygons on the edges. We can investigate the source of these errors using the Check validity tool, but for today, we’re simply going to fix them with the Fix Geometries tool from the Processing Toolbox.\n\n\nFix Geometries\n\nOpen the Processing Toolbox by clicking the cog icon from the top menu  (alternatively go to View &gt; Panels &gt; Processing Toolbox)\nIn the Processing Toolbox window Search for “Fix geometries”\nDouble-click on the Fix geometries option\nIn the Fix Geometries window, select CAPAD_Reproj from the Input layer options, then click Run\nRight click Fixed Geometries in the Layer panel, click Rename Layer, and change it to CAPAD_Fixed\n\nYou can now re-run the Intersection tool with the resulting CAPAD_Fixed layer (instead of the CAPAD2020_terrestrial_QLD layer)\n\nGo to Vector &gt; Geoprocessing Tools &gt; Intersection\nUnder Input Layer select CAPAD_Fixed\nUnder Overlay Layer select KPA_Reproj\nClick Run\n\nMagic. Our new layer with be the thin overlap between our two original layers, and will have their shared Attribute Tables. However, this is a double-edged sword, as it has retained the Area columns of the original CAPAD polygons. If we want to know our new shape’s area, we need to calculate that.\n\n\nField Calculator\nWe can use the Field Calculator to calculate the area of our polygon.\nSelect Intersection from the Layers panel, and the click the Open Field Calculator button \nIn the Field calculator window, type the following code into the Expression tab:\nsum($area)\n\n$area will give us the area of a single polygon - we could use this to create a new field in our Attribute Table based on area if we wanted to\nsum() will add together the area for every polygon in that layer.\nBelow the text box, you will see a field titled Preview:, the value following that contains the results of our expression. Copy that number.\nClick Cancel\n\nDo the same Field Calculator steps for the original koala_priority_area.\nYou can now use the Field calculator to determine the percentage of the Koala Priority area which is currently protected. 1506573200.936991 / 5776218019.211894 = 26%\nOnly 26%! Let’s look into this further. Perhaps our dataset is missing some new conservation areas.\nLet’s turn on the OpenStreetMap to see if we can see anything missing here. Let’s have a look at the dense collection of koala sightings near Springwood and the Daisy Hill Conservation Park (if you can’t find them, paste these coordinates -27.581128,153.176828 into the Coordinate box at the bottom of the window, and change the Scale to 1:10000). We can see that there are some protected bushlands in this area that aren’t in our CAPAD2020 dataset. It may be that these aren’t strict enough conservation areas, or our dataset may be out of date. Regardless, this gives us a good opportunity to use an important tool in GIS: Digitisation.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#map-digitisation",
    "href": "QGIS/vector/QGIS_vector.html#map-digitisation",
    "title": "QGIS: Vector Analysis",
    "section": "Map Digitisation",
    "text": "Map Digitisation\nYou may often need to create your own points, lines, and polygons when digitising satellite data, or simply highlighting a particular area. Let’s use the OpenStreeMap (in Browser, scroll down to XYZ Tiles, and double-click on OpenStreetMap) and digitise the Emu Street Bushland Refuge.\n\nGo to Layer &gt; Create Layer &gt; New GeoPackage Layer...\nClick the three dots ... next to the Database section\nNavigate to your data &gt; processed folder and save the file as ESBR_polygon\nFrom Geometry type select MultiPolygon\nMake sure the CRS is set to EPSG:7856 - GDA2020 / MGA zone 56\nLeave the other fields blank for now and click OK\n\nWe now have a brand new layer that we can add polygons to.\n\nSelect the new ESBR_polygon layer and then click the Toggle Editing pencil  from the top menu (or go to Layer &gt; Toggle Edititng\nOn your keyboard, press Ctrl + . (or click  Add Polygon Feature) to start adding a new polygon\nZoom in to a corner of the area you want to create the polygon, and then Left click to start drawing your polygon (You can use the mouse wheel to zoom in, and also press and drag on the mouse wheel to navigate)\n\nA red dotted line will now appear between that first point and your cursor.\n\nContinue adding points to your polygon until your return back to the start, Right click to stop digitising and create your polygon.\nLeave the fid as Autogenerate and click OK\nTo save what you’ve done, click the Save Layer Edits button next to the Toggle Editing button\nTo finish editing your layer click the Toggle Editing button\n\nYou now know how to digitise a polygon, but the same steps apply for creating a point or a line layer. We created a new layer here, and you can also do the same steps to edit a pre-exisitng layer too.\nThe Vertex Tool  will allow you to move the location of points (or corners of polygons) that you have already created (you also need to click the Toggle Editing button for this tool)\nDespite these missing Refuge area polygons, you can still see that there are a lot of koalas which are found outside of protected areas. In fact, most sightings seem to occur outside of protected areas! Is this poor protected area management, or might our data be biased by when and where people are more likely to encounter koalas?",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#analysis-how-do-koalas-and-people-overlap",
    "href": "QGIS/vector/QGIS_vector.html#analysis-how-do-koalas-and-people-overlap",
    "title": "QGIS: Vector Analysis",
    "section": "Analysis: How do koalas and people overlap?",
    "text": "Analysis: How do koalas and people overlap?\n\nCount Points in Polygons\nEarlier we looked at overlap between polygons, we can also look at points overlapping with a polygon. Let’s use the Count Points in Polygons tool to quickly count the number of points from a particular layer inside a polygon. We could look at a few things here, we could look at koala sightings in protected areas or in the priority areas, but let’s try to get an idea of how people and koala sightings overlap. You would expect there to be more koalas where there are fewer people, but perhaps our data is skewed by population levels.\nLet’s determine how many koalas are inside of each SA2 suburb.\n\nGo to Vector &gt; Analysis Tools &gt; Count Points in Polygons...\nIn the Polygons field select SA2_SEQ\nIn the Points field select Koalas_Reproj\nIn the Count field name field type in something like NUM_KOALAS\nClick the three dots ... next to the Count section, and click Save to File…\nNavigate to your processed folder and save the file as SA2_SEQ_koalas\nClick Run\n\nIf it’s being really slow, we might be able to benefit from the Fix Geometries tool again.\n\nClick Cancel in the Count Points in Polygons window, but leave it open for now\nIn the Processing Toolbox double-click on the Fix geometries option\n\nFor Input Layer choose Koalas_Reproj\nClick Run\n\nReturn to the Count Points in Polygons window, click Change Parameters\nChange the Points to the new Fixed geometries layer\nClick Run\n\nYou can now look at the Attribute Table (F6) for this layer to see the number of koala sightings in each suburb. Let’s visualise this.\n\nClick on the SA2_SEQ_koalas layer in the Layers panel\nOpen the Layer Styling Panel by pressing F7 (or fn + F7)\nChange the Symbology from Single Symbol to Graduated\nSet the Value to NUM_KOALAS\nChoose a Color ramp of your liking\nClick Classify\nYou might want to play with the Mode to get a feel for the data\n\nNow we can quickly see how many Koalas are in each suburb.\nWe can go further and use the Field Calculator to compare koala numbers to the current human population in that area, and create a new field with that information.\n\nClick on the Field Calculator button \n\nUnder Create a new field set the Output field name to KOALAS_PP\nSet the Output field type to Decimal number (real) (we need to choose this option to ensure that we have decimals in our output)\nIn the Expression tab enter \"NUM_KOALAS\" / \"ERP_2021\"\n\nERP stands for Estimated Resident Population\n\nClick OK\n\nNow you can change the Value in Layer Styling to KOALA_PP\n\nClick Classify again if needed\n\n\nWe can now see how koala populations compare with human populations.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#wrap-up",
    "href": "QGIS/vector/QGIS_vector.html#wrap-up",
    "title": "QGIS: Vector Analysis",
    "section": "Wrap up",
    "text": "Wrap up\nToday we explored projections, looked at a variety of data sources, questioned the quality of our data, used the Intersection tool, the Field Calculator, digitised a map, and used polygon point counts.\nAfter running these tests and analyses, do we feel that there is adequate protection and conservation areas for koalas in QLD? How might you show this?\nHow might you use these tools in your own analysis?",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#feedback",
    "href": "QGIS/vector/QGIS_vector.html#feedback",
    "title": "QGIS: Vector Analysis",
    "section": "Feedback",
    "text": "Feedback\nPlease visit our website to provide feedback and find upcoming training courses we have on offer.",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "QGIS/vector/QGIS_vector.html#stretch-goals",
    "href": "QGIS/vector/QGIS_vector.html#stretch-goals",
    "title": "QGIS: Vector Analysis",
    "section": "Stretch goals",
    "text": "Stretch goals\n\nExport a Map\nUse the data and summary statistics to export a useful map from this data.\n\n\nHeatmaps\nTry changing the Symbology of the koala_reduced point dataset to the Heatmap option or even the Point Cluster option.\nCompare the Heatmap from Symbology with one that you can create with the Heatmap (Kernel Density Estimation) tool from the Processing Toolbox\n\n\nUse Zonal Statistics to calculate values from Rasters\nYou can use the Zonal statistics tool from the Processing Toolbox to count the number of raster squares, sum together all of the values, and find the average value from a raster that overlaps with a chosen polygon.\n\n\nVector to Raster conversion\nConvert a species distribution point dataset to a raster image",
    "crumbs": [
      "Home",
      "![](/images/QGIS.svg){width=20} QGIS",
      "4. Vector analysis"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html",
    "href": "R/ANOVA-lm/anova-lm.html",
    "title": "R statistics: ANOVA and linear regression",
    "section": "",
    "text": "In this hands-on session, you will use R, RStudio to run analysis of variance (ANOVA) and linear regression models.\nSpecifically, you will learn about:\n\ndata visualisation in base R and ggplot2\nanalysis of variance (ANOVA) in base R\nlinear models in base R\nthe tidy model approach",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#what-are-we-going-to-learn",
    "href": "R/ANOVA-lm/anova-lm.html#what-are-we-going-to-learn",
    "title": "R statistics: ANOVA and linear regression",
    "section": "",
    "text": "In this hands-on session, you will use R, RStudio to run analysis of variance (ANOVA) and linear regression models.\nSpecifically, you will learn about:\n\ndata visualisation in base R and ggplot2\nanalysis of variance (ANOVA) in base R\nlinear models in base R\nthe tidy model approach",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#keep-in-mind",
    "href": "R/ANOVA-lm/anova-lm.html#keep-in-mind",
    "title": "R statistics: ANOVA and linear regression",
    "section": "Keep in mind",
    "text": "Keep in mind\n\nEverything we write today will be saved in your project. Please remember to save it in your H drive or USB if you are using a Library computer.\nR is case sensitive: it will tell the difference between uppercase and lowercase.\nRespect the naming rules for objects (no spaces, does not start with a number…)\n\n\nHelp\nFor any dataset or function doubts that you might have, don’t forget the three ways of getting help in RStudio:\n\nthe shortcut command: ?functionname\nthe help function: help(functionname)\nthe keyboard shortcut: press F1 with your cursor on a function name",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#open-rstudio",
    "href": "R/ANOVA-lm/anova-lm.html#open-rstudio",
    "title": "R statistics: ANOVA and linear regression",
    "section": "Open RStudio",
    "text": "Open RStudio\n\nIf you are using your own laptop please open RStudio\n\nIf you need them, we have installation instructions\n\nMake sure you have a working internet connection\nOn Library computers (the first time takes about 10 min.):\n\nLog in with your UQ credentials (student account if you have two)\nMake sure you have a working internet connection\nGo to search at bottom left corner (magnifying glass)\nOpen the ZENworks application\nLook for RStudio\nDouble click on RStudio which will install both R and RStudio",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#setting-up",
    "href": "R/ANOVA-lm/anova-lm.html#setting-up",
    "title": "R statistics: ANOVA and linear regression",
    "section": "Setting up",
    "text": "Setting up\n\nInstall and load required packages for first sections\n\n# install.packages(\"readr\")\nlibrary(readr)        # data import\n# install.packages(\"dplyr\")\nlibrary(dplyr)        # data manipulation\n# install.packages(\"ggplot2\")\nlibrary(ggplot2)      # data visualisation\n# install.packages(\"car\")\nlibrary(car)          # Companion to the book \"An R Companion to Applied Regression\"\n# install.packages(\"performance\")\nlibrary(performance)  # Assessment of regression models performance\n\n\nRemember to use Ctrl+Enter to execute a command from the script.\n\n\n\nNew project\n\nClick the “File” menu button (top left corner), then “New Project”\nClick “New Directory”\nClick “New Project” (“Empty project” if you have an older version of RStudio)\nIn “Directory name”, type the name of your project, e.g. “r_statistics”\nSelect the folder where to locate your project: for example, a Documents/RProjects folder, which you can create if it doesn’t exist yet.\nClick the “Create Project” button\n\n\n\nCreate a script\nWe will use a script to write code more comfortably.\n\nMenu: Top left corner, click the green “plus” symbol, or press the shortcut (for Windows/Linux) Ctrl+Shift+N or (for Mac) Cmd+Shift+N. This will open an “Untitled1” file.\nGo to “File &gt; Save” or press (for Windows/Linux) Ctrl+S or (for Mac) Cmd+S. This will ask where you want to save your file and the name of the new file.\nCall your file “process.R”\n\n\n\nIntroducing our data\nThe following section will be using data from Constable (1993) to explore how three different feeding regimes affect the size of sea urchins over time.\nSea urchins reportedly regulate their size according to the level of food available to regulate maintenance requirements. The paper examines whether a reduction in suture width (i.e., connection points between plates; see Fig. 1 from constable 1993) is the basis for shrinking due to low food conditions.\n\n\n\nFigure 1 from Constable 1993 paper showing sea urchin plates and suture width\n\n\nThe data in csv format is available from the tidymodels website.\n\nurchins &lt;- \n   # read in the data\n   read_csv(\"https://tidymodels.org/start/models/urchins.csv\") %&gt;% \n   # change the names to be more descriptive\n   setNames(c(\"food_regime\", \"initial_volume\", \"width\")) %&gt;% \n   # convert food_regime from character to factor, helpful for modeling\n   mutate(food_regime = factor(food_regime, \n                               levels = c(\"Initial\", \"Low\", \"High\")))\n\n\nurchins # see the data as a tibble\n\n\n  \n\n\n\nWe have 72 urchins with data on:\n\nexperimental feeding regime group with 3 levels (Initial, Low, or High)\nsize in milliliters at the start of the experiment (initial_volume)\nsuture width in millimeters at the end of the experiment (width, see Fig. 1)",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#statistics-in-r-using-base-and-stats",
    "href": "R/ANOVA-lm/anova-lm.html#statistics-in-r-using-base-and-stats",
    "title": "R statistics: ANOVA and linear regression",
    "section": "Statistics in R using base and stats",
    "text": "Statistics in R using base and stats\n\nVisualise the data\nUse a boxplot to visualize width versus food_regime as a factor and a scatterplot for width versus initial_volume as a continuous variable.\n\nboxplot(width ~ food_regime, data = urchins)\n\n\n\n\n\n\n\nplot(width ~ initial_volume, data = urchins)\n\n\n\n\n\n\n\n\nWe can see that there are some relationships between the response variable (width) and our two covariates (food_regime and initial volume). But what about the interaction between the two covariates?\nChallenge 1 - Use ggplot2 to make a plot visualising the interaction between our two variables. Add a trendline to the data.\n\nHint: think about grouping and coloring.\n\n\nggplot(urchins,\n       aes(x = initial_volume,\n           y = width,\n           col = food_regime)) +\n   geom_point() +\n   geom_smooth(method = \"lm\", se = FALSE) # add a linear trend line without a confidence interval\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nUrchins that were larger in volume at the start of the experiment tended to have wider sutures at the end. Slopes of the lines look different so this effect may depend on the feeding regime indicating we should include an interaction term.\n\n\nAnalysis of Variance (ANOVA)\nInformation in this section was taken from rpubs.com and Data Analysis in R Ch 7.\nWe can do an ANOVA with the aov() function to test for differences in sea urchin suture width between our groups. We are technically running and analysis of covariance (ANCOVA) as we have both a continuous and a categorical variable. ANOVAs are for categorical variables and we will see that some of the post-hoc tests are not amenable to continuous variables.\n\naov() uses the model formula response variable ~ covariate1 + covariate2. The * denotes the inclusion of both main effects and interactions which we have done below. The formula below is equivalent to reponse ~ covar1 + covar2 + covar1:covar2 i.e., the main effect of covar1 and covar2, and the interaction between the two.\n\n\naov_urch &lt;- aov(width ~ food_regime * initial_volume, \n                data = urchins)\nsummary(aov_urch)  # print the summary statistics\n\n                           Df   Sum Sq  Mean Sq F value   Pr(&gt;F)    \nfood_regime                 2 0.012380 0.006190  13.832 9.62e-06 ***\ninitial_volume              1 0.008396 0.008396  18.762 5.15e-05 ***\nfood_regime:initial_volume  2 0.004609 0.002304   5.149  0.00835 ** \nResiduals                  66 0.029536 0.000448                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBoth the main effects and interaction are significant (p &lt; 0.05) indicating a significant interactive effect between food regime and initial volume on urchin suture width. We need to do a pairwise-comparison to find out which factor levels and combination of the two covariates have the largest effect on width.\n\nPair-wise comparison\nRun a Tukey’s Honestly Significant Difference (HSD) test - note it does not work for non-factors as per the warning message.\n\nTukeyHSD(aov_urch)\n\nWarning in replications(paste(\"~\", xx), data = mf): non-factors ignored:\ninitial_volume\n\n\nWarning in replications(paste(\"~\", xx), data = mf): non-factors ignored:\nfood_regime, initial_volume\n\n\nWarning in TukeyHSD.aov(aov_urch): 'which' specified some non-factors which\nwill be dropped\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = width ~ food_regime * initial_volume, data = urchins)\n\n$food_regime\n                     diff          lwr         upr     p adj\nLow-Initial  -0.006791667 -0.021433881 0.007850548 0.5100502\nHigh-Initial  0.023791667  0.009149452 0.038433881 0.0006687\nHigh-Low      0.030583333  0.015941119 0.045225548 0.0000129\n\n\nThe comparison between High-Initial and High-Low food regimes are significant (p &lt; 0.05).\n\n\nChecking the model\nWe also want to check that our model is a good fit and does not violate any ANOVA assumptions:\n\nData are independent and normally distributed.\nThe residuals from the data are normally distributed (homogeneity of variance).\nThe variances of the sampled populations are equal.\n\n\nNormal distribution\nChallenge 2 - Use a histogram and quantile-quantile (QQ) plots to visually check data are normally distributed.\n\nhist(urchins$width)\n\n\n\n\n\n\n\n# run the two together to combine them\nqqnorm(urchins$width)\nqqline(urchins$width)\n\n\n\n\n\n\n\n\nQQ plots show the actual data vs their theoretical quantiles (i.e. the quantiles if the data was normally distributed). If the data was normally distributed, the points would follow closely the diagonal.\nYou could also run a Shapiro-Wilk test on the data:\n\nshapiro.test(urchins$width)\n\n\n    Shapiro-Wilk normality test\n\ndata:  urchins$width\nW = 0.95726, p-value = 0.01552\n\n\nThe p-value is less than 0.05 so the data are significantly different from a normal distribution.\n\n\nHomogeneity of variance\nWe plot the model’s residuals (difference between observed and predicted values) versus the fitted values to check for homogeneity of variance - we do not want too much deviation from 0. (The red line is the average of the values.)\n\nplot(aov_urch, 1) # only plot first of 4 plots\n\n\n\n\n\n\n\n\nWe can also plot the predicted values from the model with the actual values:\n\nplot(predict(aov_urch) ~ urchins$width)\nabline(0, 1, col = \"red\") # plot a red line with intercept of 0 and slope of 1\n\n\n\n\n\n\n\n\nThis QQ plot show the residuals vs their theoretical quantiles (i.e. the quantiles if the data was normally distributed). If the residuals were normally distributed, they would follow closely the diagonal.\n\nplot(aov_urch, 2)\n\n\n\n\n\n\n\n\nTo check the normality of residuals, we can also run a Shapiro-Wilk test on residuals:\n\nshapiro.test(resid(aov_urch))\n\n\n    Shapiro-Wilk normality test\n\ndata:  resid(aov_urch)\nW = 0.98456, p-value = 0.5244\n\n\nThe residuals fall on the Normal Q-Q plot diagonal and the Shapiro-Wilk result is non-significant (p &gt; 0.05), which means we can’t reject the hypothesis that the data comes from a population with a normal distribution.\nAnother way to check for homogeneity of variance is by using Levene’s test.\nChallenge 3 - use the help documentation for leveneTest() from the car package to check homogeneity of variance on food_regime.\n\nAgain, only works for factor groups.\n\n\nleveneTest(width ~ food_regime, data = urchins)\n\n\n  \n\n\n\nThe Levene’s Test is significant for food_regime, which is not what we were hoping for: it means the assumption of homogeneity of variance is not met. There are a few options to deal with this. You can ignore this violation based on your own a priori knowledge of the distribution of the population being samples, drop the p-value significance, or use a different test.\n\n\n\n\nLinear Model\n\nlm_urch &lt;- lm(width ~ food_regime * initial_volume, \n              data = urchins)\nsummary(lm_urch)\n\n\nCall:\nlm(formula = width ~ food_regime * initial_volume, data = urchins)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.045133 -0.013639  0.001111  0.013226  0.067907 \n\nCoefficients:\n                                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                     0.0331216  0.0096186   3.443 0.001002 ** \nfood_regimeLow                  0.0197824  0.0129883   1.523 0.132514    \nfood_regimeHigh                 0.0214111  0.0145318   1.473 0.145397    \ninitial_volume                  0.0015546  0.0003978   3.908 0.000222 ***\nfood_regimeLow:initial_volume  -0.0012594  0.0005102  -2.469 0.016164 *  \nfood_regimeHigh:initial_volume  0.0005254  0.0007020   0.748 0.456836    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02115 on 66 degrees of freedom\nMultiple R-squared:  0.4622,    Adjusted R-squared:  0.4215 \nF-statistic: 11.35 on 5 and 66 DF,  p-value: 6.424e-08\n\n\nIn the output, we have the model call, residuals, and the coefficients. The first coefficient is the (Intercept) and you might notice the food_regimeInitial is missing. The function defaults to an effects parameterisation where the intercept is the reference or baseline of the categorical group - Initial in this case.\n\nYou can change the reference level of a factor using the relevel() function.\n\nThe estimates of the remaining group levels of food_regime represents the effect of being in that group. To calculate the group coefficients for all group levels you add the estimates for the level to the intercept (first group level) estimate. For example, the estimate for the ‘Initial’ feeding regime is 0.0331 and we add the estimate of ‘Low’ (0.0331 + 0.0197) to get the mean maximum size of 0.0528 mm for width.\nFor the continuous covariate, the estimate represents the change in the response variable for a unit increase in the covariate. For example, initial_volume’s estimate of 0.0015 represents a 0.0015 mm increase (the estimate is positive) in width per ml increase in urchin initial volume.\nWe can get ANOVA test statistics on our linear model using anova() in base or Anova() from the car package.\n\nanova(lm_urch)\n\n\n  \n\n\n\n\nAnova(lm_urch)\n\n\n  \n\n\n\nThese are effectively the same as the aov() model we ran before.\n\nNote: The statistics outputs are the same comparing the aov() and anova() models while the Anova() model is not exactly the same. The Anova() output tells us it was a Type II test and the aov() documentation says it is only for balanced designs which means the Type 1 test is applied (see here). The type of test can be set for Anova() but not the others. Here, the overall take-away from the different ANOVA functions are comparable.\n\nChallenge 4 - use the check_model() documentation to apply the function to our lm_urch model.\nThe performance package has a handy function check_model() that will check several aspects of your model in one go:\n\ncheck_model(lm_urch)\n\n\n\n\n\n\n\n\nChallenge 5 - conduct your own ANOVA or linear regression using the mgp dataset from {ggplot2}.\n\nTest whether # of cylinders and/or engine displacement affect fuel efficiency.\nMake a plot to visualize the relationship.\n\n\nHint: Check out the documentation for the dataset ?mpg to see the variables in the dataset. Are the variables the right data type? Suggest saving the dataset locally in your environment i.e., mpg2 &lt;- mpg so you can change data types if necessary.\n\n\nmpg2 &lt;-  mpg\nmpg2$cyl &lt;- as.factor(mpg$cyl) # convert cyl from numeric to factor\n\n# base R ANOVA\naov_cars &lt;- aov(hwy ~ cyl * displ, data = mpg2)\nsummary(aov_cars)\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \ncyl           3   4836  1612.1  142.69  &lt; 2e-16 ***\ndispl         1    219   218.8   19.37 1.66e-05 ***\ncyl:displ     2    642   321.1   28.42 9.65e-12 ***\nResiduals   227   2565    11.3                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nTukeyHSD(aov_cars, \"cyl\")\n\nWarning in replications(paste(\"~\", xx), data = mf): non-factors ignored: displ\n\n\nWarning in replications(paste(\"~\", xx), data = mf): non-factors ignored: cyl,\ndispl\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = hwy ~ cyl * displ, data = mpg2)\n\n$cyl\n            diff        lwr       upr     p adj\n5-4  -0.05246914  -4.508182  4.403244 0.9999898\n6-4  -5.97968433  -7.355259 -4.604110 0.0000000\n8-4 -11.17389771 -12.593534 -9.754261 0.0000000\n6-5  -5.92721519 -10.385582 -1.468848 0.0038274\n8-5 -11.12142857 -15.593586 -6.649271 0.0000000\n8-6  -5.19421338  -6.622156 -3.766270 0.0000000\n\n# linear model\nlm_cars &lt;- lm(hwy ~ cyl * displ, data = mpg2)\nsummary(lm_cars)\n\n\nCall:\nlm(formula = hwy ~ cyl * displ, data = mpg2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.6698 -2.0533 -0.4563  1.6948 13.1597 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   46.601      2.586  18.018  &lt; 2e-16 ***\ncyl5           2.887      1.773   1.628  0.10484    \ncyl6         -10.221      3.795  -2.693  0.00760 ** \ncyl8         -35.626      4.390  -8.114 3.07e-14 ***\ndispl         -8.295      1.193  -6.954 3.74e-11 ***\ncyl5:displ        NA         NA      NA       NA    \ncyl6:displ     4.318      1.440   2.998  0.00302 ** \ncyl8:displ     9.591      1.376   6.969 3.43e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.361 on 227 degrees of freedom\nMultiple R-squared:  0.6896,    Adjusted R-squared:  0.6814 \nF-statistic: 84.05 on 6 and 227 DF,  p-value: &lt; 2.2e-16\n\nAnova(lm_cars) # from the car package\n\nNote: model has aliased coefficients\n      sums of squares computed by model comparison\n\n\n\n  \n\n\n\n\nggplot(data = mpg2,\n       aes(x = displ,\n           y = hwy,\n           color = cyl)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") \n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#the-inbetween",
    "href": "R/ANOVA-lm/anova-lm.html#the-inbetween",
    "title": "R statistics: ANOVA and linear regression",
    "section": "The inbetween…",
    "text": "The inbetween…\nBefore going into Tidymodels, it should be mentioned there are many excellent linear regression packages. To name a few:\n\nnlme\nlmer\nlmerTest\nglmmTMB\nand more…\n\nThe packages vary in the methods, how to specify random factors, etc. The model outputs also tend to be not so friendly to export into a table and document.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#introducing-tidymodels",
    "href": "R/ANOVA-lm/anova-lm.html#introducing-tidymodels",
    "title": "R statistics: ANOVA and linear regression",
    "section": "Introducing Tidymodels",
    "text": "Introducing Tidymodels\nLike the tidyverse package, the Tidymodels framework is a collection of packages for modeling and machine learning following the tidyverse principles.\n\nLoad more packages\n\n# install.packages(\"tidymodels\")\nlibrary(tidymodels) # for parsnip package and rest of tidymodels\n# install.packages(\"dotwhisker\")\nlibrary(dotwhisker)# for visualizing regression results\n\n\n\nBuild and fit a model\nLet’s apply a standard two-way analysis of variance (ANOVA) model to the dataset as we did before. For this kind of model, ordinary least squares is a good initial approach.\nFor Tidymodels, we need to specify the following:\n\nThe functional form using the parsnip package.\nThe method for fitting the model by setting the engine.\n\nWe will specify the functional form or model type as “linear regression” as there is a numeric outcome with a linear slope and intercept. We can do this with:\n\nlinear_reg()  \n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\nOn its own, not that interesting. Next, we specify the method for fitting or training the model using the set_engine() function. The engine value is often a mash-up of the software that can be used to fit or train the model as well as the estimation method. For example, to use ordinary least squares, we can set the engine to be lm.\nThe documentation page for linear_reg() lists the possible engines. We’ll save this model object as lm_mod.\n\nlm_mod &lt;- \nlinear_reg() %&gt;% \n   set_engine(\"lm\")\n\nNext, the model can be estimated or trained using the fit() function and the model formula we used for the ANOVA:\nwidth ~ initial_volume * food_regime\n\nlm_fit &lt;- \n   lm_mod %&gt;% \n   fit(width ~ initial_volume * food_regime, data = urchins)\n\nlm_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = width ~ initial_volume * food_regime, data = data)\n\nCoefficients:\n                   (Intercept)                  initial_volume  \n                     0.0331216                       0.0015546  \n                food_regimeLow                 food_regimeHigh  \n                     0.0197824                       0.0214111  \n initial_volume:food_regimeLow  initial_volume:food_regimeHigh  \n                    -0.0012594                       0.0005254  \n\n\nWe can use the tidy() function for our lm object to output model parameter estimates and their statistical properties. Similar to summary() but the results are more predictable and in a useful format.\n\ntidy(lm_fit)\n\n\n  \n\n\n\nThis output can be used to generate a dot-and-whisker plot of our regression results using the dotwhisker package:\n\ntidy(lm_fit) %&gt;% \n   dwplot(dot_args = list(size = 2, color = \"black\"),\n          whisker_args = list(color = \"black\"),\n          vline = geom_vline(xintercept = 0, \n                             color = \"grey50\",\n                             linetype = 2))\n\nWarning: Using the `size` aesthetic with geom_segment was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead.\nℹ The deprecated feature was likely used in the dotwhisker package.\n  Please report the issue at &lt;https://github.com/fsolt/dotwhisker/issues&gt;.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#use-a-model-to-predict",
    "href": "R/ANOVA-lm/anova-lm.html#use-a-model-to-predict",
    "title": "R statistics: ANOVA and linear regression",
    "section": "Use a model to predict",
    "text": "Use a model to predict\nSay that it would be interesting to make a plot of the mean body size for urchins that started the experiment with an initial volume of 20 ml.\nFirst, lets make some new example data to predict for our graph:\n\nnew_points &lt;- expand.grid(initial_volume = 20,\n                          food_regime = c(\"Initial\", \"Low\", \"High\"))\nnew_points\n\n\n  \n\n\n\nWe can then use the predict() function to find the mean values at 20 ml initial volume.\nWith tidymodels, the types of predicted values are standardized so that we can use the same syntax to get these values.\nLet’s generate the mean suture width values:\n\nmean_pred &lt;- predict(lm_fit, new_data = new_points)\nmean_pred\n\n\n  \n\n\n\nWhen making predictions, the tidymodels convention is to always produce a tibble of results with standardized column names. This makes it easy to combine the original data and the predictions in a usable format:\n\nconf_int_pred &lt;- predict(lm_fit, \n                         new_data = new_points,\n                         type = \"conf_int\")\nconf_int_pred\n\n\n  \n\n\n# now combine:\nplot_data &lt;- \n   new_points %&gt;% \n   bind_cols(mean_pred, conf_int_pred)\n\nplot_data\n\n\n  \n\n\n# and plot:\nggplot(plot_data, \n       aes(x = food_regime)) +\n   geom_point(aes(y = .pred)) +\n   geom_errorbar(aes(ymin = .pred_lower,\n                     ymax = .pred_upper),\n                 width = .2) +\n   labs(y = \"urchin size\")\n\n\n\n\n\n\n\n\nThere is also an example of a Bayesian model in the tidymodels article I have not included here.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#close-project",
    "href": "R/ANOVA-lm/anova-lm.html#close-project",
    "title": "R statistics: ANOVA and linear regression",
    "section": "Close project",
    "text": "Close project\nClosing RStudio will ask you if you want to save your workspace and scripts. Saving your workspace is usually not recommended if you have all the necessary commands in your script.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#useful-links",
    "href": "R/ANOVA-lm/anova-lm.html#useful-links",
    "title": "R statistics: ANOVA and linear regression",
    "section": "Useful links",
    "text": "Useful links\n\nFor statistical analysis in R:\n\nSteve Midway’s Data Analysis in R Part II Analysis\nJeffrey A. Walker’s Applied Statistics for Experiemental Biology\nChester Ismay and Albert Y. Kim’s ModernDive Statistical Inference via Data Science\n\nFor tidymodels:\n\ntidymodels website\n\nOur compilation of general R resources",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ANOVA-lm/anova-lm.html#licence",
    "href": "R/ANOVA-lm/anova-lm.html#licence",
    "title": "R statistics: ANOVA and linear regression",
    "section": "Licence",
    "text": "Licence\nBecause it draws heavily from the CC BY-SA-licensed Tidymodels article titled “Build a model”, this material is also released under a CC BY-SA 4.0 licence.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "10. ANOVA and linear regression"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "",
    "text": "TipUpcoming workshop(s) available!\n\n\n\nThe next workshop is on Mon Mar 16 at 01:00 PM.\nBook in to the next offering now.\nAlternatively, check our calendar for future events.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#open-rstudio",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#open-rstudio",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Open RStudio",
    "text": "Open RStudio\nOn Library computers:\n\nLog in with your UQ username and password (use your student credentials if you are both staff and student)\nMake sure you have a working internet connection\nGo to search the magnifying glass (bottom left)\nOpen the ZENworks application\nLook for the letter R\nDouble click on RStudio which will install both R and RStudio\n\nIf you are using your own laptop:\n\nMake sure you have a working internet connection\nOpen RStudio",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#disclaimer",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#disclaimer",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Disclaimer",
    "text": "Disclaimer\nWe will assume you are an R intermediate user and that you have used ggplot2 before.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#what-are-we-going-to-learn",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#what-are-we-going-to-learn",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "What are we going to learn?",
    "text": "What are we going to learn?\nDuring this hands-on session, you will:\n\ninstall a tool for picking colours\ncustomise scales and ranges\ndivide a visualisation into facets\nexplore new geometries\nmodify statistical transformations\nadjust a geometry’s position\nfurther modify themes\nmake a plot interactive",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#setting-up",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#setting-up",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Setting up",
    "text": "Setting up\nInstall ggplot2 if you don’t already have it, with: install.packages(\"ggplot2\")\nCreate a new project to keep everything nicely contained in one directory:\n\nClick the “Create a project” button (top left cube icon)\nClick “New Directory”\nClick “New Project” (“Empty project” if you have an older version of RStudio)\nIn “Directory name”, type the name of your project, e.g. “ggplot2_intermediate”\nSelect the folder where to locate your project: e.g. Documents/RProjects, which you can create if it doesn’t exist yet. You can use your H drive at UQ to make sure you can find it again.\nClick the “Create Project” button\n\nLet’s also create a “plots” folder to store exports:\n\ndir.create(\"plots\")\n\nCreate a new script (File &gt; New File &gt; R Script) and add a few comments to give context:\n# Description : ggplot2 intermediate with gapminder data\n# Author: &lt;your name&gt;\n# Date: &lt;today's date&gt;\nFinally, make sure you load ggplot2 so we can use its functions:\n\nlibrary(ggplot2)",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#import-data",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#import-data",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Import data",
    "text": "Import data\n\nChallenge 1 – import data\nOur data is located at https://raw.githubusercontent.com/resbaz/r-novice-gapminder-files/master/data/gapminder-FiveYearData.csv\nUsing the following syntax, how can you read the online CSV data into an R object?\ngapminder &lt;- ...\nYou have to use the read.csv() function, which can take a URL:\n\ngapminder &lt;- read.csv(\n  file = \"https://raw.githubusercontent.com/resbaz/r-novice-gapminder-files/master/data/gapminder-FiveYearData.csv\")\n\nIf you are not familiar with the dataset, View() and summary() can help you explore it.\n\nView(gapminder)    # view as a separate spreadsheet\nsummary(gapminder) # summary statistics for each variable\n\nThe Environment pane gives you an overview of the variables.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#explore-data-visually",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#explore-data-visually",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Explore data visually",
    "text": "Explore data visually\nLet’s start with a question: how do Gross Domestic Product (GDP) and life expectancy relate?\nWe can make a simple plot with the basics of ggplot2:\n\nggplot(data = gapminder,\n       mapping = aes(x = gdpPercap,\n                     y = lifeExp)) +\n  geom_point()\n\n\n\n\n\n\n\n\nRemember that the 3 main elements of a ggplot2 visualisation are:\n\nthe data\nthe mapping of aesthetics to variables\nthe geometry",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#aesthetics-available",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#aesthetics-available",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Aesthetics available",
    "text": "Aesthetics available\nSo far we have been using the x and y aesthetics. There are more available, depending on the geometry that you are using.\nHere are some common examples:\n\nTo change the shape based on a variable, use shape = &lt;discrete variable&gt; inside the aes() call.\nIf you want to change the size of the geometric object, you can use the size = &lt;continuous variable&gt; argument.\nSimilarly, to change the colour based on a variable, use colour = &lt;variable&gt; and fill = &lt;variable&gt; inside the aes() call.\n\nLet’s modify our plot to colour the points according to the continent variable.\n\nggplot(data = gapminder,\n       mapping = aes(x = gdpPercap,\n                     y = lifeExp)) +\n  geom_point(aes(colour = continent)) +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\nChallenge 2 - save our plot\nHow can we save our scatter plot to the ‘plots’ directory we created using a function?\n\nggsave(filename = \"plots/gdpPercap_v_lifeExp.png\", width = 7, height = 4)\n\n\nRemember that the ggsave function saves the last plot.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#modifying-scales",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#modifying-scales",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Modifying scales",
    "text": "Modifying scales\n\nMore control over colours\nThis plot uses the default discrete palette.\n\nSaving some typing: We will keep modifying this plot. To reuse the constant base of our plot (the ggplot() call and the point geometry), we can create an object:\n\n\np &lt;- ggplot(data = gapminder,\n       mapping = aes(x = gdpPercap,\n                     y = lifeExp)) +\n  geom_point(aes(colour = continent)) +\n  geom_smooth()\n\nWe can use other palettes than the default one. ggplot2 provides extra functions to modify colour scales. For example:\n\np +\n  scale_colour_viridis_d()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nViridis is a collection of palettes that are designed to be accessible (i.e. perceptually uniform in colour or black and white, and perceivable for various forms of colour blindness). The structure of the function name is scale_&lt;aesthetic&gt;_viridis_&lt;datatype&gt;(), the different data types being discrete, continuous or binned.\nAnother collection of palettes is the ColorBrewer collection:\n\np +\n  scale_colour_brewer(palette = \"Set1\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nYou can see the palettes available by looking at the help page of scale_colour_brewer(), under the header “Palettes”. However, the names alone might not be enough to picture them, so head to http://colorbrewer2.org/ to find the one that you like. Importantly, the website allows you to tick the options “colorblind safe” and “print friendly”… which would rule out all the qualitative palettes for our 5 continents!\nA useful package that introduces many palettes for ggplot2 is the colorspace package, which promotes the Hue-Chroma-Luminance (HCL) colour space. This colour space is perceptually-based, which means it is particularly suited for human perception of colours.\nFor colorspace, the function names are structured as follows: scale_&lt;aesthetic&gt;_&lt;datatype&gt;_&lt;colorscale&gt;()\nLet’s first use an alternative qualitative palette:\n\nlibrary(colorspace)\np +\n  scale_colour_discrete_qualitative()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThis is the default ggplot2 palette! Which means ggplot2 already use a HCL palette. But having the colorspace package loaded, we can now see all the palettes available:\n\nhcl_palettes(plot = TRUE)\n\n\n\n\n\n\n\n\nLet’s try a different one:\n\np +\n  scale_colour_discrete_sequential(\"Batlow\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nFinally, to use a custom palette, we can use the ggplot2 function scale_colour_manual() and provide a list of colour names.\n\np +\n  scale_colour_manual(values = c(\"lightblue\", \"pink\", \"purple\", \"black\", \"red\"))\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nYou can list all the R colour names with the function colours(), which prints out a list of their names, but know that you are not limited to these 657 values: you can also use HEX values, which is particularly useful if you have to respect a colour scheme you were given.\nYou can find visual lists of all the R colours, but there is a way to pick colours more comfortably: we can use the colourpicker package, which adds a handy add-in to RStudio. Install it and use the new “Addins &gt; Colour Picker” tool to create a vector of colours for your custom palette.\n\nThat was a lot of options about colours, but know that in some cases, the most straight-forward way to make your visualisations readable by most is to use symbols instead of colours. In ggplot2, you would use the shape aesthetic.\n\n\n\nAxis scale modifiers\nWe could further modify our plot to make it more readable. For example, we can use a different x axis scale to distribute the data differently:\n\np +\n  scale_x_log10()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nIt is possible to use various transformations with the scale_x_continuous() function’s tranform argument.\nWe can also further customise a scale with breaks and labels:\n\np +\n  scale_x_log10(breaks = c(5e2, 1e3, 1e4, 1e5),\n                labels = c(\"500\", \"1 k\", \"10 k\", \"100 k\"))\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\n\nYou can use the scientific notation 1e5 to mean “a 1 followed by 5 zeros”.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#zooming-in",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#zooming-in",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Zooming in",
    "text": "Zooming in\nWe might want to focus on the left hand side part of our original plot:\n\np\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nTo zoom in, we might want to change our axis limits by using ylim().\n\np +\n  xlim(c(0, 6e4))\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 5 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\nNotice the warning message? ggplot2 informs us that it couldn’t represent part of the data because of the axis limits.\nThe method we use works for our point geometry, but is problematic for other shapes that could disappear entirely or change their appearance because they are based on different data: we are actually clipping our visualisation! Notice how the trend line now looks different?\nA better way to focus on one part of the plot would be to modify the coordinate system:\n\np +\n  coord_cartesian(xlim = c(0, 6e4))\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nThe Cartesian coordinate system is the default one in ggplot2. You could change the coordinate system to coord_polar() for circular visualisations, or to coord_map() to visualise spatial data.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#interactive-plots",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#interactive-plots",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Interactive plots",
    "text": "Interactive plots\nThe plotly package brings the power of the Plotly javascript library to R. Install it with install.packages(plotly), and you’ll then be able to convert a ggplot2 visualisation into an interactive HTML visualisation with one single function!\nLet’s reuse our original plot object with some modification, and feed it to ggplotly():\n\np &lt;- ggplot(data = gapminder,\n       mapping = aes(x = gdpPercap,\n                     y = lifeExp,\n                     colour = continent)) + # move the colour aesthetic\n  geom_point() +\n  # remove trend line\n  scale_x_log10() # spread the data on the x axis\nlibrary(plotly)\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\nggplotly(p)\n\n\n\n\n\nYou can now identify single points, zoom into your plot, and show/hide categories. However, the pop-up does not tell us which country the point corresponds to. That’s because we don’t mention the country variable in our code. Let’s reveal that information by slightly modifying the p object:\n\np &lt;- ggplot(data = gapminder,\n       mapping = aes(x = gdpPercap,\n                     y = lifeExp,\n                     colour = continent,\n                     label = country)) + # one extra aesthetic for an extra variable\n  geom_point() +\n  scale_x_log10()\n\nThe interactive version now tells us which country each point correspond to:\n\nggplotly(p)\n\n\n\n\n\nTo add yet another variable to the visualisation, we can animate the plot b&lt; associating the frame aesthetic with the year variable. This adds a slider and a “Play” button under the visualisation.\n\np &lt;- ggplot(data = gapminder,\n       mapping = aes(x = gdpPercap,\n                     y = lifeExp,\n                     colour = continent,\n                     label = country,\n                     frame = year)) + # one frame per year\n  geom_point() +\n  scale_x_log10()\nggplotly(p)\n\n\n\n\n\n\nYou can export the visualisation as a HTML page for sharing with others.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#histograms",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#histograms",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Histograms",
    "text": "Histograms\n\nChallenge 3 – histogram of life expectancy\nSearch for the histogram geometry function, and plot the life expectancy. How can we modify the bars?\n\nggplot(gapminder, aes(x = lifeExp)) +\n  geom_histogram() # by default, bins = 30\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\n\nSaving some typing: remember we can omit the names of the arguments if we use them in order? Being explicit about the argument names is useful when learning the ins and outs of a function, but as you get more familiar with ggplot2, you can do away with the obvious ones, like data = and mapping = (as long as they are used in the right order!).\n\nLet’s change the bin width:\n\nggplot(gapminder, aes(x = lifeExp)) +\n  geom_histogram(binwidth = 1)\n\n\n\n\n\n\n\n\nHere, each bar contains one year of life expectancy.\nWe can also change the number of bins:\n\nggplot(gapminder, aes(x = lifeExp)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nNow, let’s colour the bins by continent. Instinctively, you could try the colour aesthetic:\n\nggplot(gapminder, aes(x = lifeExp, colour = continent)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\n…but it only colours the outline of the rectangles!\nSome aesthetics will work better with some geometries than others. We have to use the fill aesthetic to colour the areas instead:\n\nggplot(gapminder, aes(x = lifeExp, fill = continent)) +\n  geom_histogram(bins = 10)\n\n\n\n\n\n\n\n\nColouring our bins allows us to experiment with the geometry’s position. The histogram geometry uses the “stack” position by default. It might convey different information if we make it use ratios instead, using the position = \"fill\" argument:\n\nggplot(gapminder,\n       aes(x = lifeExp,\n           fill = continent)) +\n  geom_histogram(bins = 10,\n                 position = \"fill\")\n\n\n\n\n\n\n\n\nYou might have noticed that the y-axis is still labeled ‘count’ when it has changed to a proportion. We can modify the y-axis labels to percent using the percent() function from the scales package.\nWe can also modify the y-axis labels with the labs() function.\n\nlibrary(scales)\n\nggplot(gapminder,\n       aes(x = lifeExp,\n           fill = continent)) +\n  geom_histogram(bins = 10,\n                 position = \"fill\") +\n  scale_y_continuous(labels = percent) +\n  labs(y = \"Percent\")\n\n\n\n\n\n\n\n\nWe can also make the bars “dodge” each other:\n\nggplot(gapminder,\n       aes(x = lifeExp,\n           fill = continent)) +\n  geom_histogram(bins = 10,\n                 position = \"dodge\")\n\n\n\n\n\n\n\n\nThis “dodged” version reads similarly to a density plot (although the number of countries per continent does not influence the size):\n\nggplot(gapminder,\n       aes(x = lifeExp,\n           colour = continent)) +\n  geom_density()",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#faceting",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#faceting",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Faceting",
    "text": "Faceting\nAn even more readable representation of the dodged histogram could use faceting:\n\nggplot(gapminder,\n       aes(x = lifeExp,\n           fill = continent)) +\n  geom_histogram(bins = 40) +\n  facet_wrap(vars(continent))\n\n\n\n\n\n\n\n\nWe have to wrap the variable(s) we want to facet by into the vars() function.\n\nFaceting is a great way to add yet another variable to your visualisation, instead of using another aesthetic. We can now compare distributions across labelled panels, with the axes using the same ranges by default.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#theming",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#theming",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Theming",
    "text": "Theming\nThe legend is probably superfluous. We want to keep the colours, but we use the theme() function to customise the look of our plot and remove the legend:\n\nggplot(gapminder,\n       aes(x = lifeExp,\n           fill = continent)) +\n  geom_histogram(bins = 40) +\n  facet_wrap(vars(continent)) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\nIf you use a pre-built theme function, make sure you place it before customising the legend. Otherwise it will bring the legend back!\n\nggplot(gapminder,\n       aes(x = lifeExp,\n           fill = continent)) +\n  geom_histogram(bins = 40) +\n  facet_wrap(vars(continent)) +\n  theme_minimal() + # before customising the legend!\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#a-more-refined-facetted-example",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#a-more-refined-facetted-example",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "A more refined facetted example",
    "text": "A more refined facetted example\nThis extra example gives an idea of how a refined ggplot2 visualisation might be constructed. It represents 4 different variables, using the larger diamonds data set.\n\nggplot(diamonds,\n       aes(x = carat,\n           y = price)) +\n  geom_point(aes(colour = color),\n             alpha = 0.5,\n             size = 0.5) +\n  scale_colour_viridis_d(option = \"turbo\") +\n  geom_smooth(se = FALSE,\n              linetype = \"dashed\",\n              colour = \"black\",\n              linewidth = 0.5) +\n  facet_wrap(vars(cut)) +\n  theme_minimal() +\n  labs(y = \"price (USD)\")\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nIn this visualisation:\n\n4 different variables are represented, thanks to both aesthetics and facets\ntwo geometries are layered on top of each other to represent a relationship\nboth geometries are customised to make the plot readable (important here, since there are close to 54,000 rows of data)\nthe default colour scale is replaced\na built-in theme is used\na label clarifies the unit of measurement",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#boxplots",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#boxplots",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Boxplots",
    "text": "Boxplots\nA simple boxplot can help visualise a distribution in categories:\n\nggplot(gapminder, aes(x = continent, y = lifeExp)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nChallenge 4 – code comprehension\nWhat do you think this extra line might do to our boxplots?\n\nggplot(gapminder, aes(x = continent, y = lifeExp)) +\n  geom_boxplot() +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\n\nThis is useful if the x labels get too cramped on the x axis: you can rotate them to whatever angle you want.\n\nTry turning this plot into an interactive visualisation to see stats easily!",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#summarise-data-and-plot",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#summarise-data-and-plot",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Summarise data and plot",
    "text": "Summarise data and plot\nLet’s try summarising the average and standard deviation of life expectancy by continent from the gapminder data and piping it directly into a ggplot. We will need to install/load dplyr for this.\n\n# install.packages(\"dplyr\")\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\ngapminder %&gt;% \n  group_by(continent) %&gt;% \n  summarise(aveLE = mean(lifeExp),\n            sdLE = sd(lifeExp)) %&gt;% \n  ggplot(aes(x = continent, \n             y = aveLE)) +\n  geom_bar(stat = \"identity\",\n           fill = \"tomato\") +\n  geom_errorbar(aes(ymin = aveLE - sdLE,\n                    ymax = aveLE + sdLE),\n                width = 0.1) +\n  labs(x = \"Continent\",\n       y = \"Life Expectancy in Years\")",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#plot-from-multiple-summarised-dataframes",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#plot-from-multiple-summarised-dataframes",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Plot from multiple summarised dataframes",
    "text": "Plot from multiple summarised dataframes\nIn ggplot2, multiple data frames can be plotted on the same plot. For example, let’s say we wanted to make a bar graph of total population by year with the gapminder data set colored by continent and also include the standard deviation of the total population per year at the top of each column as an error bar? This may or may not make the most sense in terms of representing the data, but we will do it here as an exercise.\n\nsummarise the data by year to calculate the standard deviation\nsummarise the data by continent and year for the filled columns\nplot the continent data and add the error bars from the first summary\n\n\ntotal &lt;- gapminder %&gt;% \n   group_by(year) %&gt;% # group by year\n   summarise(tot = sum(pop), # sum the population for every year\n             SD = sd(pop))\ntotal\n\n\n  \n\n\ncont_ave &lt;- gapminder %&gt;% \n  group_by(continent, year) %&gt;% \n  summarise(totalpop = sum(pop)) %&gt;% \n   ungroup()\n\n`summarise()` has grouped output by 'continent'. You can override using the\n`.groups` argument.\n\ncont_ave\n\n\n  \n\n\n\nWhen specifying a data set outside of the main ggplot() function - the data = argument must be used. The other functions geom_bar() etc. assume the first argument is mapping = aes() unless the data argument is explicitly defined.\n\nggplot(data = cont_ave, aes(x = year, \n             y = totalpop)) +\n  geom_bar(aes(fill = continent),\n           stat = \"identity\") + # default is `stat = count` like a histogram\n  geom_errorbar(data = total, # must use `data = ` to specify a new data set being used\n                 aes(y = tot, # the y from the ggplot(aes()) is inherited, update\n                     ymin = tot - SD, # error bar arguments\n                     ymax = tot + SD),\n                width = 0.8) +\n      scale_x_continuous(breaks = seq(from = 1952, to = 2007, by = 5)) + # use seq to get years from 1952 - 2007 every 5 yrs to label every column\n   scale_y_continuous(breaks = c(2e9, 4e9, 6e9), # keep the same breaks\n      labels = c(\"2\", \"4\", \"6\")) + # relabel so number represent billions\n   labs(x = \"Year\", \n        y = \"Population in Billions\", # rename units in billions\n        fill = \"Continent\") + # relabel legend from the 'fill' in the geom_bar\n   theme_dark() + # different theme \n   theme(panel.grid = element_blank()) # remove the grid lines",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#close-project",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#close-project",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Close project",
    "text": "Close project\nClosing RStudio will ask you if you want to save your workspace and scripts. Saving your workspace is usually not recommended if you have all the necessary commands in your script.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/ggplot2_intermediate/ggplot2_intermediate.html#useful-links",
    "href": "R/ggplot2_intermediate/ggplot2_intermediate.html#useful-links",
    "title": "R ggplot2: intermediate data visualisation",
    "section": "Useful links",
    "text": "Useful links\n\nFor visualisations:\n\nggplot2 cheatsheet\nOfficial ggplot2 documentation\nOfficial ggplot2 website\nChapter on data visualisation in the book R for Data Science\nFrom Data to Viz, a website to explore different visualisations and the code that generates them\nSelva Prabhakaran’s r-statistics.co section on ggplot2\nCoding Club’s data visualisation tutorial\nSTHDA’s ggplot2 essentials\nLear more about plotly and exploratory data analysis with the book Interactive web-based data visualization with R, plotly, and shiny\n\nOur compilation of general R resources",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "4. ggplot2: intermediate visualisation"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html",
    "href": "R/heatmaps/heatmaps_intermediate.html",
    "title": "R data visualisation: heatmaps",
    "section": "",
    "text": "A heatmap is a way of visualising a table of numbers, where you substitute the numbers with colored cells. It’s useful for finding highs and lows, and see patterns more clearly. There are many functions available in R to create this kind of visualisations, but we will focus on four options here.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#what-are-we-going-to-learn",
    "href": "R/heatmaps/heatmaps_intermediate.html#what-are-we-going-to-learn",
    "title": "R data visualisation: heatmaps",
    "section": "What are we going to learn?",
    "text": "What are we going to learn?\nDuring this session, you will:\n\nLearn how to produce a simple heatmap with the base function heatmap();\nLearn about alternatives to produce more complex heatmaps, like heatmap.2() and pheatmap();\nLearn how to produce a rudimentary heatmap with the ggplot2 package.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#disclaimer",
    "href": "R/heatmaps/heatmaps_intermediate.html#disclaimer",
    "title": "R data visualisation: heatmaps",
    "section": "Disclaimer",
    "text": "Disclaimer\nWe will assume you are an R intermediate user and that you have used RStudio before.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#open-rstudio",
    "href": "R/heatmaps/heatmaps_intermediate.html#open-rstudio",
    "title": "R data visualisation: heatmaps",
    "section": "Open RStudio",
    "text": "Open RStudio\n\nInstallation instructions are available if you don’t have R and RStudio installed already.\n\n\nOn your own computer:\n\nOpen RStudio\nMake sure you have a working Internet connection\n\nOn Library computers:\n\nLog in with your UQ username and password\nMake sure you have a working Internet connection\nOpen the ZENworks application\nLook for the letter “R”\nDouble click on RStudio which will install both R and RStudio",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#essential-shortcuts",
    "href": "R/heatmaps/heatmaps_intermediate.html#essential-shortcuts",
    "title": "R data visualisation: heatmaps",
    "section": "Essential shortcuts",
    "text": "Essential shortcuts\n\nfunction or dataset help: press F1 with your cursor anywhere in a function name.\nexecute from script: Ctrl + Enter\nassignment operator (&lt;-): Alt + -",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#r-project",
    "href": "R/heatmaps/heatmaps_intermediate.html#r-project",
    "title": "R data visualisation: heatmaps",
    "section": "R Project",
    "text": "R Project\nEverything we write today will be saved in your script, so please remember to create your project on your H drive (or USB stick) if you use the University computers, so you can go back to it after the session.\n\nCreate a new project:\n\nClick the “New project” menu icon\nClick “New Directory”\nClick “New Project”\nIn “Directory name”, type the name of your project, e.g. “heatmaps”\nBrowse and select a folder where to locate your project (for example, an “r_projects” directory where all your projects live)\nClick the “Create Project” button\n\nCreate new folders with the following commands:\n\n\ndir.create(\"scripts\")\ndir.create(\"plots\")\n\n\nCreate a new R script called “heatmaps.R” in the “scripts” folder:\n\n\nfile.create(\"scripts/heatmaps.R\")\nfile.edit(\"scripts/heatmaps.R\")",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#method-1-the-base-heatmap-function",
    "href": "R/heatmaps/heatmaps_intermediate.html#method-1-the-base-heatmap-function",
    "title": "R data visualisation: heatmaps",
    "section": "Method 1: the base heatmap() function",
    "text": "Method 1: the base heatmap() function\nAs a first example, we will use a built-in dataset called mtcars.\n\nExplore the data\n\n?mtcars\ndim(mtcars)\nstr(mtcars)\nhead(mtcars)\nView(mtcars)\n\n\n\nPrepare data\nThe data is a dataframe, but it has to be a numeric matrix to make your heatmap. Dataframes can contain variables with different data classes, whereas matrices only contain one data class.\n\nclass(mtcars)\n\n[1] \"data.frame\"\n\nmtcars_matrix &lt;- data.matrix(mtcars) # convert a DF to a numeric matrix\nclass(mtcars_matrix)\n\n[1] \"matrix\" \"array\" \n\n\n\n\nVisualise\nWe are now going to use the heatmap() function to create our first heatmap:\n\nheatmap(mtcars_matrix)\n\n\n\n\n\n\n\n\nDoes it look like what you expected?\nLook at the function’s help page, and read the description of the scale argument in particular:\n\n?heatmap\n\nScale is important: the values should be centered and scaled in either rows or columns. In our case, we want to visualise highs and lows in each variable, which are in columns.\n\nheatmap(mtcars_matrix, scale = \"column\")\n\n\n\n\n\n\n\n\nWe can now see the high (red) and low (white) values in each variable, and visualise groups of similar cars.\n\n\nColours\nWith versions of R up to 3.5, the default heatmap palette was heat.colors(), which is not the most intuitive as it goes from red for low values to white for high values.\nSince R 3.6, the default palette is “YlOrRd”, which stand for “Yellow, Orange, Red”.\nYou can however replace the default palette and use different colours, and different numbers of levels. For example, in the palette function cm.colors(n), n is the number of levels (&gt;= 1) contained in the cyan-to-magenta palette. This function can be used in the col argument:\n\nheatmap(mtcars_matrix,\n        scale = \"column\",\n        col = cm.colors(n = 15))\n\n\n\n\n\n\n\n\nYou can try other functions, like terrain.colors() or hcl.colors() (in R &gt; 3.6), and you can reverse them with the rev = TRUE argument.\n\n\nChallenge 1: Remove dendrograms\nDoes it make sense to have both columns and rows for this dataset?\nLook at the help documentation for heatmap to see if the dendrograms can be removed for rows and/or columns.\n\nHint: see the Rowv and Colv arguments.\n\n\nheatmap(mtcars_matrix,\n        scale = \"column\",\n        col = cm.colors(15),\n        Colv = NA)\n\n\n\n\n\n\n\n\n\nIf dendrograms are removed, the data won’t be reorganised according to the clustering method.\n\n\n\nClean the environment\nWe can start with a fresh environment, using:\n\nrm(list = ls())",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#method-2-gplotsheatmap.2",
    "href": "R/heatmaps/heatmaps_intermediate.html#method-2-gplotsheatmap.2",
    "title": "R data visualisation: heatmaps",
    "section": "Method 2: gplots::heatmap.2()",
    "text": "Method 2: gplots::heatmap.2()\nIf you don’t have the gplots package yet, use install.packages(\"gplots\").\n\nlibrary(gplots)\n\nThis gplots heatmap function provides a number of extensions to the standard R heatmap function.\n\n?heatmap.2\n\n\nProtein data example\nThis dataset contains observations for 63 proteins in three control experiments and three experiments where cells are treated with a growth factor. We need to import it from the web:\n\nrawdata &lt;- read.csv(\"https://raw.githubusercontent.com/ab604/heatmap/master/leanne_testdata.csv\")\n\nWe can then explore the data:\n\nstr(rawdata)\nhead(rawdata)\nView(rawdata)\n\nIt’s important to note that a lot of visualisations involve gathering and preparing data. Rarely do you get data exactly how you need it, so you should expect to do some data munging before producing the visuals.\nHere, we need to remove useless columns, and we also want to rename them for clarity.\n\nprot_data &lt;- rawdata[ , 2:7] # remove superfluous columns\ncolnames(prot_data) &lt;- c(paste(\"Control\", 1:3, sep = \"_\"), \n                       paste(\"Treatment\", 1:3, sep = \"_\"))\n\nWe also need to convert the dataframe to a matrix, just like in our first example.\n\nclass(rawdata)\n\n[1] \"data.frame\"\n\nprot_matrix &lt;- data.matrix(prot_data)\nclass(prot_matrix)\n\n[1] \"matrix\" \"array\" \n\n\nWe can now visualise the data with heatmap.2():\n\nheatmap.2(prot_matrix)\n\n\n\n\n\n\n\n\n\nThe scale argument in heatmap.2() is by default set to \"none\"!\n\nFor a more informative visualisation, we can scale the data for each protein:\n\nheatmap.2(prot_matrix,\n          scale = \"row\")\n\n\n\n\n\n\n\n\nWe can now see each protein’s response to treatments.\n\nNotice how the visualisation is more readable, but the clustering does not take into account the scaling? That’s because the scaling is done after the clustering.\n\nWith heatmap.2(), if we want to cluster rows according to the scaled data, we have to scale it prior to generating the heatmap.\n\n?scale\n\nscale() is a function that centres and scales the columns of a numeric matrix. We transpose the matrix with t() to then centre and scale each protein’s data (i.e. the rows) with scale(). Finally, we transpose the data back to the original form.\n\nprot_scaled &lt;- prot_matrix |&gt; \n  t() |&gt; \n  scale() |&gt; \n  t()\n\nLet’s visualise it once more:\n\nheatmap.2(prot_scaled)\n\n\n\n\n\n\n\n\nWe can now see clear groups.\n\n\nMore control over colours\nLet’s create a new palette function:\n\nmy_palette &lt;- colorRampPalette(c(\"blue\",\n                                 \"white\",\n                                 \"red\")) # from low to high\n\nNow, we can use it and further customise our heatmap:\n\nheatmap.2(prot_scaled,\n          trace = \"none\",               # turn off trace lines from heatmap\n          col = my_palette(25))         # use my colour scheme with 25 levels\n\n\n\n\n\n\n\n\nFix a few things and add a few extras:\n\nheatmap.2(prot_scaled,\n          Colv = FALSE,               # no clustering on columns\n          trace = \"none\",\n          col = my_palette(25),\n          main = \"Protein abundance\", # add title\n          margins = c(6, 3),          # more space from border\n          keysize = 2,                # make key and histogram bigger\n          cexRow = 0.4,               # amend row font\n          cexCol = 0.8)               # amend column font\n\nWarning in heatmap.2(prot_scaled, Colv = FALSE, trace = \"none\", col =\nmy_palette(25), : Discrepancy: Colv is FALSE, while dendrogram is `both'.\nOmitting column dendogram.\n\n\n\n\n\n\n\n\n\nTo suppress the warning, you can also specify which dendrogram you want to show:\n\nheatmap.2(prot_scaled,\n          Colv = FALSE,\n          dendrogram = \"row\",     # only show the row dendrogram\n          trace = \"none\",\n          col = my_palette(25),\n          main = \"Protein abundance\",\n          margins = c(6, 3),\n          keysize = 2,\n          cexRow = 0.4,\n          cexCol = 0.8)\n\n\n\n\n\n\n\n\nClean up the environment with:\n\nrm(list = ls())",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#method-3-pheatmappheatmap",
    "href": "R/heatmaps/heatmaps_intermediate.html#method-3-pheatmappheatmap",
    "title": "R data visualisation: heatmaps",
    "section": "Method 3: pheatmap::pheatmap()",
    "text": "Method 3: pheatmap::pheatmap()\nIf you don’t have it already, install pheatmap with install.packages(\"pheatmap\").\nLoad the required package with:\n\nlibrary(pheatmap)\n\nHow does pheatmap() (which stands for “pretty heatmap”) differ from other functions?\n\n?pheatmap\n\n\nA function to draw clustered heatmaps where one has better control over some graphical parameters such as cell size, etc.\n\nCreate a data matrix from pseudo-random numbers:\n\nd &lt;- matrix(rnorm(25), nrow = 5)\ncolnames(d) &lt;- paste0(\"Treatment\", 1:5)\nrownames(d) &lt;- paste0(\"Gene\", 1:5)\n\nTry it out:\n\npheatmap(d)\n\n\n\n\n\n\n\n\nBy default, pheatmap adapts to the available space. You can however fix the size of the cells, for example to squares:\n\npheatmap(d, \n         main = \"Pretty heatmap\",\n         cellwidth =  30,\n         cellheight = 30,\n         fontsize = 10,\n         display_numbers = TRUE)\n\n\n\n\n\n\n\n\n\nBy default, the scale argument is set to \"none\". If you do scale the data, the clustering will take it into account (i.e. the clustering happens after the scaling).\n\n\npheatmap(d, \n         main = \"Pretty heatmap\",\n         cellwidth =  50,\n         cellheight = 30,\n         fontsize = 12,\n         display_numbers = TRUE,\n         scale = \"row\")\n\n\n\n\n\n\n\n\nYou can save your plot with an extra argument:\n\npheatmap(d, \n         main = \"Pretty heatmap\",\n         cellwidth =  50,\n         cellheight = 30,\n         fontsize = 12,\n         filename = \"plots/heatmap.pdf\")\n\nClean up your environment with:\n\nrm(list = ls())",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#summary-of-first-three-methods",
    "href": "R/heatmaps/heatmaps_intermediate.html#summary-of-first-three-methods",
    "title": "R data visualisation: heatmaps",
    "section": "Summary of first three methods",
    "text": "Summary of first three methods\nThe first three methods differ in their default settings and in the order of the processing steps:\nstats::heatmap():     scale (row) -&gt; cluster -&gt; colour\ngplots::heatmap.2():  cluster -&gt; scale (none) -&gt; colour\npheatmap::pheatmap(): scale (none) -&gt; cluster -&gt; colour",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#method-4-a-dataframe-in-ggplot2",
    "href": "R/heatmaps/heatmaps_intermediate.html#method-4-a-dataframe-in-ggplot2",
    "title": "R data visualisation: heatmaps",
    "section": "Method 4: a dataframe in ggplot2",
    "text": "Method 4: a dataframe in ggplot2\nIf you want to stick to the ggplot2 package for all your data visualisation, there is a way to create a simple heatmap (without clustering). So far, we have seen methods that make use of data matrices; however, ggplot2 deals with dataframes that contain “tidy data”.\nIf you don’t have ggplot2 installed on your system, you can do that with the command install.packages(\"ggplot2\").\nLoad the necessary library:\n\nlibrary(ggplot2)\n\nWe are using a built-in dataset about oesophageal cancer occurrence: esoph.\n\n?esoph\n\nLet’s subset the data we want to look at, i.e. only 55-64 year-olds:\n\nesoph_sub &lt;- subset(esoph, agegp == \"55-64\")\n\nCreate a basic heatmap from the dataframe:\n\nggplot(esoph_sub, aes(x = alcgp,\n                      y = tobgp,\n                      fill = ncases / (ncases + ncontrols))) +\n  geom_tile() +\n  scale_fill_gradient(low = \"white\",\n                      high = \"darkred\") +\n  theme_minimal() +\n  labs(fill = \"Cancer freq.\",\n       x = \"Alcohol consumption\",\n       y = \"Tobacco consumption\")\n\n\n\n\n\n\n\n\nThis ggplot2 method does not allow to create dendrograms. However, other packages are based on ggplot2 and support dendrograms. (Search for “heatmap” in the ggplot2 extension gallery.)\nClean up your environment with:\n\nrm(list = ls())",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#optional-method-5-complexheatmapheatmap",
    "href": "R/heatmaps/heatmaps_intermediate.html#optional-method-5-complexheatmapheatmap",
    "title": "R data visualisation: heatmaps",
    "section": "(optional) Method 5: ComplexHeatmap::Heatmap",
    "text": "(optional) Method 5: ComplexHeatmap::Heatmap\nThis extra method come from a different repository than the official CRAN repositories: the Bioconductor project.\nThe package we use is the ComplexHeatmap package, which is fully documented here.\nStep 1: install and load\nBiocManager is used to install Bioconductor packages.\n\n# install.packages(\"BiocManager\")\n# BiocManager::install(\"ComplexHeatmap\")\nlibrary(ComplexHeatmap)\n\nLoading required package: grid\n\n\n========================================\nComplexHeatmap version 2.26.0\nBioconductor page: http://bioconductor.org/packages/ComplexHeatmap/\nGithub page: https://github.com/jokergoo/ComplexHeatmap\nDocumentation: http://jokergoo.github.io/ComplexHeatmap-reference\n\nIf you use it in published research, please cite either one:\n- Gu, Z. Complex Heatmap Visualization. iMeta 2022.\n- Gu, Z. Complex heatmaps reveal patterns and correlations in multidimensional \n    genomic data. Bioinformatics 2016.\n\n\nThe new InteractiveComplexHeatmap package can directly export static \ncomplex heatmaps into an interactive Shiny app with zero effort. Have a try!\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(ComplexHeatmap))\n========================================\n! pheatmap() has been masked by ComplexHeatmap::pheatmap(). Most of the arguments\n   in the original pheatmap() are identically supported in the new function. You \n   can still use the original function by explicitly calling pheatmap::pheatmap().\n\n\n\nAttaching package: 'ComplexHeatmap'\n\n\nThe following object is masked from 'package:pheatmap':\n\n    pheatmap\n\nlibrary(circlize) # for the colorRamp2() function\n\n========================================\ncirclize version 0.4.17\nCRAN page: https://cran.r-project.org/package=circlize\nGithub page: https://github.com/jokergoo/circlize\nDocumentation: https://jokergoo.github.io/circlize_book/book/\n\nIf you use it in published research, please cite:\nGu, Z. circlize implements and enhances circular visualization\n  in R. Bioinformatics 2014.\n\nThis message can be suppressed by:\n  suppressPackageStartupMessages(library(circlize))\n========================================\n\n\nHow is the Heatmap() function different to the base heatmap()?\n\n?Heatmap\n\nStep 2: create and manipulate data\nCreate a data matrix:\n\nSee this StackOverflow article on the importance of setting a seed.\n\n\nset.seed(123)\nmat &lt;- cbind(rbind(matrix(rnorm(16, -1), 4),\n                   matrix(rnorm(32, 1), 8)),\n             rbind(matrix(rnorm(24, 1), 4),\n                   matrix(rnorm(48, -1), 8)))\n\nPermute the rows and columns:\n\nmat &lt;- mat[sample(nrow(mat),\n                  nrow(mat)),\n           sample(ncol(mat),\n                  ncol(mat))]\nrownames(mat) &lt;- paste0(\"R\", 1:12)\ncolnames(mat) &lt;- paste0(\"C\", 1:10)\n\nStep 3: make a heatmap\n\nHeatmap(mat)\n\n\n\n\n\n\n\n\nModify the colour and the labels, remove dendrograms (and don’t cluster the data):\n\nHeatmap(mat, \n        col = colorRamp2(c(-3, 0, 3),\n                         c(\"brown\", \"white\", \"yellow\")), \n        cluster_rows = FALSE, \n        cluster_columns = FALSE,\n        heatmap_legend_param = list(title = \"Values\"))\n\n\n\n\n\n\n\n\nThe cluster_ arguments can take external clustering information, which means you can use any type of clustering method.\nNow, let’s see how this function deals with missing values:\n\nmat_with_na &lt;- mat\nmat_with_na[sample(c(TRUE, FALSE),\n                   nrow(mat)*ncol(mat),\n                   replace = TRUE,\n                   prob = c(1, 9))] &lt;- NA\nHeatmap(mat_with_na, \n        col = topo.colors(100),\n        na_col = \"orange\", \n        clustering_distance_rows = \"pearson\",\n        heatmap_legend_param = list(title = \"Values\"))\n\nWarning: NA exists in the matrix, calculating distance by removing NA values.\n\n\n\n\n\n\n\n\n\nHeatmap() automatically removes NA values to calculate the distance.\nWe can also reorganise dendrograms and labels:\n\nHeatmap(mat, \n        name = \"abundance\", \n        row_names_side = \"left\", \n        row_dend_side = \"right\", \n        column_names_side = \"top\", \n        column_dend_side = \"bottom\")\n\n\n\n\n\n\n\n\nTo separate clusters, we can use the km argument, which allows k-means clustering on rows.\n\nHeatmap(mat, \n        name = \"abundance\", \n        row_names_side = \"left\", \n        row_dend_side = \"right\", \n        column_names_side = \"top\", \n        column_dend_side = \"bottom\",\n        km = 2)\n\n\n\n\n\n\n\n\nWe can add options, save the base plot as an object and then slightly modify if with the draw() function:\n\nh1 &lt;- Heatmap(mat, \n        name = \"abundance\", \n        col = topo.colors(50),\n        color_space = \"sRGB\",\n        row_dend_width = unit(1, \"cm\"),\n        column_dend_height = unit(1, \"cm\"),\n        row_dend_reorder = TRUE,\n        column_dend_reorder = TRUE,\n        row_names_gp = gpar(fontsize = 7),\n        column_names_gp = gpar(fontsize = 9),\n        column_names_max_height = unit(2, \"cm\"),\n        row_names_max_width = unit(9, \"cm\"),\n        column_title = \"This is a complex heatmap\")\ndraw(h1, heatmap_legend_side = \"left\")\n\n\n\n\n\n\n\n\nClean my environment with:\n\nrm(list = ls())",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#close-r-project",
    "href": "R/heatmaps/heatmaps_intermediate.html#close-r-project",
    "title": "R data visualisation: heatmaps",
    "section": "Close R project",
    "text": "Close R project\nWhen closing RStudio, you should be prompted to save your workspace. If your script contains all the steps required to generate your data and visualisations, it is best practice to not save your workspace: you can execute the whole script when you go back to your project.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/heatmaps/heatmaps_intermediate.html#further-resources",
    "href": "R/heatmaps/heatmaps_intermediate.html#further-resources",
    "title": "R data visualisation: heatmaps",
    "section": "Further resources",
    "text": "Further resources\n\nMore on R heatmaps:\n\na more in-depth example with pheatmap\nLeanne Wicken’s dataset with heatmap.2 and an interactive heatmap with d3heatmap\nthe R Graph Gallery of heatmaps\n\nMore heatmap packages:\n\nFull reference on ComplexHeatmap\ntidyHeatmap, built on ComplexHeatmap but for tidy data\ntidyheatmaps, built on pheatmap but for tidy data\niheatmapr, for richer interactive heatmaps\nSearch for “heatmap” in the ggplot2 extension gallery\n\nOur compilation of R resources",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "7. Heatmaps"
    ]
  },
  {
    "objectID": "R/packaging/packaging.html",
    "href": "R/packaging/packaging.html",
    "title": "R advanced: packaging and sharing functions",
    "section": "",
    "text": "The strength of the R programming language lies in the endless possibilities custom functions can bring, and a community of users who happily share their contributions with peers.\nThis session is directed at intermediate to advanced R users wanting to learn about creating, packaging and sharing functions.\nIn this session, you will learn about:",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "9. Functions and packaging"
    ]
  },
  {
    "objectID": "R/packaging/packaging.html#setting-up",
    "href": "R/packaging/packaging.html#setting-up",
    "title": "R advanced: packaging and sharing functions",
    "section": "Setting up",
    "text": "Setting up\n\nDownload the project\nSo we can get straight into the interesting stuff, we have an R project that already contains relevant custom functions: download the archive.\nUnzip this archive, and open the .Rproj file to open the project in RStudio.\n\n\nInstall packages\nLet’s make sure we have the whole Tidyverse packages ready. If you don’t have them installed on your computer just yet, run this command in the Console:\n\ninstall.packages(\"tidyverse\")\n\n\n\nCreate a script\nCreate a new script by using the New File menu (first icon in the toolbar) and selecting “R Script”.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "9. Functions and packaging"
    ]
  },
  {
    "objectID": "R/packaging/packaging.html#how-to-build-a-function",
    "href": "R/packaging/packaging.html#how-to-build-a-function",
    "title": "R advanced: packaging and sharing functions",
    "section": "How to build a function",
    "text": "How to build a function\nIn R, once we find we are limited by the functions available (in both R and in packages available online), there always is the possibility of designing our own functions.\nThis is what a basic function definition looks like:\n\nhuman_age &lt;- function(dog_age) {\n  dog_age * 7\n}\n\nHere, we create a custom function that will take the age of a dog, and convert it to human years.\nWe need to:\n\ngive the function a name (just like when we create an object)\nspecify which arguments are available when the function is used\ndefine what happens when the function is called, in between the curly braces {}\n\nAfter executing this block of code, we have defined our function, and we can see it listed in the Environment panel. We can now use it just like any other function:\n\nhuman_age(12)\n\n[1] 84\n\n\nAs you can see, functions will by default return the last evaluated element.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "9. Functions and packaging"
    ]
  },
  {
    "objectID": "R/packaging/packaging.html#our-example-acorn-data",
    "href": "R/packaging/packaging.html#our-example-acorn-data",
    "title": "R advanced: packaging and sharing functions",
    "section": "Our example ACORN data",
    "text": "Our example ACORN data\nLet’s have a look at our pre-defined functions now.\nThe data that we want to deal with comes from the Bureau of Meteorology website. You can find more information about it here: http://www.bom.gov.au/climate/data/acorn-sat/#tabs=Data-and-networks\nThis project provides temperature data for 112 Australian weather stations. We want to use the maximum daily temperature data, which means we will have to deal with 112 separate CSV files.\n\nDownload the data\nWe will use a custom function that downloads the data. Because the data is provided as a zipped archive, we need to do two things in the body of our function:\n\ndownload the archive from the relevant URL (with download.file())\nunzip it into a directory (with untar())\n\nOpen the “get_acorn.R” file from the Files panel, and look at the code it contains:\n\n\nget_acorn &lt;- function(dest) {\n  # download the archive of station data from BOM\n  download.file(url = \"ftp://ftp.bom.gov.au/anon/home/ncc/www/change/ACORN_SAT_daily/acorn_sat_v2.5.0_daily_tmax.tar.gz\",\n                destfile = \"acorn_sat_v2.5.0_daily_tmax.tar.gz\")\n  # extract it into a directory\n  if (!dir.exists(dest)) {\n    dir.create(dest)\n  }\n  untar(tarfile = \"acorn_sat_v2.5.0_daily_tmax.tar.gz\",\n        exdir = dest)\n}\n\nThe only argument we make available in the function is the dest argument: the destination of the files, i.e. the name of the directory where we want to store the files.\nSee the if statement? We are using branching: if the directory does not exist, it will be created. If it already exist, it will move on to the next step without executing the dir.create(dest) command.\nTo have access to this new funtion, make sure you execute the whole block of code. Back in our script, we can now call our new function to get the data:\n\nget_acorn(dest = \"acorn_data\")\n\nWe now have the 112 CSV files. How do we import and clean one single station’s data?\n\n\nRead a single file\nThe package readr provides a read_csv() function to import data from a CSV files.\n\nWe will use several tidyverse packages, so we might as well load the core Tidyverse packages.\n\n\nlibrary(tidyverse)\nread_csv(\"acorn_data/tmax.001019.daily.csv\")\n\n\n  \n\n\n\nLooks like we need to fill down the station ID, then remove the first row, which we can do by piping extra fill() and slice() steps:\n\nread_csv(\"acorn_data/tmax.001019.daily.csv\") |&gt;\n  fill(3) |&gt; # fill down station ID\n  slice(-1) # remove the first row\n\n\n  \n\n\n\nWe also want to remove the superfluous column, and rename the maximum temperature variable, which can be done in one go with the select() function:\n\nread_csv(\"acorn_data/tmax.001019.daily.csv\") |&gt;\n    fill(3) |&gt; # fill down station ID\n    slice(-1) |&gt;  # remove first row\n    select(date, station = 3, max.temp = 2) # keep interesting columns\n\n\n  \n\n\n\nThis is probably the data we want to end up with when reading a file.\nHave a look at the function defined in “read_station.R”:\n\nread_station &lt;- function(file) {\n  read_csv(file, # read the CSV\n           show_col_types = FALSE) |&gt; # be more quiet\n    fill(3) |&gt; # fill down station ID\n    slice(-1) |&gt;  # remove first row\n    select(date, station = 3, max.temp = 2) # keep interesting columns\n}\n\nThis is pretty much the steps we used before, made into a function. The only argument is file, to provide the name of the file we want to read.\nNote also the extra argument show_col_types = FALSE used to suppress noisy messages.\nMake sure you define this function by executing the code. You can also use the “Source” button at the top right of the source panel.\nBack in our script, we can now test it on our first file:\n\nread_station(\"acorn_data/tmax.001019.daily.csv\")\n\n\n  \n\n\n\n\nIt is often useful to define functions in a separate script to the data analysis script. Know that you can then “source” those custom functions at the beginning of the analysis script thanks to the source() function.\n\n\n\nRead and merge all the data\nWe now want to iterate over all the files, and create a single merged dataframe.\nWe can start with finding all the relevant files in the data directory:\n\nfiles &lt;- list.files(path = \"acorn_data\", # where to look\n           pattern = \"tmax*\",                         # what to look for\n           full.names = TRUE)                         # store full path\n\nSee that files is a character vector containing the name of 112 files?\nWe can then apply our custom function iteratively to each file. For that, purrr’s map_ function family is very useful. Because we want to end up with a dataframe, we will use map_dfr():\nWe now have close to 4 million rows in our final dataframe all_tmax.\nWe might also want to create a function from the two previous steps, so we only have to provide the name of the directory where the CSV files are located. That’s what we have in the “merge_acorn.R” file:\n\nmerge_acorn &lt;- function(dir) {\n  files &lt;- list.files(path = dir,\n           pattern = \"tmax*\",\n           full.names = TRUE)\n  map_dfr(files, read_station)\n}\n\nLet’s source this function, and try it in our script:\n\nall_tmax &lt;- merge_acorn(\"acorn_data\")\n\nThis does the same as before: we have a final merged dataset of all the max temperatures from ACORN.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "9. Functions and packaging"
    ]
  },
  {
    "objectID": "R/packaging/packaging.html#making-functions-more-resilient",
    "href": "R/packaging/packaging.html#making-functions-more-resilient",
    "title": "R advanced: packaging and sharing functions",
    "section": "Making functions more resilient",
    "text": "Making functions more resilient\nWe can make our functions more resilient by adding stop() and warning() calls.\nFor example, what if our merge_acorn() function is not provided with a valid path?\n\nmerge_acorn(\"blah\")\n\nWe could improve our function with an if statement and a stop() function:\n\nmerge_acorn &lt;- function(dir) {\n  if (!dir.exists(dir)) {\n    stop(\"the directory does not exist. Please provide a valid path as a string.\")\n  }\n  files &lt;- list.files(path = dir,\n           pattern = \"tmax*\",\n           full.names = TRUE)\n  map_dfr(files, read_station)\n}\n\nNow, let’s see what happens if we don’t provide a valid directory:\n\nmerge_acorn(\"bleh\")\n\nThis will neatly stop our function and provide an error message if the path is not found.\n\nUse the data\nLet’s have a look at a summary of our data:\n\nsummary(all_tmax)\n\nNow that we have very usable data, why not create a visualisation? For example, let’s have a look at how the yearly mean of max temperatures evolved over the years:\n\nmean_max &lt;- all_tmax |&gt; \n  filter(!is.na(max.temp)) |&gt;              # remove rows with missing temperature\n  group_by(year = year(date), station) |&gt; \n  filter(n() &gt; 250) |&gt;                     # keep station-years with enough data\n  summarise(max.temp = mean(max.temp)) |&gt;  # by year and station\n  summarise(max.temp = mean(max.temp), n_stations = n()) # by year only\n\nggplot(mean_max, aes(x = year, y = max.temp)) +\n  geom_point(aes(colour = n_stations)) +   # colour the points by number of sites\n  geom_smooth() +                          # trend line\n  labs(y = \"Yearly max temp average (°C)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWe now want to share our useful functions with the World!",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "9. Functions and packaging"
    ]
  },
  {
    "objectID": "R/packaging/packaging.html#packaging-functions",
    "href": "R/packaging/packaging.html#packaging-functions",
    "title": "R advanced: packaging and sharing functions",
    "section": "Packaging functions",
    "text": "Packaging functions\n\nSome useful packages for package development: devtools, usethis, roxygen2\n\nTo prepare for packaging our functions, we would ideally have them in separate scripts named after the function they define, which is already the case for our three functions: “get_acorn.R”, “read_station.R” and “merge_acorn.R”.\nNow, let’s create a new project for our package, to keep things tidy: File &gt; New Project… &gt; New Directory &gt; R Package.\nLet’s name our package “acornucopia”.\nWe can pick the three function scripts we created before as the base for our package.\nWe end up with a basic package structure:\n\nDESCRIPTION\nman\nNAMESPACE\nR\n\nLet’s go through those main components.\n\n1. Description\nThis is the general description of the what the package does, who developed and maintains it, what it needs to work, and what licence it is released under.\nThe file already contains a template for us to fill in. We can update the fields with relevant information, in particular for Title, Author, Maintainer, Description and License:\nPackage: acornucopia\nType: Package\nTitle: Process ACORN data\nVersion: 0.1.0\nAuthor: Your Name\nMaintainer: Your Name &lt;yourself@somewhere.net&gt;\nImports: readr, dplyr, purrr\nDescription: Functions to import, cleanup and merge temperature data from ACORN stations\nLicense: GPL-3\nEncoding: UTF-8\nLazyData: true\nNotice that we added the Imports: field. This allows us to specify what extra packages are needed for our package to work.\n\nWhich licence?\nGPL or MIT are common licences for R packages. They are Open Source and allow others to reuse your code, which is the norm in the R ecosystem. However, they differ in how much can be done when creating a derivative: MIT is more “permissive” in that it allows creating closed source derivatives, whereas GPL is more “viral” in that only compatible open source licences can be used. To help you pick one, try the “Choose a License” website.\n\n\n\n2. NAMESPACE\nThe NAMESPACE file lists the functions available to the user when the package is loaded with library().\nBy default, it uses a regular expression that will match anything with a name that starts with one or more letters (in the R directory).\n\n\n3. R\nThis is where our function definitions go. If you haven’t imported the three scripts when creating the package project, copy and paste them in now.\n\nDocumenting functions\nWe are using the package roxygen2 to document each function.\nWith a function’s R script open in the source pane, and your cursor inside the function’s code, go to Code &gt; Insert Roxygen Skeleton. This will add a template above your function, which can be used to generate the function’s documentation. You can see that it is adapted to your code. For example, for the read_station() function:\n#' Read a station's CSV file\n#'\n#' This function imports and cleans a single ACORN station CSV file.\n#'\n#' @param file Path and filename of CSV file to read (as a string)\n#'\n#' @return A clean tibble\n#' @export\n#' @import dplyr\n#' @importFrom readr read_csv\n#' @examples\n#' \\dontrun{\n#' read_station(\"path/to/file.csv\")\n#' }\n\nup the top, the first sentence is the title, and the following paragraph gives a description. A third section can be used to give more details.\n@export can be used as is to populate the NAMESPACE file with this function\nwe added the @import and @importFrom tags to specify precisely what package or functions need to be imported for our function to work.\n\\dontrun{} can be used for examples that will not work, or that shouldn’t be executed for a variety of reasons.\n\n\nPressing Return inside the Roxygen skeleton will automatically prepend the necessary comment characters #'\n\nWe can then generate and view the help file with:\n\ndevtools::document()\n?read_station\n\nNotice that we get a warning about NAMESPACE not being generated by roxygen2. If we want roxygen2 to take care of this file, we can first delete it, and then run the document() function again.\nNAMESPACE is now populated with the exports and imports defined in the Roxygen documentation.\n\n\nChallenge 1: document a function\nUse roxygen2 to create the documentation for merge_acorn(). Try generating the documentation again, and check your NAMESPACE.\n\n\n\nTesting the package\nAt any time, we can load the package to test its functions:\nCtrl + Shift + L\nAnd to check our package for issues, we can use the “Check” button in the Build tab, or the following command:\n\ndevtools::check()\n\nThis function does a very thorough check, and will take some time to go through all the steps.\nNotice any error, warning or note?\n\n\nBuilding and installing\nWe can also install our package on our system.\nThe easiest way is with the “Install and Restart” button. This will list the package in your Packages tab, which means you will be able to load it whenever you need the functions when you work on another project.\nHowever, if you want to save a copy and share it with others, you can build the package:\n\nOn Windows: Build &gt; Build Binary Package\nOn Linux or macOS: Build &gt; Build Source Package\n\nThe resulting archive is what you can share with colleagues and friends who want to try your package on their computer.\nTo install it (the name of the archive will depend on your system), either use the “Install” menu in the Packages tab (using “Install from Package Archive File” instead of the CRAN repositories), or use this command:\n\ninstall.packages(\"../acornucopia_0.1.0.tar.gz\", repos = NULL)\n\n\nWe have to set repos to NULL so R doesn’t look for the package on CRAN.\n\nOthers can now have our package listed in their packages list, ready to be loaded whenever they want to use its functions.\n\n\nBest practice\nAs a general rule, it is best to stick to a minimum of dependencies, so the package:\n\nrequires less maintenance\nis lighter to install\n\nFor function names, try to avoid dots, and use underscores instead (tidystyle).\n\n\nPublishing\nWe can then try to publish our package, making sure that we follow the guidelines / requirements relevant to where we publish it.\nIn order to publish on CRAN, we have to follow stringent policies: https://cran.r-project.org/web/packages/policies.html\nAlthough CRAN is the main and default repository for the R ecosystem, other repositories and communities exist. For example, Bioconductor hosts thousands of packages relevant to bioinformatics, and ROpenSci supports a community around hundreds of peer-reviewed packages useful for querying and analysing various scientific data sources.\n\n\nUsing version control\nWhen developing software, it is important to keep track of versions, and if you collaborate with others, of authors too.\nIt also allows you to roll back to a previous version if needed.\nRStudio integrates Git, the most popular version control system for software development. To setup a Git repository for our package, we can use “Tools &gt; Version Control &gt; Project Setup…”, and create a new Git repository. We can then use the Git tab to save snapshots of our files.\nYou can then host your code on Gitlab or GitHub to make it accessible to others. See for example: https://github.com/Lchiffon/wordcloud2\nThe package usethis provides many useful functions to setup packages, including a use_github() function to quickly create a remote repository, and a use_readme_*() to add a readme file for the project.\nOthers will then be able to install your package from your remote repository with:\n\ndevtools::install_github(\"username/myPackage\")\ndevtools::install_gitlab(\"username/myPackage\")",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "9. Functions and packaging"
    ]
  },
  {
    "objectID": "R/packaging/packaging.html#useful-links",
    "href": "R/packaging/packaging.html#useful-links",
    "title": "R advanced: packaging and sharing functions",
    "section": "Useful links",
    "text": "Useful links\n\nR Packages, by Jenny Bryan and Hadley Wickham: http://r-pkgs.had.co.nz/\nFull official guide for packaging: https://cran.r-project.org/doc/manuals/r-release/R-exts.html\nWhat to lookout for when publishing to CRAN: https://cran.r-project.org/web/packages/policies.html\nPackage development cheatsheet: https://github.com/rstudio/cheatsheets/raw/master/package-development.pdf",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "9. Functions and packaging"
    ]
  },
  {
    "objectID": "R/packaging/packaging.html#extras",
    "href": "R/packaging/packaging.html#extras",
    "title": "R advanced: packaging and sharing functions",
    "section": "Extras",
    "text": "Extras\nThese two topics are important when developing custom functions, but can not fit in this session. They are described here for reference, if needed.\n\nQuasiquotation\nTidyverse packages use quasiquotation and lazy evaluation to save us a lot of typing.\nFor example, we can do:\n\nmtcars |&gt; select(disp)\n\n… but disp is not an existing object in our environment.\nquo_name() quotes, whereas sym() gets the symbol.\nTry eval(sym(something)).\nThis might lead to issues, like:\n\nselector &lt;- function(d, col) {\n  d |&gt; select(col)\n}\n\nWe need to quote-unquote:\n\nselector &lt;- function(d, col) {\n  col &lt;- enquo(col) # do not evaluate yet!\n  d |&gt; select(!!col) # evaluate here only\n}\n\nFor multiple arguments:\n\nselector &lt;- function(d, ...) {\n  col &lt;- enquos(...)\n  d |&gt; select(!!!col)\n}\n\n\n\nClasses\nWe can assign new classes to objects:\n\nobj &lt;- \"Hello world!\"\nclass(obj) &lt;- \"coolSentence\"\nattributes(obj)\n\nThe structure() function is useful for that too.\nWe can then define methods for this specific class, for example a coolSentence.print() function.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "9. Functions and packaging"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html",
    "href": "R/rstudio_intro/rstudio_intro.html",
    "title": "R with RStudio: getting started",
    "section": "",
    "text": "TipUpcoming workshop(s) available!\n\n\n\nThe next workshop is on Mon Feb 23 at 01:00 PM.\nBook in to the next offering now.\nAlternatively, check our calendar for future events.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#r-rstudio",
    "href": "R/rstudio_intro/rstudio_intro.html#r-rstudio",
    "title": "R with RStudio: getting started",
    "section": "R + RStudio",
    "text": "R + RStudio\nThe R programming language is a language used for calculations, statistics, visualisations and many more data science tasks.\nRStudio is an open source Integrated Development Environment (IDE) for R, which means it provides many features on top of R to make it easier to write and run code.\nR’s main strong points are:\n\nOpen Source: you can install it anywhere and adapt it to your needs;\nReproducibility: makes an analysis repeatable by detailing the process in a script;\nCustomisable: being a programming language, you can create your own custom tools;\nLarge datasets: it can handle very large datasets (certainly well beyond the row limitations of Excel, and even further using HPCs and other tricks);\nDiverse ecosystem: packages allow you to extend R for thousands of different analyses.\n\nThe learning curve will be steeper than point-and-click tools, but as far as programming languages go, R is more user-friendly than others.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#installation",
    "href": "R/rstudio_intro/rstudio_intro.html#installation",
    "title": "R with RStudio: getting started",
    "section": "Installation",
    "text": "Installation\nFor this course, you need to have both R and RStudio installed (installation instructions).",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#open-rstudio",
    "href": "R/rstudio_intro/rstudio_intro.html#open-rstudio",
    "title": "R with RStudio: getting started",
    "section": "Open RStudio",
    "text": "Open RStudio\n\nIf you are using your own laptop please open RStudio\n\nMake sure you have a working Internet connection\n\nOn Library computers:\n\nLog in with your UQ username and password (if you are both staff and student, use your student account)\nMake sure you have a working Internet connection\nGo to search at bottom left corner (magnifying glass)\nOpen the ZENworks application\nSearch for “RStudio”\nDouble-click on RStudio which will install both R and RStudio",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#what-are-we-going-to-learn",
    "href": "R/rstudio_intro/rstudio_intro.html#what-are-we-going-to-learn",
    "title": "R with RStudio: getting started",
    "section": "What are we going to learn?",
    "text": "What are we going to learn?\nThis session is designed to get straight into using R in a short amount of time, which is why we won’t spend too much time on the smaller details that make the language.\nDuring this session, you will:\n\nCreate a project for data analysis\nCreate a folder structure\nKnow where to find help\nLearn about a few useful functions\nCreate a script\nImport a dataset\nUnderstand the different RStudio panels\nUse a few shortcuts\nKnow how to extend R with packages\nGenerate a data visualisation",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#r-projects",
    "href": "R/rstudio_intro/rstudio_intro.html#r-projects",
    "title": "R with RStudio: getting started",
    "section": "R Projects",
    "text": "R Projects\nLet’s first create a new project:\n\nClick the “File” menu button (top left corner), then “New Project”\nClick “New Directory”\nClick “New Project”\nIn “Directory name”, type the name of your project, for example “YYYY-MM-DD_rstudio-intro”\nBrowse and select a folder where to locate your project (~ is your home directory). For example, a folder called “r-projects”.\nClick the “Create Project” button\n\n\nR Projects make your work with R more straight forward, as they allow you to segregate your different projects in separate folders. You can create a .Rproj file in a new directory or an existing directory that already has R code and data. Everything then happens by default in this directory. The .Rproj file stores information about your project options, and allows you to go straight back to your work.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#maths-and-objects",
    "href": "R/rstudio_intro/rstudio_intro.html#maths-and-objects",
    "title": "R with RStudio: getting started",
    "section": "Maths and objects",
    "text": "Maths and objects\nThe console (usually at the bottom left in RStudio) is where most of the action happens. In the console, we can use R interactively. We write a command and then execute it by pressing Enter.\nIn its most basic use, R can be a calculator. Try executing the following commands:\n\n10 - 2\n\n[1] 8\n\n3 * 4\n\n[1] 12\n\n2 + 10 / 5\n\n[1] 4\n\n11^6\n\n[1] 1771561\n\n\nThose symbols are called “binary operators”: we can use them to multiply, divide, add, subtract and exponentiate. Once we execute the command (the “input”), we can see the result in the console (the “output”).\nWhat if we want to keep reusing the same value? We can store data by creating objects, and assigning values to them with the assignment operator &lt;-:\n\nnum1 &lt;- 42\nnum2 &lt;- num1 / 9\nnum2\n\n[1] 4.666667\n\n\nWe can also store text data:\n\nsentence &lt;- \"Hello World!\"\nsentence\n\n[1] \"Hello World!\"\n\n\nYou should now see your objects listed in you environment pane (top right).\nAs you can see, you can store different kinds of data as objects. If you want to store text data (a “string of characters”), you have to use quotes around them.\n\nYou can use the shortcut Alt+- to type the assignement operator quicker.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#create-a-folder-structure",
    "href": "R/rstudio_intro/rstudio_intro.html#create-a-folder-structure",
    "title": "R with RStudio: getting started",
    "section": "Create a folder structure",
    "text": "Create a folder structure\nTo keep it tidy, we are creating 3 folders in our project directory:\n\nscripts\ndata\nplots\n\nFor that, we use the function dir.create():\ndir.create(\"scripts\")\ndir.create(\"data\")\ndir.create(\"plots\")\n\nYou can recall your recent commands with the up arrow, which is especially useful to correct typos or slightly modify a long command.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#scripts",
    "href": "R/rstudio_intro/rstudio_intro.html#scripts",
    "title": "R with RStudio: getting started",
    "section": "Scripts",
    "text": "Scripts\nScripts are simple text files that contain R code. They are useful for:\n\nsaving a set of commands for later use (and executing it in one click)\nmaking research reproducible\nmaking writing and reading code more comfortable\ndocumenting the code with comments, and\nsharing your work with peers\n\nLet’s create a new R script with a command:\nfile.create(\"scripts/process.R\")\n\nAll the file paths are relative to our current working directory, i.e. the project directory. To use an absolute file path, we can start with /.\n\nTo edit the new script, use the file.edit() function. Try using the Tab key to autocomplete your function name and your file path!\nfile.edit(\"scripts/process.R\")\nThis opens our fourth panel in RStudio: the source panel.\n\nMany ways to do one thing\nAs in many programs, there are many ways to achieve one thing.\nFor example, we used commands to create and edit a script, but we could also:\n\nuse the shortcut Ctrl+Shift+N\nuse the top left drop-down menus\n\nLearning how to use functions rather than the graphical interface will allow you to integrate them in scripts, and will sometimes help you to do things faster.\n\n\nComments\nWe should start with a couple of comments, to document our script. Comments start with #, and will be ignored by R:\n# Description: Introduction to R and RStudio\n# Author: &lt;your name&gt;\n# Date: &lt;today's date&gt;\n\n\nSyntax highlighting\nNow, add some commands to your script:\n\nnum1 &lt;- 42\nnum2 &lt;- num1 / 9\n\nNotice the colours? This is called syntax highlighting. This is one of the many ways RStudio makes it more comfortable to work with R. The code is more readable when working in a script.\n\nWhile editing your script, you can run the current command (or the selected block of code) by using Ctrl+Enter. Remember to save your script regularly with the shortcut Ctrl+S. You can find more shortcuts with Alt+Shift+K, or the menu “Tools &gt; Keyboard Shortcuts Help”.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#functions",
    "href": "R/rstudio_intro/rstudio_intro.html#functions",
    "title": "R with RStudio: getting started",
    "section": "Functions",
    "text": "Functions\nAn R function is a little program that does a particular job. It usually looks like this:\n&lt;functionname&gt;(&lt;argument(s)&gt;)\nArguments tell the function what to do. Some functions don’t need arguments, others need one or several, but they always need the parentheses after their name.\nFor example, try running the following command:\n\nround(num2)\n\n[1] 5\n\n\nThe round() function rounds a number to the closest integer. The only argument we give it is num2, the number we want to round.\n\nIf you scroll back to the top of your console, you will now be able to spot functions in the text.\n\n\nHelp\nWhat if we want to learn more about a function?\nThere are two main ways to find help about a specific function in RStudio:\n\nthe shortcut command: ?functionname\nthe keyboard shortcut: press F1 with your cursor in a function name (you can do this by simply clicking on the function name)\n\nLet’s look through the documentation for the round() function:\n?round\nAs you can see, different functions might share the same documentation page.\nThere is quite a lot of information in a function’s documentation, but the most important bits are:\n\nDescription: general description of the function(s)\nUsage: overview of what syntax can be used\nArguments: description of what each argument is\nExamples: some examples that demonstrate what is possible\n\nSee how the round() function has a second argument available? Try this now:\n\nround(num2, digits = 2)\n\n[1] 4.67\n\n\nWe can change the default behaviour of the function by telling it how many digits we want after the decimal point, using the argument digits. And if we use the arguments in order, we don’t need to name them:\n\nround(num2, 2)\n\n[1] 4.67\n\n\nTo group values together in a single object, use the c() function.\nc() combines the arguments into a vector. In other words, it takes any number of arguments (hence the ...), and stores all those values together, as one single object. For example, let’s store the ages of our pet dogs in a new object:\n\nages &lt;- c(4, 10, 2, NA, 3)\n\n\nYou can store missing data as NA.\n\nWe can now reuse this vector, and calculate their human age:\n\nages * 7\n\n[1] 28 70 14 NA 21\n\n\nR can create visualisations with functions too. Try a bar plot of your dogs’ ages with the barplot() function:\n\nbarplot(ages)\n\n\n\n\n\n\n\n\nWe can customise the plot with a title and some colours, for example:\n\nbarplot(ages, main = \"How old are my dogs?\", col = \"pink\")\n\n\n\n\n\n\n\n\n\nChallenge 1 – Finding help\nUse the help pages to find out what these functions do, and try executing commands with them:\n\nrep.int()\nmean()\nrm()\ncitation()\n\nrep.int() creates vectors like c(), but it is designed to easily replicate values. For example, if you find something very funny:\n\nrep.int(\"Ha!\", 30)\n\n [1] \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\"\n[13] \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\"\n[25] \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\" \"Ha!\"\n\n\nThe next function, mean(), returns the mean of a vector of numbers:\n\nmean(ages)\n\n[1] NA\n\n\nWhat happened there?\nWe have an NA value in the vector, which means the function can’t tell what the mean is. If we want to change this default behaviour, we can use an extra argument: na.rm, which stands for “remove NAs”.\n\nmean(ages, na.rm = TRUE)\n\n[1] 4.75\n\n\n\nIn our last command, if we hadn’t named the na.rm argument, R would have understood TRUE to be the value for the trim argument!\n\nrm() removes an object from your environment (remove() and rm() point to the same function). For example:\n\nrm(num1)\n\n\nR does not check if you are sure you want to remove something! As a programming language, it does what you ask it to do, which means you might have to be more careful. But you’ll see later on that, when working with scripts, this is less of a problem.\n\nLet’s do some more complex operations by combining two functions:\nls() returns a character vector: it contains the names of all the objects in the current environment (i.e. the objects we created in this R session). Is there a way we could combine it with rm()?\nYou can remove all the objects in the environment by using ls() as the value for the list argument:\n\nrm(list = ls())\n\nWe are nesting a function inside another one. More precisely, we are using the output of the ls() function as the value passed on to the list argument in the rm() function.\nFinally, the citation() function allows you to cite R or a specific package.\n\n\n\nIncomplete functions\nIf you don’t finish a function, by leaving off the last bracket ) for example, the line of code won’t necessarily give you an error, but it won’t work very well. If you forget to include that last bracket, R will run the code, and then wait for further instructions before giving you an output. This will appear as a + in the console like so:\n&gt; round(1.23\n+\nIf you try to give any further instructions to R, it will likely just continue giving you + symbols, and not return anything. To stop this, click on the console and press the Esc key on your keyboard.\n\n\nMore help\nWe’ve practised how to find help about functions we know the name of. What if we don’t know what the function is called? Or if we want general help about R?\n\nThe function help.start() is a good starting point: it opens a browser of official R help.\nIf you want to search for a word in all the documentation, you can use the ?? syntax. For example, try executing ??anova.\nFinally, you will often go to your web browser and search for a particular question, or a specific error message: most times, there already is an answer somewhere on the Internet. The challenge is to ask the right question!",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#import-data",
    "href": "R/rstudio_intro/rstudio_intro.html#import-data",
    "title": "R with RStudio: getting started",
    "section": "Import data",
    "text": "Import data\n\nChallenge 2 – Import data\nCopy and paste the following two commands into your script:\ndownload.file(url = \"https://raw.githubusercontent.com/resbaz/r-novice-gapminder-files/master/data/gapminder-FiveYearData.csv\",\n  destfile = \"data/gapminderdata.csv\")\ngapminder &lt;- read.csv(\"data/gapminderdata.csv\")\nWhat do you think they do? Describe each one in detail, and try executing them.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#explore-data",
    "href": "R/rstudio_intro/rstudio_intro.html#explore-data",
    "title": "R with RStudio: getting started",
    "section": "Explore data",
    "text": "Explore data\nWe have downloaded a CSV file from the Internet, and read it into an object called gapminder.\nYou can type the name of your new object to print it to screen:\n\ngapminder\n\nThat’s a lot of lines printed to your console. To have a look at the first few lines only, we can use the head() function:\n\nhead(gapminder)\n\n\n  \n\n\n\nNow let’s use a few functions to learn more about our dataset:\n\nclass(gapminder) # what kind of object is it stored as?\n\n[1] \"data.frame\"\n\nnrow(gapminder) # how many rows?\n\n[1] 1704\n\nncol(gapminder) # how many columns?\n\n[1] 6\n\ndim(gapminder) # rows and columns\n\n[1] 1704    6\n\nnames(gapminder) # variable names\n\n[1] \"country\"   \"year\"      \"pop\"       \"continent\" \"lifeExp\"   \"gdpPercap\"\n\n\nAll the information we just saw (and more) is available with one single function:\n\nstr(gapminder) # general structure\n\n'data.frame':   1704 obs. of  6 variables:\n $ country  : chr  \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ year     : int  1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ...\n $ pop      : num  8425333 9240934 10267083 11537966 13079460 ...\n $ continent: chr  \"Asia\" \"Asia\" \"Asia\" \"Asia\" ...\n $ lifeExp  : num  28.8 30.3 32 34 36.1 ...\n $ gdpPercap: num  779 821 853 836 740 ...\n\n\n\nThe RStudio’s environment panel already shows us some of that information (click on the blue arrow next to the object name).\n\nAnd to explore the data in a viewer, click on the table icon next to the object in the Environment pane.\nThis viewer allows you to explore your data by scrolling through, searching terms, filtering rows and sorting the data. Remember that it is only a viewer: it will never modify your original object.\n\nNotice that RStudio actually runs the View() function. Feel free to use that instead of clicking on the button, but note that the case matters: using a lowercase “v” will yield an error.\n\nTo see summary statistics for each of our variables, you can use the summary() function:\n\nsummary(gapminder)\n\n   country               year           pop             continent        \n Length:1704        Min.   :1952   Min.   :6.001e+04   Length:1704       \n Class :character   1st Qu.:1966   1st Qu.:2.794e+06   Class :character  \n Mode  :character   Median :1980   Median :7.024e+06   Mode  :character  \n                    Mean   :1980   Mean   :2.960e+07                     \n                    3rd Qu.:1993   3rd Qu.:1.959e+07                     \n                    Max.   :2007   Max.   :1.319e+09                     \n    lifeExp        gdpPercap       \n Min.   :23.60   Min.   :   241.2  \n 1st Qu.:48.20   1st Qu.:  1202.1  \n Median :60.71   Median :  3531.8  \n Mean   :59.47   Mean   :  7215.3  \n 3rd Qu.:70.85   3rd Qu.:  9325.5  \n Max.   :82.60   Max.   :113523.1  \n\n\nNotice how categorical and numerical variables are handled differently?\nLet’s now plot the relationship between GDP per capita and life expectancy:\n\nplot(gapminder$gdpPercap, gapminder$lifeExp,\n     xlab = \"GDP per capita (USD)\",\n     ylab = \"Life expectancy (years)\")\n\n\n\n\n\n\n\n\n\nFor more on visualisations, we will dive into the ggplot2 package during two of our other R sessions.\n\nFinally, let’s fit a linear model to see how strongly correlated the two variables are:\n\nlinear_model &lt;- lm(gapminder$lifeExp ~ gapminder$gdpPercap)\nsummary(linear_model)\n\n\nCall:\nlm(formula = gapminder$lifeExp ~ gapminder$gdpPercap)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-82.754  -7.758   2.176   8.225  18.426 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.396e+01  3.150e-01  171.29   &lt;2e-16 ***\ngapminder$gdpPercap 7.649e-04  2.579e-05   29.66   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.49 on 1702 degrees of freedom\nMultiple R-squared:  0.3407,    Adjusted R-squared:  0.3403 \nF-statistic: 879.6 on 1 and 1702 DF,  p-value: &lt; 2.2e-16\n\n\nThe P-value suggests that there is a strong relationship between the two.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#packages",
    "href": "R/rstudio_intro/rstudio_intro.html#packages",
    "title": "R with RStudio: getting started",
    "section": "Packages",
    "text": "Packages\nPackages add functionalities to R and RStudio. There are more than 21000 available.\nYou can see the list of installed packages in your “Packages” tab, or by using the library() function without any argument.\nWe are going to install a package called “skimr”. We can do that in the Packages tab:\n\nOpen the “Packages” tab (bottom-right pane)\nClick the “Install” button\nSearch for “skimr”\nClick “Install”\n\nNotice how it runs an install.packages() command in the console? You can use that too.\nIf I now try running the command skim(), I get an error. That’s because, even though the package is installed, I need to load it every time I start a new R session. The library() function does that. Let’s load the package, and use the skim() function to get an augmented summary of our gapminder dataset:\n\nlibrary(skimr) # load the package\nskim(gapminder) # use a function from the package\n\n\nData summary\n\n\nName\ngapminder\n\n\nNumber of rows\n1704\n\n\nNumber of columns\n6\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncountry\n0\n1\n4\n24\n0\n142\n0\n\n\ncontinent\n0\n1\n4\n8\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n1979.50\n17.27\n1952.00\n1965.75\n1979.50\n1993.25\n2007.0\n▇▅▅▅▇\n\n\npop\n0\n1\n29601212.33\n106157896.75\n60011.00\n2793664.00\n7023595.50\n19585221.75\n1318683096.0\n▇▁▁▁▁\n\n\nlifeExp\n0\n1\n59.47\n12.92\n23.60\n48.20\n60.71\n70.85\n82.6\n▁▆▇▇▇\n\n\ngdpPercap\n0\n1\n7215.33\n9857.45\n241.17\n1202.06\n3531.85\n9325.46\n113523.1\n▇▁▁▁▁\n\n\n\n\n\nThis function provides further summary statistics, and even displays a small histogram for each numeric variable.\n\nPackages are essential to use R to its full potential, by making the most out of what other users have created and shared with the community. To get an idea of some of the most important packages depending on your field of study, you can start with the CRAN Task Tiews.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#closing-rstudio",
    "href": "R/rstudio_intro/rstudio_intro.html#closing-rstudio",
    "title": "R with RStudio: getting started",
    "section": "Closing RStudio",
    "text": "Closing RStudio\nYou can close RStudio after making sure that you saved your script.\nWhen you create a project in RStudio, you create an .Rproj file that gathers information about the state of your project. When you close RStudio, you have the option to save your workspace (i.e. the objects in your environment) as an .Rdata file. The .Rdata file is used to reload your workspace when you open your project again. Projects also bring back whatever source file (e.g. script) you had open, and your command history. You will find your command history in the “History” tab (upper right panel): all the commands that we used should be in there.\nIf you have a script that contains all your work, it is a good idea not to save your workspace: it makes it less likely to run into errors because of accumulating objects. The script will allow you to get back to where you left it, by executing all the clearly laid-out steps.\nThe console, on the other hand, only shows a brand new R session when you reopen RStudio. Sessions are not persistent, and a clean one is started when you open your project again, which is why you have to load any extra package your work requires again with the library() function.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/rstudio_intro/rstudio_intro.html#what-next",
    "href": "R/rstudio_intro/rstudio_intro.html#what-next",
    "title": "R with RStudio: getting started",
    "section": "What next?",
    "text": "What next?\n\n\nWe have a compilation of resources for the rest of your R learning\nAnd a cheatsheet of main terms and concepts for R",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "1. Getting started"
    ]
  },
  {
    "objectID": "R/tidyverse_next_steps/tidyverse_next_steps.html",
    "href": "R/tidyverse_next_steps/tidyverse_next_steps.html",
    "title": "R tidyverse: loops and data tidying",
    "section": "",
    "text": "If needed, review the installation instructions.\n\n\nIf you are using your own laptop please open RStudio\n\nMake sure you have a working Internet connection\n\nOn the Library’s training computers:\n\nLog in with your UQ username and password\nMake sure you have a working Internet connection\nOpen the ZENworks application\nLook for “RStudio”\nDouble click on RStudio, which will install both R and RStudio\n\n\nWith RStudio open, let’s make sure we have the necessary packages installed by running this command (this might take a few minutes):\n\ninstall.packages(\"tidyverse\")\n\nThis will install all the Tidyverse packages (and their dependencies).",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "5. tidyr and purrr: tidying and loops"
    ]
  },
  {
    "objectID": "R/tidyverse_next_steps/tidyverse_next_steps.html#setting-up",
    "href": "R/tidyverse_next_steps/tidyverse_next_steps.html#setting-up",
    "title": "R tidyverse: loops and data tidying",
    "section": "",
    "text": "If needed, review the installation instructions.\n\n\nIf you are using your own laptop please open RStudio\n\nMake sure you have a working Internet connection\n\nOn the Library’s training computers:\n\nLog in with your UQ username and password\nMake sure you have a working Internet connection\nOpen the ZENworks application\nLook for “RStudio”\nDouble click on RStudio, which will install both R and RStudio\n\n\nWith RStudio open, let’s make sure we have the necessary packages installed by running this command (this might take a few minutes):\n\ninstall.packages(\"tidyverse\")\n\nThis will install all the Tidyverse packages (and their dependencies).",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "5. tidyr and purrr: tidying and loops"
    ]
  },
  {
    "objectID": "R/tidyverse_next_steps/tidyverse_next_steps.html#what-are-we-going-to-learn",
    "href": "R/tidyverse_next_steps/tidyverse_next_steps.html#what-are-we-going-to-learn",
    "title": "R tidyverse: loops and data tidying",
    "section": "What are we going to learn?",
    "text": "What are we going to learn?\ntidyr and purrr, just like dplyr and ggplot2, are core to the Tidyverse.\n\ntidyr can be used to tidy your data\npurrr is useful to apply functions iteratively on lists or vectors",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "5. tidyr and purrr: tidying and loops"
    ]
  },
  {
    "objectID": "R/tidyverse_next_steps/tidyverse_next_steps.html#create-a-project-and-a-script",
    "href": "R/tidyverse_next_steps/tidyverse_next_steps.html#create-a-project-and-a-script",
    "title": "R tidyverse: loops and data tidying",
    "section": "Create a project and a script",
    "text": "Create a project and a script\nUse the project menu (top right) to create a “New project…”. Let’s name this one “tidyverse”.\nWe also want to work more comfortably by typing our code in a script. You can use the new file dropdown menu, or Ctrl+Shift+N, and save your script as “process.R” in the current working directory.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "5. tidyr and purrr: tidying and loops"
    ]
  },
  {
    "objectID": "R/tidyverse_next_steps/tidyverse_next_steps.html#load-the-necessary-packages",
    "href": "R/tidyverse_next_steps/tidyverse_next_steps.html#load-the-necessary-packages",
    "title": "R tidyverse: loops and data tidying",
    "section": "Load the necessary packages",
    "text": "Load the necessary packages\nWe can use one single command to load the 8 core Tidyverse packages:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.6\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.1     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.2\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "5. tidyr and purrr: tidying and loops"
    ]
  },
  {
    "objectID": "R/tidyverse_next_steps/tidyverse_next_steps.html#tidy-data",
    "href": "R/tidyverse_next_steps/tidyverse_next_steps.html#tidy-data",
    "title": "R tidyverse: loops and data tidying",
    "section": "Tidy data",
    "text": "Tidy data\nTidy data makes it easy to transform and analyse data in R (and many other tools). Tidy data has observations in rows, and variables in columns. The whole Tidyverse is designed to work with tidy data.\nOften, a dataset is organised in a way that makes it easy for humans to read and populate. This is usually called “wide format”. Tidy data is usually in “long” format.\nThe ultimate rules of tidy data are:\n\nEach row is an observation\nEach column is a variable\nEach cell contains one single value\n\n\nTo learn more about Tidy Data, you can read Hadley Wickham’s 2014 article on the topic.\n\n\nImport data\nWe are using a dataset from the World Bank, which contains data about energy consumption and greenhouse gas emissions.\nLet’s download the file:\n\n# download data, save locally\ndownload.file(url = \"https://raw.githubusercontent.com/uqlibrary/technology-training/master/R/tidyverse_next_steps/data_wb_climate.csv\",\n              destfile = \"data_wb_climate.csv\")\n\n… and read the data into an object:\n\n# read CSV into an object\nclimate_raw &lt;- read_csv(\"data_wb_climate.csv\",\n                    na = \"..\")\n\nRows: 1165 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (4): Country code, Country name, Series code, Series name\ndbl (23): SCALE, Decimals, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1...\nlgl  (1): 2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWe defined with the na argument that, in this dataset, missing data is recorded as “..”.\nYou can use View() to explore your dataset. We can see that it doesn’t respect the tidy data principles in a couple of ways, the most obvious one being that different years are spread out between different columns.\n\n\nReshaping data\n\nLengthening\nTo go from wide format to long format, we can use the tidyr function pivot_longer(). Here, we want to gather all the columns titled with a year: we store the data in a “value” variable, and the years in a “year” variable.\n\nclimate_long &lt;- pivot_longer(climate_raw,\n                             `1990`:`2011`,\n                             names_to = \"year\",\n                             values_to = \"value\")\n\nColumn names are stored as character by default, so we also use an extra argument to convert the type for the year column from character to integer:\n\nclimate_long &lt;- pivot_longer(climate_raw,\n                             `1990`:`2011`,\n                             names_to = \"year\",\n                             values_to = \"value\",\n                             names_transform = as.integer)\n\nWe had to use backticks to refer to our year columns because they are numbers. We have to do the same for columns with spaces in their names, so let’s replace those spaces with dots:\n\nclimate_long &lt;- pivot_longer(climate_raw,\n                             `1990`:`2011`,\n                             names_to = \"year\",\n                             values_to = \"value\",\n                             names_transform = as.integer) |&gt; \n  rename_with(make.names)\n\nThis is better, but there is still an issue: our value variable contains many different indicators (i.e. entirely different units).\n\n\nWidening\nTo do the opposite, going from long to wide format, we can use the pivot_wider() function.\nWe have single observations spread across several rows, so we should spread the “value” column.\nFirst, let’s keep a record of the correspondence between long descriptive variable names and their “code”, for later reference:\n\ncodes &lt;- climate_long |&gt; \n  select(Series.code, Series.name) |&gt; \n  unique()\ncodes\n\n\n  \n\n\n\nThis will be our key to variable details, or “code book”, for future reference.\nNow, let’s widen the data (and remove some useless columns with dplyr::select()):\n\nclimate_tidy &lt;- climate_long |&gt; \n  select(-Series.name, -SCALE, -Decimals) |&gt; \n  pivot_wider(names_from = Series.code,\n              values_from = value)\n\n\n\n\nChallenge 1: Code comprehension\nThere’s one more cleaning step we need to apply.\nHave a look at this block of code. What do you think it does?\n\ngroups &lt;- c(\"Europe & Central Asia\",\n            \"East Asia & Pacific\",\n            \"Euro area\",\n            \"High income\",\n            \"Lower middle income\",\n            \"Low income\",\n            \"Low & middle income\",\n            \"Middle income\",\n            \"Middle East & North Africa\",\n            \"Latin America & Caribbean\",\n            \"South Asia\",\n            \"Small island developing states\",\n            \"Sub-Saharan Africa\",\n            \"Upper middle income\",\n            \"World\")\nclimate_tidy &lt;- climate_tidy |&gt; \n  filter(!Country.name %in% groups)\n\nTurns out this dataset contains grouped data as well as unique countries. Here, we created a vector of group names, and removed them from the data by using dplyr’s filter() function (inverting the filter with !).\nWe can now check that we’ve only got single countries left:\n\nunique(climate_tidy$Country.name)\n\n\n\nVisualising\nNow that we have clean, tidy data, we can process and visualise it more comfortably! For example, to visualise the increase in KT of CO2-equivalent for each country:\n\nclimate_tidy |&gt; \n  ggplot(aes(x = year,\n             y = EN.ATM.CO2E.KT,\n             group = Country.name)) +\n  geom_line()\n\nWarning: Removed 1091 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\n\nChallenge 2\nLooks like our data is missing after 2008, so how can we remove that?\nOne solution is to remove rows with missing data:\n\nco2e_no_na &lt;- climate_tidy |&gt; \n  drop_na(EN.ATM.CO2E.KT)\n\nggplot(co2e_no_na,\n       aes(x = year,\n           y = EN.ATM.CO2E.KT,\n           group = Country.name)) +\n  geom_line()\n\n\n\n\n\n\n\n\nAlternatively, we could filter on the actual year. (Which would not be ideal if the data was to be updated in the future!)\nThere are a lot of countries represented here. This kind of visualisation would benefit from focusing on a handful of countries we’re interested in, depending on what story we are telling. We can then overlay two line geometries: one for the whole dataset, and the other for our selection.\n\n# find top 4 for 2008\ntop4 &lt;- co2e_no_na |&gt;\n  filter(year == 2008) |&gt;\n  slice_max(EN.ATM.CO2E.KT, n = 4)\n\n# plot them on top of the rest\nggplot(co2e_no_na,\n       aes(x = year,\n           y = EN.ATM.CO2E.KT,\n           group = Country.name)) +\n  geom_line(colour = \"darkgrey\") +\n  geom_line(data = filter(co2e_no_na, Country.name %in% top4$Country.name),\n            mapping = aes(colour = Country.name)) +\n  labs(y = \"CO2-equivalent (KT)\",\n       colour = \"Top emitters\")",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "5. tidyr and purrr: tidying and loops"
    ]
  },
  {
    "objectID": "R/tidyverse_next_steps/tidyverse_next_steps.html#functional-programming",
    "href": "R/tidyverse_next_steps/tidyverse_next_steps.html#functional-programming",
    "title": "R tidyverse: loops and data tidying",
    "section": "Functional programming",
    "text": "Functional programming\nFunctional programming (as opposed to “imperative programming”) makes use of functions rather than loops to iterate over objects. The functions will allow to simplify our code, by abstracting common building blocks used in different cases of iteration. However, it means that there will usually be a different function for each different pattern.\nYou can iterate over elements by using:\n\nthe basic building blocks in R (for loops, while loops…), or\nthe apply function family from base R, or\nthe purrr functions.\n\nImagine we want to find out the median value for each variable in the mtcars dataset. Here is an example of a for loop:\n\noutput &lt;- vector(\"double\", ncol(mtcars))\nfor (i in seq_along(mtcars)) {\n  output[[i]] &lt;- median(mtcars[[i]])\n}\noutput\n\n [1]  19.200   6.000 196.300 123.000   3.695   3.325  17.710   0.000   0.000\n[10]   4.000   2.000\n\n\nBetter than having the same code repeated 11 times!\nWe allocate space in the expected output first (more efficient). We then specify the sequence for the loop, and put what we want to iterate in the loop body.\nThe apply family in base R is useful to replace for loops, but the purrr functions are easier to learn because they are more consistent. This package offers several tools to iterate functions over elements in a vector or a list (e.g. a dataframe).\n\nThe map family\nAt purrr’s core, there is the map family:\n\nmap() outputs a list.\nmap_lgl() outputs a logical vector.\nmap_int() outputs an integer vector.\nmap_dbl() outputs a double vector.\nmap_chr() outputs a character vector.\n\nFor example, to do a similar operation to our previous for loop:\n\nmap_dbl(mtcars, median)\n\n    mpg     cyl    disp      hp    drat      wt    qsec      vs      am    gear \n 19.200   6.000 196.300 123.000   3.695   3.325  17.710   0.000   0.000   4.000 \n   carb \n  2.000 \n\n\nA lot leaner, right?\nThe map functions automatically name the values in the resulting vector, which makes the result easier to read.\nLets try a different type of output. Here, we want to find out which columns in the World Bank dataset are numeric variables:\n\nmap_lgl(climate_tidy, is.numeric)\n\n        Country.code         Country.name                 year \n               FALSE                FALSE                 TRUE \nEG.USE.COMM.GD.PP.KD    EG.USE.PCAP.KG.OE       EN.ATM.CO2E.KT \n                TRUE                 TRUE                 TRUE \n      EN.ATM.CO2E.PC EN.ATM.CO2E.PP.GD.KD \n                TRUE                 TRUE \n\n\nIf we don’t want to use the default behaviour of the mapped function, we can use extra arguments to pass to it. For example, for a trimmed mean:\n\nmap_dbl(mtcars, mean, trim = 0.2)\n\n     mpg      cyl     disp       hp     drat       wt     qsec       vs \n 19.2200   6.3000 219.1750 137.9000   3.5755   3.1970  17.8175   0.4000 \n      am     gear     carb \n  0.3500   3.5500   2.7000 \n\n\nJust like most functions in the Tidyverse, the first argument is the data that we want to process (which means we can use the pipe). The second argument is the name of the function we want to apply, but it can also be a custom anonymous function. For example:\n\n# round the mean to closest integer\nmap_int(mtcars, \\(x) round(mean(x)))\n\n mpg  cyl disp   hp drat   wt qsec   vs   am gear carb \n  20    6  231  147    4    3   18    0    0    4    3 \n\n# is the maximum more than three times the minimum?\nmap_lgl(mtcars, \\(x) max(x) &gt; 3 * min(x))\n\n  mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb \n TRUE FALSE  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE FALSE  TRUE \n\n\nWe have to use the shorthand backslash syntax \\(x) to introduce a custom anonymous function, and the argument x to place the element being processed.\n\nChallenge 3: custom formula\nHow can we find out the number of unique values in each variable of the starwars data.frame?\n\nmap_int(starwars, \\(x) length(unique(x)))\n\n      name     height       mass hair_color skin_color  eye_color birth_year \n        87         46         39         12         31         15         37 \n       sex     gender  homeworld    species      films   vehicles  starships \n         5          3         49         38         24         11         16 \n\n\n\n\n\nSplitting\nTo split a dataset and map an operation to separate parts, we can use the group_split() function:\n\nunique(mtcars$cyl)\n\n[1] 6 4 8\n\nmtcars |&gt; \n  group_split(cyl) |&gt; # split into three dataframes\n  map(summary) # mapped to each dataframe\n\n[[1]]\n      mpg             cyl         disp              hp              drat      \n Min.   :21.40   Min.   :4   Min.   : 71.10   Min.   : 52.00   Min.   :3.690  \n 1st Qu.:22.80   1st Qu.:4   1st Qu.: 78.85   1st Qu.: 65.50   1st Qu.:3.810  \n Median :26.00   Median :4   Median :108.00   Median : 91.00   Median :4.080  \n Mean   :26.66   Mean   :4   Mean   :105.14   Mean   : 82.64   Mean   :4.071  \n 3rd Qu.:30.40   3rd Qu.:4   3rd Qu.:120.65   3rd Qu.: 96.00   3rd Qu.:4.165  \n Max.   :33.90   Max.   :4   Max.   :146.70   Max.   :113.00   Max.   :4.930  \n       wt             qsec             vs               am        \n Min.   :1.513   Min.   :16.70   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:1.885   1st Qu.:18.56   1st Qu.:1.0000   1st Qu.:0.5000  \n Median :2.200   Median :18.90   Median :1.0000   Median :1.0000  \n Mean   :2.286   Mean   :19.14   Mean   :0.9091   Mean   :0.7273  \n 3rd Qu.:2.623   3rd Qu.:19.95   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :3.190   Max.   :22.90   Max.   :1.0000   Max.   :1.0000  \n      gear            carb      \n Min.   :3.000   Min.   :1.000  \n 1st Qu.:4.000   1st Qu.:1.000  \n Median :4.000   Median :2.000  \n Mean   :4.091   Mean   :1.545  \n 3rd Qu.:4.000   3rd Qu.:2.000  \n Max.   :5.000   Max.   :2.000  \n\n[[2]]\n      mpg             cyl         disp             hp             drat      \n Min.   :17.80   Min.   :6   Min.   :145.0   Min.   :105.0   Min.   :2.760  \n 1st Qu.:18.65   1st Qu.:6   1st Qu.:160.0   1st Qu.:110.0   1st Qu.:3.350  \n Median :19.70   Median :6   Median :167.6   Median :110.0   Median :3.900  \n Mean   :19.74   Mean   :6   Mean   :183.3   Mean   :122.3   Mean   :3.586  \n 3rd Qu.:21.00   3rd Qu.:6   3rd Qu.:196.3   3rd Qu.:123.0   3rd Qu.:3.910  \n Max.   :21.40   Max.   :6   Max.   :258.0   Max.   :175.0   Max.   :3.920  \n       wt             qsec             vs               am        \n Min.   :2.620   Min.   :15.50   Min.   :0.0000   Min.   :0.0000  \n 1st Qu.:2.822   1st Qu.:16.74   1st Qu.:0.0000   1st Qu.:0.0000  \n Median :3.215   Median :18.30   Median :1.0000   Median :0.0000  \n Mean   :3.117   Mean   :17.98   Mean   :0.5714   Mean   :0.4286  \n 3rd Qu.:3.440   3rd Qu.:19.17   3rd Qu.:1.0000   3rd Qu.:1.0000  \n Max.   :3.460   Max.   :20.22   Max.   :1.0000   Max.   :1.0000  \n      gear            carb      \n Min.   :3.000   Min.   :1.000  \n 1st Qu.:3.500   1st Qu.:2.500  \n Median :4.000   Median :4.000  \n Mean   :3.857   Mean   :3.429  \n 3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :5.000   Max.   :6.000  \n\n[[3]]\n      mpg             cyl         disp             hp             drat      \n Min.   :10.40   Min.   :8   Min.   :275.8   Min.   :150.0   Min.   :2.760  \n 1st Qu.:14.40   1st Qu.:8   1st Qu.:301.8   1st Qu.:176.2   1st Qu.:3.070  \n Median :15.20   Median :8   Median :350.5   Median :192.5   Median :3.115  \n Mean   :15.10   Mean   :8   Mean   :353.1   Mean   :209.2   Mean   :3.229  \n 3rd Qu.:16.25   3rd Qu.:8   3rd Qu.:390.0   3rd Qu.:241.2   3rd Qu.:3.225  \n Max.   :19.20   Max.   :8   Max.   :472.0   Max.   :335.0   Max.   :4.220  \n       wt             qsec             vs          am              gear      \n Min.   :3.170   Min.   :14.50   Min.   :0   Min.   :0.0000   Min.   :3.000  \n 1st Qu.:3.533   1st Qu.:16.10   1st Qu.:0   1st Qu.:0.0000   1st Qu.:3.000  \n Median :3.755   Median :17.18   Median :0   Median :0.0000   Median :3.000  \n Mean   :3.999   Mean   :16.77   Mean   :0   Mean   :0.1429   Mean   :3.286  \n 3rd Qu.:4.014   3rd Qu.:17.55   3rd Qu.:0   3rd Qu.:0.0000   3rd Qu.:3.000  \n Max.   :5.424   Max.   :18.00   Max.   :0   Max.   :1.0000   Max.   :5.000  \n      carb     \n Min.   :2.00  \n 1st Qu.:2.25  \n Median :3.50  \n Mean   :3.50  \n 3rd Qu.:4.00  \n Max.   :8.00  \n\n\nUsing purrr functions with ggplot2 functions allows us to generate several plots in one command:\n\nmtcars |&gt; \n  group_split(cyl) |&gt; \n  map(\\(x) ggplot(x, aes(wt, mpg)) +\n        geom_point() +\n        geom_smooth() +\n        labs(title = paste(x$cyl, \"cylinders\"))) # give a title\n\n[[1]]\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n[[2]]\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n[[3]]\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nPredicate functions\nPurrr also contains functions that check for a condition, so we can set up conditions before iterating.\n\nstr(iris)\n\n'data.frame':   150 obs. of  5 variables:\n $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\n $ Species     : Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\niris |&gt;\n  map_dbl(mean) # warning, NA for Species\n\nWarning in mean.default(.x[[i]], ...): argument is not numeric or logical:\nreturning NA\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n    5.843333     3.057333     3.758000     1.199333           NA \n\niris |&gt;\n  discard(is.factor) |&gt; \n  map_dbl(mean) # clean!\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width \n    5.843333     3.057333     3.758000     1.199333 \n\nstarwars |&gt; \n  keep(is.character) |&gt; \n  map_int(\\(x) length(unique(x)))\n\n      name hair_color skin_color  eye_color        sex     gender  homeworld \n        87         12         31         15          5          3         49 \n   species \n        38 \n\n\nis.factor() and is.character() are examples of “predicate functions”.\nTo return everything, but apply a function only if a condition is met, we can use map_if():\n\nclimate_tidy |&gt;\n  map_if(is.numeric, round) |&gt; \n  str()\n\nList of 8\n $ Country.code        : chr [1:4796] \"ABW\" \"ABW\" \"ABW\" \"ABW\" ...\n $ Country.name        : chr [1:4796] \"Aruba\" \"Aruba\" \"Aruba\" \"Aruba\" ...\n $ year                : num [1:4796] 1990 1991 1992 1993 1994 ...\n $ EG.USE.COMM.GD.PP.KD: num [1:4796] NA NA NA NA NA NA NA NA NA NA ...\n $ EG.USE.PCAP.KG.OE   : num [1:4796] NA NA NA NA NA NA NA NA NA NA ...\n $ EN.ATM.CO2E.KT      : num [1:4796] 1841 1929 1723 1771 1764 ...\n $ EN.ATM.CO2E.PC      : num [1:4796] 30 30 25 24 23 22 22 22 19 19 ...\n $ EN.ATM.CO2E.PP.GD.KD: num [1:4796] NA NA NA NA NA NA NA NA NA NA ...\n\n\nThis results in a list in which the elements are rounded only if they store numeric data.",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "5. tidyr and purrr: tidying and loops"
    ]
  },
  {
    "objectID": "R/tidyverse_next_steps/tidyverse_next_steps.html#using-the-tidyverse-packages-together",
    "href": "R/tidyverse_next_steps/tidyverse_next_steps.html#using-the-tidyverse-packages-together",
    "title": "R tidyverse: loops and data tidying",
    "section": "Using the Tidyverse packages together",
    "text": "Using the Tidyverse packages together\nNow, let’s create another visualisation with our climate dataset. In this one, we use functions from 4 packages: tidyr, dplyr, stringr and ggplot2.\n\n# cumulative and yearly change in CO2 emissions dataset\nclimate_cumul &lt;- climate_tidy |&gt; \n  drop_na(EN.ATM.CO2E.KT) |&gt; \n  arrange(Country.name, year) |&gt; \n  group_by(Country.name) |&gt;\n  mutate(cumul.CO2.KT = cumsum(EN.ATM.CO2E.KT),\n         dif.CO2.KT = EN.ATM.CO2E.KT - lag(EN.ATM.CO2E.KT)) |&gt;\n  ungroup() |&gt; \n  mutate(across(ends_with(\"KT\"), \\(x) x / 10^6)) |&gt; \n  rename_with(\\(x) str_replace(x, \"KT\", \"PG\"))\n\n# visualise cumulative change\np &lt;- climate_cumul |&gt;\n  ggplot() +\n  aes(x = year,\n      y = cumul.CO2.PG,\n      colour = Country.name) +\n  geom_line() +\n  theme(legend.position = \"none\")\np\n\n\n\n\n\n\n\n\nIf you want to create an interactive visualisation, you can use plotly:\n\nlibrary(plotly)\nggplotly(p)\n\n\n\n\n\nPlot the annual change in PG CO2 by country:\n\npdif &lt;- climate_cumul |&gt;\n  ggplot() +\n  aes(x = year,\n      y = dif.CO2.PG,\n      colour = Country.name) +\n  geom_line() +\n  theme(legend.position = \"none\")\n# interactive plot\nggplotly(pdif)",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "5. tidyr and purrr: tidying and loops"
    ]
  },
  {
    "objectID": "R/tidyverse_next_steps/tidyverse_next_steps.html#what-next",
    "href": "R/tidyverse_next_steps/tidyverse_next_steps.html#what-next",
    "title": "R tidyverse: loops and data tidying",
    "section": "What next",
    "text": "What next\n\nChapter on iteration in the book R for Data Science\nCheatsheets:\n\ntidyr\npurrr\n\nExplore our recommended resources, online and around UQ\nTidy Data paper",
    "crumbs": [
      "Home",
      "![](/images/R.svg){width=20} R with RStudio",
      "5. tidyr and purrr: tidying and loops"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html",
    "href": "Shell/shell_intro.html",
    "title": "Unix Shell: scripting and automating",
    "section": "",
    "text": "The Unix shell we use for this lesson is called Bash.\nInstallation instructions are available on this page.",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#installation",
    "href": "Shell/shell_intro.html#installation",
    "title": "Unix Shell: scripting and automating",
    "section": "",
    "text": "The Unix shell we use for this lesson is called Bash.\nInstallation instructions are available on this page.",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#data",
    "href": "Shell/shell_intro.html#data",
    "title": "Unix Shell: scripting and automating",
    "section": "Data",
    "text": "Data\nDownload the data archive from this link and extract its contents on your Desktop.",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#introduction",
    "href": "Shell/shell_intro.html#introduction",
    "title": "Unix Shell: scripting and automating",
    "section": "Introduction",
    "text": "Introduction\n\nThe shell is a program that enables us to send commands to the computer and receive output. It is also referred to as the “terminal” or “command line”. When we use the shell, we use a command-line interface (or CLI) instead of a graphical user interface (or GUI). We type a command, and press enter to execute it.\n\n\nWhy use the shell?\n\nThe shell’s main advantages are its high action-to-keystroke ratio, its support for light task automation, and its capacity to access networked machines.\nThe shell’s main disadvantages are its primarily textual nature and how cryptic its commands and operation can be.\n\nThe Unix shell has been around longer than most of its users have been alive. It has survived so long because it’s a power tool that allows people to do complex things with just a few keystrokes. More importantly, it helps them combine existing programs in new ways and automate repetitive tasks so they aren’t typing the same things over and over again. Use of the shell is fundamental to using a wide range of other powerful tools and computing resources (including “high-performance computing” supercomputers). This lesson will start you on a path towards using these resources effectively.\n\n\nFormat\nWe will learn doing some live-coding, which means we will all be using the shell and typing the same things – a great way to learn. No need to take notes as they are available online for later reference.\n\n\nOur data: Nelle’s research\nThe data we use as an example for this lesson is a collection of 1520 files that contain information about protein abundance in samples collected by a marine biologist, Nelle Nemo. They need to be run through a program called goostats but that would take too much time if each file was run manually.\nThe shell might be helpful to automate this repetitive task.\nFirst, we’ll need to understand how to navigate our file system using the shell.",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#navigating-the-file-system",
    "href": "Shell/shell_intro.html#navigating-the-file-system",
    "title": "Unix Shell: scripting and automating",
    "section": "Navigating the file system",
    "text": "Navigating the file system\nThe part of the operating system responsible for managing files and directories is called the file system. It organises our data into files, which hold information, and directories (also called “folders”), which hold files or other directories.\nTo navigate our file system in the shell, let’s learn a few useful commands. Type the following command and press enter:\npwd\npwd stand for “print working directory” and outputs the name of the directory we are currently located in. For most, it will be the home directory of the current user.\nNow, try this command:\nls\nls stands for “listing”. It lists the contents of the current working directory.\nCommands can often take extra parameters, called flags (also called “options”). We can add the flag -a (for “all”) to our ls command in order to also list hidden elements:\nls -a\nTo find out more about a particular command, including what flags exist for it, use the --help flag after it, like so:\nls --help\n\nYou might have to use the command man ls (for manual) on some systems\n\nTo look at the contents of a different directory, we can specify it by adding the directory’s name as an argument:\nls Desktop\nAs you can see, a command can take both flags and arguments. For example, the command:\nls -lh Documents\n… associates the two flags -l (for “long listing”) and -h (for “human-readable”) to output extra information and make file sizes more user-friendly, and specifies that we want to list what the Documents directory contains.\nTo navigate into our data directory, we’ll use a new command called cd for “change directory”.\ncd Desktop\ncd data-shell\ncd data\nWe just navigated down three levels of directories, one at a time, starting from our home directory. It is also possible to do that in one command:\ncd Desktop/data-shell/data\nYou can always check where you are currently with pwd, and have a look at where you can navigate next with ls.\nI you want to go back to the data-shell directory, there is a shortcut to move up to the parent directory:\ncd ..\nSimilarly, the shortcut to specify the current working directory is a single dot: ..\ncd on its own will bring you back to your home directory.\nWe have been using relative paths so far, always referring to where we currently are in the file system, but we can also specify absolute paths by using a leading /, which represents the root directory (i.e. the highest in your file system). For example, you can always use one of the following commands to go to the data-shell folder, wherever you are (replace “username” by your user name):\ncd /Users/username/Desktop/data-shell\ncd /home/username/Desktop/data-shell\nTwo more shortcuts are handy when it comes to changing or specifying directories: ~ is the home directory, and - is the previous directory we were in.",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#working-with-files-and-directories",
    "href": "Shell/shell_intro.html#working-with-files-and-directories",
    "title": "Unix Shell: scripting and automating",
    "section": "Working with files and directories",
    "text": "Working with files and directories\nWe now know how to explore files and directories, but how do we create, modify and delete them?\nIn the data-shell directory, let’s create a new directory called thesis thanks to the mkdir command (for “make directory”):\ncd ../..\nmkdir thesis\n\nTo work more comfortably with the shell, it is a good idea to name files and directory without using whitespaces, as they are usually used to separate arguments in commands.\n\nUsing ls will now list the newly created directory.\nWe can check that the new directory is in fact empty:\nls thesis\nLet’s move into it and create a new text file called draft.txt using a text editor called Nano:\ncd thesis\nnano draft.txt\nType a few lines of text, and save with Ctrl+O. (Nano uses the symbol ^ for the control key.) Nano also checks that you are happy with the file name: press enter at the prompt, and exit the editor with Ctrl+X.\nNano does not leave any ouptut, but you can check that the file exists with ls. You can also see the contents of a text file with the cat command (it stands for “concatenate”):\ncat\nIf you are not happy with your work, you can remove the file with the rm command, but beware: in the shell, deleting is forever! There is no rubbish bin.\nrm draft.txt\nLet’s re-create that file and then move up one directory to /Users/username/Desktop/data-shell using cd ..:\nnano draft.txt\nls\ncd ..\nIf we try to delete the thesis directory, we get an error message:\nrm thesis\nThis happens because rm by default only works on files, not directories.\nTo really get rid of thesis we must also delete the file draft.txt. We can do this with the recursive flag for rm:\nrm -r thesis\n\nRemoving the files in a directory recursively can be a very dangerous operation. If we’re concerned about what we might be deleting we can add the “interactive” flag -i to rm which will ask us for confirmation before each step.\n\nrm -ri thesis\n\nThis removes everything in the directory, then the directory itself, asking at each step for you to confirm the deletion. Type “y” and press Enter to confirm.\n\nLet’s create the directory and file one more time:\nmkdir thesis\nnano thesis/draft.txt\nls thesis\nThe name of our new file is not very informative. We can change it with the mv command (for “move”):\nmv thesis/draft.txt thesis/quotes.txt\nThe first argument tells mv what we’re “moving”, while the second is where it’s to go.\n\nmv can silently overwrite any existing file with the same name, which is why using the -i flag is also a good idea here.\n\nLet’s move quotes.txt into the current working directory, by using the . shortcut:\nmv thesis/quotes.txt .\nWe can now check that thesis is empty, and that quotes.txt exists in the current directory:\nls thesis\nls quotes.txt\nThe cp command copies a file. Let’s copy the file into the thesis directory, with a new name, and check that the original file and the copy both exist:\ncp quotes.txt thesis/quotations.txt\nls quotes.txt thesis/quotations.txt\nNow, let’s delete the original file and check with ls that it is actually gone:\nrm quotes.txt\nls quotes.txt",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#filters-and-pipes",
    "href": "Shell/shell_intro.html#filters-and-pipes",
    "title": "Unix Shell: scripting and automating",
    "section": "Filters and pipes",
    "text": "Filters and pipes\n\nFilters and pipes are the two building blocks for more complex commands. Filters are commands that allow the transformation of a stream of input into a stream of output, whereas pipes send the output of a command as the input of another one. Many commands fit the definition of filters and constitute “small pieces” that can be “loosely joined”, i.e. stringed in new ways. The “pipes and filters” programming model is permitted by the Unix focus on creating small single-purpose tools that work well together.\n\nIn the molecules directory, let’s use the wc command (for “word count”):\ncd molecules\nwc *.pdb\nThe * wildcard is used to match zero of more characters. Other wildcards include ? to match one single character.\nNotice how the output has three numbers for each file? They are the number of lines, words and characters. Flags for wc include -l for restricting the output to line numbers, -w for words, and -c for characters.\nTo figure out which file is the shortest, we can first redirect the number of lines into a new file thanks to &gt;, so we can reuse it later on:\nwc -l *.pdb &gt; lengths.txt\nThis creates the file, or overwrites it if it already exists. &gt;&gt; on the other hand will append to an existing file.\nThe sort command will print the alphabetically sorted data to screen. Using the -n flag will sort it numerically instead:\nsort -n lengths.txt\nWe now know that the top line is the shortest file. However, intermediate files make a long process complicated to follow, and clutter your hard drive. We can instead run two commands together:\nwc -l *.pdb | sort -n\nThe vertical bar, |, is called a pipe. It tells the shell we want to use the output of the command on the left as the input for the command on the right.\nhead and tail will respectively show the beginning and the end of some text. It is possile to overwrite the default of 10 lines with a flag that specifies how many lines we want returned. Let’s use head in our process to only show the first line of the sorted text:\nwc -l *.pdb | sort -n | head -1\nWe can string as many pipes and filters as we want, which makes it possible to do the whole task in one pipeline.\nThe pipeline can be read as a sentence: “Count the number of lines in all the PDB files, then sort them numerically, then return only the first line.”\n\nNelle’s pipeline\nNelle has run samples through the assay machines and created 17 files located in the north-pacific-gyre/2012-07-03 directory.\n\nA useful feature in CLIs is “tab completion”. To access folders with longer names, it is often possible to auto-complete the folder name by hitting the Tab key after typing a few letters: typing cd nor and pressing the Tab key will auto-complete to cd north-pacific-gyre/. Another press of the Tab key will add 2012-07-03/ to the command as it is the only item in the folder. If there are several options, pressing the Tab key twice will bring up a list.\n\nTo check the consistency of her data, she types:\nwc -l *.txt | sort -n | head -5\nOne file seems to be 60 lines shorter than the others. Before re-running that sample, she checks if other files have too much data:\nwc -l *.txt | sort -n | tail -5\n\nTo re-run a command you typed not long ago, or to slightly modify it, use the up arrow to navigate your history of commands.\n\nThe numbers look good, but the “Z” in there is not expected: everything should be marked either “A” or “B”, by convention. To find others, she types:\nls *Z.txt\nThose two files do not match with any depth she recorded, and she therefore won’t use them in her analysis. In case she still might need them later on, she won’t delete them; in the future, she might instead select the files she wants with a wildcard expression, like in this example:\nls *[AB].txt\nThis will match all files ending in A.txt or B.txt.",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#loops",
    "href": "Shell/shell_intro.html#loops",
    "title": "Unix Shell: scripting and automating",
    "section": "Loops",
    "text": "Loops\n\nHow can we perform the same action on many different files?\n\n\nLoops are key to productivity improvements through automation as they allow us to execute commands repetitively. Similar to wildcards and tab completion, using loops also reduces the amount of typing (and typing mistakes).\n\nIn the creatures directory (reached with cd ../../creatures), using the following command to create backups of our data files will throw an error:\ncp *.dat backup-*.dat\nThe issue is that it expands to giving cp more than two inputs, and therefore expects the last one to be a directory where the copies can go.\nThe way around that is to use a loop, to do some operation once for each element in a list.\nTo solve our file copying problem we can use this loop:\nfor filename in *.dat\ndo\n    cp $filename backup-$filename\ndone\nIn this loop, filename is a variable which is assigned a different file name in each run. The variable can be named whatever we want, but a descriptive name is better.\nWhen running this loop, the shell does the following: * expand *.dat to create a list of files * execute the loop body for each of those files: * copy the currently processed file and prepend “original-” to its name. * close the loop with “done”\nYou can check that your loop will do what you expect it to do beforehand, by prefixing the command in the loop body with echo:\nfor filename in *.dat\ndo\n    echo cp $filename original-$filename\ndone\necho is also useful to give extra information while the loop executes, as we’ll see later on.\n\nIf your file names contain spaces, you will have to use quotation marks around the filenames and the variable calls. But it is simpler to always avoid using whitespaces when naming files and directories!\n\n\nNelle’s pipeline\nNelle now wants to calculate stats on her data files with her lab’s program called goostats. The program takes two arguments: an input file (the raw data) and an output file (to store the stats).\nLocated in the north-pacific-gyre/2012-07-03, she designs the following loop:\nfor datafile in NENE*[AB].txt\n    do bash goostats $datafile stats-$datafile\ndone\n\nbash is a program that executes the contents of a script (here, the “goostats” script). More about scripts in a little bit!\n\nWhen she runs it, the shell seems stalled and nothing gets printed to the screen. She kills the running command with Ctrl + C, uses the up arrow to edit the command and adds anecho` line to the loop body in order to know which file is being processed:\nfor datafile in NENE*[AB].txt; do echo $datafile; bash goostats $datafile stats-$datafile; done\n\nNotice how you have to separate distinct parts of your code with a ; when it is written in one single line.\n\nIt looks like processing her whole dataset (1518 files) will take about two hours. She checks that a sample output file looks good, runs her loop and lets the computer process it all.\nHere is another example of how useful a loop can be: to create a logical directory structure. Say a researcher wants to organise experiments measuring reaction rate constants with different compounds and different temperatures. They could use a nested loop like this one:\nfor species in cubane ethane methane butane\ndo\n    for temperature in 25 30 37 40 50 60\n    do\n        mkdir $species-$temperature\n    done\ndone\nThis nested loop would create 24 directories in less than a second. How much time would that take with a graphical file browser?",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#shell-scripts",
    "href": "Shell/shell_intro.html#shell-scripts",
    "title": "Unix Shell: scripting and automating",
    "section": "Shell scripts",
    "text": "Shell scripts\n\nHow can I save and reuse commands?\n\nWe are finally ready to see what makes the shell such a powerful programming environment. We are going to take the commands we repeat frequently and save them in files so that we can re-run all those operations again later by typing a single command. For historical reasons, a bunch of commands saved in a file is usually called a shell script, but make no mistake: these are actually small programs.\nTo store her analytics and make them reproducible, Nelle creates a script. She create a new file with Nano:\nnano do-stats.sh\n… and write the following inside it:\nfor datafile in NENE*[AB].txt\ndo\n    echo $datafile\n    bash goostats $datafile stats-$datafile\ndone\n\nWriting Nelle’s loop in the command line wasn’t very comfortable. When you start writing blocks of code that do more complex things, you want to use a code editor like Nano. Notice how it highlights parts of your code differently? This is called “syntax highlighting”.\n\nNelle can now run her script with:\nbash do-stats.sh\nThis works well, but what if she wants to make her script more versatile, so the user can decide what the data files are? She modifies the script:\n# Calculate stats for data files.\n# Usage: bash do-stats.sh file(s)_to_process\nfor datafile in \"$@\"\ndo\n    echo $datafile\n    bash goostats $datafile stats-$datafile\ndone\nIn this new version, she added comments, with lines starting with #. These comments will be ignored by the shell, but will help others understand what the script does, and how to use it.\nShe also used the special variable $@, which means “any number of arguments”. The user can now provide one or several file names when using the script.\n\nIf you prefer to use a specific number of arguments, and use them according to their position, use the variable $1, $2, $3, etc.\n\nHer script now lets her decide what files to process, but she has to remember to exclude the “Z” files.\n\nDesigning a script always involves tradeoffs between flexibility and complexity.",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#finding-things",
    "href": "Shell/shell_intro.html#finding-things",
    "title": "Unix Shell: scripting and automating",
    "section": "Finding things",
    "text": "Finding things\n\nHow can I find files, and find things in files?\n\ngrep (for “global / regular expression / print”) is a command that finds and prints lines in files that match a pattern. To test this, we are going to work on a file that contains three haikus. To have a look at it, run the following commands:\ncd\ncd Desktop/data-shell/writing\ncat haiku.txt\nTo find lines that contatin the word “not”, run the following:\ngrep not haiku.txt\nThe output is the three lines in the file that contain the letters “not”.\nIf we look for the pattern “The”:\ngrep The haiku.txt\n… the output will show two lines, with one instance of those letters contained within a larger word: “Thesis”.\nTo restrict to lines containing “The” on its own, we can use the -w flag (for “word”):\ngrep -w The haiku.txt\nWe can also search for a phrase:\ngrep -w \"is not\" haiku.txt\nWe don’t have to use quotes for patterns without spaces, but we still can do that to be consistent. Another useful flag is -n (for line number):\ngrep -n \"it\" haiku.txt\nWe can also combine flags with this command. Let’s add the -i flag to make the search case-insensitive:\ngrep -nwi \"the\" haiku.txt\nWe can also invert our search with the -v flag, i.e. to output the lines that do not contain the pattern “the”:\ngrep -nwv \"the\" haiku.txt\nThe are many more flags available for grep. You can see a full list with the command grep --help.\ngrep’s real power comes from the fact that patterns can contain regular expressions. Regular expressions are both complex and powerful. For example, you can search for lines that have an “o” in the second position:\ngrep -E '^.o' haiku.txt\nWe use quotes and the -E flag (for “extended regular expression”) to prevent the shell from interpreting it. The ^ anchors the match to the start of the line; the . matches a single character; the o matches an actual lowercase “o”.\nLet’s try to analyse a bigger file, like the text from Little Women by Louisa May Alcott. We want to figure out which of the four sisters in the book (Jo, Meg, Beth and Amy) is the most mentioned, something we can achieve with a for loop and grep:\ncd data\nfor sis in Jo Meg Beth Amy\ndo\n    echo $sis:\n    grep -ow $sis LittleWomen.txt | wc -l\ndone\nWe use the -o flag (for “only matching”) in order to account for multiple occurences on a single line.\nWhile grep finds lines in files, the find command finds files themselves. Let’s move into the writing directory and test it:\ncd ..\nfind .\nWhen given the current working directory as the only argument, find’s output is the names of every file and directory under the current working directory. We can start filtering the output with the -type flag. d is for directory, and f is for files:\nfind . -type d\nfind . -type f\nWe can also match by name:\nfind . -name *.txt\nThe issue here is that the shell expanded the wildcard before running the command. To find all the text files in the directory tree, we have to use quotes:\nfind . -name '*.txt'\nIf we want to combine find with other commands, we might need a different method than building a pipeline. For example, to count the lines in each one of the found files, one would intuitively try the following:\nfind . -name '*.txt' | wc -l\n… which would only return the number of files find found. In order to pass each of the found files as separate arguments, we can use the following syntax instead:\nwc -l $(find . -name '*.txt')\nWhen the shell executes this command, it first expands whatever is inside $() before running the rest of the command, just like for wildcards. In short, $(command) inserts a command’s output in place.\nHere is an example combining grep and find:\ngrep \"FE\" $(find .. -name '*.pdb')\nThis command will list all the PDB files that contain iron atoms.",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#further-resources",
    "href": "Shell/shell_intro.html#further-resources",
    "title": "Unix Shell: scripting and automating",
    "section": "Further resources",
    "text": "Further resources\nTo learn more about the Unix Shell:\n\nSee the full Carpentries course and practise with challenges.\nA shell cheatsheet\nPractise your shell skills and learn from others on Exercism",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Shell/shell_intro.html#licence",
    "href": "Shell/shell_intro.html#licence",
    "title": "Unix Shell: scripting and automating",
    "section": "Licence",
    "text": "Licence\nThis short course is based on the longer course The Unix Shell developped by the non-profit organisation The Carpentries. The original material is licensed under a Creative Commons Attribution license (CC-BY 4.0), and this modified version uses the same license. You are therefore free to:\n\nShare — copy and redistribute the material in any medium or format\nAdapt — remix, transform, and build upon the material\n\n… as long as you give attribution, i.e. you give appropriate credit to the original author, and link to the license.",
    "crumbs": [
      "Home",
      "![](/images/bash.svg){width=20} Unix Shell"
    ]
  },
  {
    "objectID": "Word/word.html",
    "href": "Word/word.html",
    "title": "Microsoft Word",
    "section": "",
    "text": "Find below the resources for our Word sessions.",
    "crumbs": [
      "Home",
      "![](/images/word.svg){width=20} Word"
    ]
  },
  {
    "objectID": "Word/word.html#styles-and-structure",
    "href": "Word/word.html#styles-and-structure",
    "title": "Microsoft Word",
    "section": "Styles and structure",
    "text": "Styles and structure\n\nManual\nFiles",
    "crumbs": [
      "Home",
      "![](/images/word.svg){width=20} Word"
    ]
  },
  {
    "objectID": "Word/word.html#creating-a-structured-thesis-cast",
    "href": "Word/word.html#creating-a-structured-thesis-cast",
    "title": "Microsoft Word",
    "section": "Creating a structured thesis (CaST)",
    "text": "Creating a structured thesis (CaST)\n\nManual\nFiles\nRecording",
    "crumbs": [
      "Home",
      "![](/images/word.svg){width=20} Word"
    ]
  },
  {
    "objectID": "more_training.html",
    "href": "more_training.html",
    "title": "Training elsewhere",
    "section": "",
    "text": "Apart from the training the UQ Library offers, here are other providers, at UQ and elsewhere."
  },
  {
    "objectID": "more_training.html#at-uq",
    "href": "more_training.html#at-uq",
    "title": "Training elsewhere",
    "section": "At UQ",
    "text": "At UQ\n\nCBCS\nThe Centre for Biodiversity and Conservation Science hosts seminars and workshops, notably the R Workshops @ UQ series.\n\n\nData Science CRP\nThe Data Science Collaborative Research Platform (Data Science CRP) provides support and training on topics like bioinformatics, statistics, biometry and applied AI.\n\n\nGraduate School CDF\nThe Graduate School Career Development Framework (Graduate School CDF) lists various workshop directed at HDR students, including training offered by the Data Science CRP and the Institute for Social Science Research (ISSR).\n\n\nLADAL\nThe Language Technology and Data Analysis Laboratory (LADAL) offers numerous open educational resources, in particular on the topic of text analysis with R.\n\n\nResearch Computing Centre\nThe Research Computing Centre (RCC) offers training on High Performance Computing, as well as research platforms like Galaxy."
  },
  {
    "objectID": "more_training.html#around-uq",
    "href": "more_training.html#around-uq",
    "title": "Training elsewhere",
    "section": "Around UQ",
    "text": "Around UQ\n\nThe Carpentries\nThe Carpentries is a global, non-profit organisation offering workshops to learn foundation coding and data science skills to researchers. See upcoming workshops."
  }
]